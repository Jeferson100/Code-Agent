{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "179e03a0",
   "metadata": {},
   "source": [
    "# Agente de codificacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "129c806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "from IPython.display import Image, display\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb77182",
   "metadata": {},
   "source": [
    "## Criando prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7d6c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_code = \"\"\"You are a **Coding Assistant Agent**.  \n",
    "Your mission is to help the user with programming tasks across different languages and frameworks.  \n",
    "You must:  \n",
    "1. Write clean, efficient, and well-documented code.  \n",
    "2. Explain your reasoning and key choices in simple terms.  \n",
    "3. Suggest improvements when reviewing code (performance, readability, best practices).  \n",
    "4. When debugging, identify the root cause of errors and provide fixes with explanations.  \n",
    "5. Adapt your coding style to match the user’s context (e.g., beginner-friendly explanations or advanced optimizations).  \n",
    "6. When there are multiple solutions, compare trade-offs clearly.  \n",
    "7. Always follow security, scalability, and maintainability principles.  \n",
    "8. Never just drop code — guide the user through understanding it.  \n",
    "\n",
    "Primary objectives:  \n",
    "- Generate working code examples.  \n",
    "- Provide step-by-step explanations.  \n",
    "- Support iterative development and testing.  \n",
    "\n",
    "You are not just a code generator — you are a **teacher, reviewer, and collaborator**.  \n",
    "\n",
    "{messages}\n",
    "\n",
    "{feedback}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "supervisor_code= \"\"\"\n",
    "\n",
    "                You are a **Code Supervisor Agent**.  \n",
    "                Your mission is to **review and evaluate** code generated by other coding agents. \n",
    "                You do not write new solutions unless necessary — your role is to **analyze, critique, and guide**. \n",
    "                \n",
    "                The question of user was this:\n",
    "                {messages}\n",
    "                \n",
    "                The code generated to other agent was this:\n",
    "            \n",
    "                {code}\n",
    "                \n",
    "                You receive on message of test excution:\n",
    "                \n",
    "                {error}\n",
    "\n",
    "                Your responsibilities:  \n",
    "                1. **Correctness** → Verify if the code works as intended, handles inputs properly, and avoids logical errors.  \n",
    "                2. **Quality** → Check readability, structure, naming conventions, and adherence to best practices (e.g., PEP8 for Python).  \n",
    "                3. **Efficiency** → Assess performance, memory usage, and scalability. Suggest optimizations if beneficial.  \n",
    "                4. **Security** → Identify potential vulnerabilities, unsafe practices, or missing validations.  \n",
    "                5. **Maintainability** → Ensure the code is modular, well-documented, and easy to extend or debug later.  \n",
    "                6. **Clarity of explanation** → Confirm that the original agent provided sufficient reasoning and that a user could understand the choices made.  \n",
    "                7. **Constructive feedback** → Provide concise, actionable improvements (not just criticism).  \n",
    "                8. **Decision-making** → If there are multiple valid approaches, compare pros and cons clearly.  \n",
    "\n",
    "                Evaluation format:  \n",
    "                - **Strengths**: What is good about the code.  \n",
    "                - **Weaknesses**: Problems, risks, or bad practices.  \n",
    "                - **Suggestions**: Clear, prioritized improvements.  \n",
    "                - **Verdict**: Acceptable as-is or requires changes.  \n",
    "\n",
    "                You are not a coder — you are a **mentor, auditor, and quality gatekeeper**.  \n",
    "                Your goal is to ensure the code is **correct, safe, efficient, and teachable**.  \n",
    "                \n",
    "                \n",
    "                \"\"\"\n",
    "\n",
    "return_messages_prompt = \"\"\"\"\n",
    "You are the **Respondent Agent** in a coding assistant system.  \n",
    "Your mission is to communicate with the user in a clear, helpful, and professional way.  \n",
    "\n",
    "<Available Tools>\n",
    "You have access to one so tools: \n",
    "- **think_response**: Use this tool to reflect and analyze before crafting your final response.\n",
    "</Available Tools>\n",
    "\n",
    "\n",
    "<inputs>\n",
    "You receive two inputs:  \n",
    "- {messages} → the user’s request or question.  \n",
    "- {code} → the code generated, reviewed, or corrected by other agents.  \n",
    "- {feedback} → the supervisor agent's feedback on the code is working.\n",
    "</inputs>\n",
    "\n",
    "Your responsibilities:  \n",
    "1. **Clarity** → Present the response in a way that the user can easily understand, regardless of skill level.  \n",
    "2. **Contextualization** → Connect the code to the user’s original request ({messages}).  \n",
    "3. **Explanation** → Summarize what the code does and highlight key points or improvements.  \n",
    "4. **Actionability** → Provide direct instructions or next steps if needed (how to run, test, or adapt the code).  \n",
    "5. **Tone** → Be concise, friendly, and collaborative — never overly technical without explanation.  \n",
    "6. **Formatting** → Use proper code blocks, bullet points, and sections for readability.  \n",
    "\n",
    "Your goal:  \n",
    "Deliver the **final, user-facing answer**, combining the code with clear explanations so the user can immediately apply it.  \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt_agent_pesquisador = \"\"\"\n",
    "        You are part of a multi-agent coding assistant system.  \n",
    "        Your role is the **Research Agent**, with the ability to search the internet using the provided tool. \n",
    "        You analyze the {code} and {error_message}, then use the internet search tool to gather solutions or corrections that can fix the code.\n",
    "        \n",
    "        <Available Tools>                                                                                              \n",
    "        You have access to two main tools:                                                                             \n",
    "        1. **tavily_search**: For conducting web searches to gather information                                        \n",
    "        2. **think_tool**: For reflection and strategic planning during research                                                                                                                                                       │\n",
    "        **CRITICAL: Use think_tool after each search to reflect on results and plan next steps**                       \n",
    "        </Available Tools>  \n",
    "         \n",
    "        Inputs you may receive:  \n",
    "        - {messages} → the user’s original request.  \n",
    "        - {error_message}, {code}, {feedback} → context from previous phases to refine your research.  \n",
    "\n",
    "        Your goal is to about search the internet to gather solutions or corrections that can fix the code. \n",
    "        \n",
    "        <think_tool>\n",
    "        After each search tool call, use think_tool to analyze the results: \n",
    "        <think_tool>\n",
    "        \n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d481ab11",
   "metadata": {},
   "source": [
    "## Criando estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "695eed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated\n",
    "from typing import Literal, TypedDict, Tuple, List\n",
    "import operator\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class StateCode(TypedDict):\n",
    "    messages: Annotated[List, operator.add]\n",
    "    code: str\n",
    "    feedback: str\n",
    "    valid: Literal[True, False]\n",
    "    interactions: int\n",
    "    error_message : str\n",
    "    \n",
    "\n",
    "class SupervisorResponse(BaseModel):\n",
    "    valid: Literal[True, False] = Field(description=\"Valid from the supervisor, if not is need more interactions return True, else return False\")\n",
    "    feedback: str = Field(description=\"Feedback from the supervisor\")\n",
    "    \n",
    "class CodeOutput(BaseModel):\n",
    "    \"\"\"Schema for code solutions to questions about LCEL.\"\"\"\n",
    "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "    imports: str = Field(description=\"Code block import statements, if there are imports, leave it empty\")\n",
    "    code: str = Field(description=\"Code block not including import statements\")\n",
    "    \n",
    "class SearchResponse(BaseModel):\n",
    "    \"\"\"Response of agent that search in the internet\"\"\"\n",
    "    \n",
    "    feedback: str = Field(description= \"Fedeback of agent that search the internet for agent of code\")\n",
    "    code : str = Field(description= \"Code generated by the search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61b4494",
   "metadata": {},
   "source": [
    "## Criando roteador de LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60908863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "from groq import AsyncGroq\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import asyncio\n",
    "from typing import Optional, Dict, Any, Union\n",
    "import logging\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# client = Groq()\n",
    "client = AsyncGroq()\n",
    "\n",
    "\n",
    "class GetLlmResponse:\n",
    "    \"\"\"\n",
    "    Classe para obter respostas de diferentes provedores LLM\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, messages: str, model_llm: str, strutured_output: Optional[BaseModel] = None):\n",
    "        self.messages = messages\n",
    "        self.strutured_output = strutured_output\n",
    "        self.model_llm = model_llm\n",
    "    \n",
    "    async def llm_structured_groq(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Chama modelo Groq com saída estruturada\n",
    "        \"\"\"\n",
    "        response = await client.chat.completions.create(\n",
    "            model=self.model_llm,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": self.messages},  # Corrigido: usar self.messages\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"structured_response\",\n",
    "                    \"schema\": self.strutured_output.model_json_schema(),\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        content = response.choices[0].message.content or \"{}\"\n",
    "        return json.loads(content)\n",
    "    \n",
    "    async def llm_groq(self) -> str:\n",
    "        \"\"\"\n",
    "        Chama modelo Groq sem saída estruturada\n",
    "        \"\"\"\n",
    "        response = await client.chat.completions.create(\n",
    "            model=self.model_llm,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": self.messages},  # Corrigido: usar self.messages\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content or \"\"\n",
    "    \n",
    "    async def llm_pydantic_structured(self) -> Any:\n",
    "        \"\"\"\n",
    "        Chama modelo HuggingFace via Pydantic AI com saída estruturada\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from pydantic_ai import Agent\n",
    "            from pydantic_ai.models.huggingface import HuggingFaceModel\n",
    "            \n",
    "            model = HuggingFaceModel(self.model_llm)\n",
    "            agent = Agent(model, output_type=self.strutured_output)\n",
    "            \n",
    "            async with agent.run_stream(self.messages) as response:  # Corrigido: usar self.messages\n",
    "                async for profile in response.stream_output():\n",
    "                    return profile\n",
    "                    \n",
    "        except ImportError:\n",
    "            raise ImportError(\"pydantic_ai não está instalado. Instale com: pip install pydantic-ai\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro no llm_pydantic_structured: {e}\")\n",
    "            raise\n",
    "    \n",
    "    async def llm_pydantic(self) -> Any:\n",
    "        \"\"\"\n",
    "        Chama modelo HuggingFace via Pydantic AI sem saída estruturada\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from pydantic_ai import Agent\n",
    "            from pydantic_ai.models.huggingface import HuggingFaceModel\n",
    "            \n",
    "            model = HuggingFaceModel(self.model_llm)\n",
    "            agent = Agent(model)\n",
    "            \n",
    "            async with agent.run_stream(self.messages) as response:  # Corrigido: usar self.messages\n",
    "                async for profile in response.stream_output():\n",
    "                    return profile\n",
    "                    \n",
    "        except ImportError:\n",
    "            raise ImportError(\"pydantic_ai não está instalado. Instale com: pip install pydantic-ai\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro no llm_pydantic: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "class LlmRouter:\n",
    "    \"\"\"\n",
    "    Classe para roteamento automático entre diferentes modelos LLM com fallback\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, messages: str, strutured_output: Optional[BaseModel] = None):\n",
    "        self.messages = messages\n",
    "        self.strutured_output = strutured_output\n",
    "        \n",
    "        # Lista de modelos em ordem de prioridade\n",
    "        self.groq_models = [\n",
    "            \"moonshotai/kimi-k2-instruct-0905\",\n",
    "            \"moonshotai/kimi-k2-instruct\",\n",
    "            \"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "            \"openai/gpt-oss-120b\",\n",
    "            \n",
    "        ]\n",
    "        \n",
    "        self.huggingface_models = [\n",
    "            \"Qwen/Qwen3-235B-A22B-Instruct-2507\",\n",
    "            \"microsoft/DialoGPT-large\",\n",
    "            \"meta-llama/Llama-4-Maverick-17B-128E\"\n",
    "        ]\n",
    "    \n",
    "    async def _try_groq_models(self) -> Union[Dict[str, Any], str]:\n",
    "        \"\"\"\n",
    "        Tenta usar modelos Groq em ordem de prioridade\n",
    "        \"\"\"\n",
    "        for model in self.groq_models:\n",
    "            try:\n",
    "                logger.info(f\"Tentando modelo Groq: {model}\")\n",
    "                llm_response = GetLlmResponse(self.messages, model, self.strutured_output)\n",
    "                \n",
    "                if self.strutured_output:\n",
    "                    result = await llm_response.llm_structured_groq()\n",
    "                else:\n",
    "                    result = await llm_response.llm_groq()\n",
    "                \n",
    "                logger.info(f\"Sucesso com modelo Groq: {model}\")\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Falha no modelo Groq {model}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        raise Exception(\"Todos os modelos Groq falharam\")\n",
    "    \n",
    "    async def _try_huggingface_models(self) -> Any:\n",
    "        \"\"\"\n",
    "        Tenta usar modelos HuggingFace em ordem de prioridade\n",
    "        \"\"\"\n",
    "        for model in self.huggingface_models:\n",
    "            try:\n",
    "                logger.info(f\"Tentando modelo HuggingFace: {model}\")\n",
    "                llm_response = GetLlmResponse(self.messages, model, self.strutured_output)\n",
    "                \n",
    "                if self.strutured_output:\n",
    "                    result = await llm_response.llm_pydantic_structured()\n",
    "                else:\n",
    "                    result = await llm_response.llm_pydantic()\n",
    "                \n",
    "                logger.info(f\"Sucesso com modelo HuggingFace: {model}\")\n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Falha no modelo HuggingFace {model}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        raise Exception(\"Todos os modelos HuggingFace falharam\")\n",
    "    \n",
    "    async def llm_router(self) -> Union[Dict[str, Any], str, Any]:\n",
    "        \"\"\"\n",
    "        Método principal que implementa o sistema de fallback\n",
    "        \"\"\"\n",
    "        logger.info(\"Iniciando roteamento LLM\")\n",
    "        \n",
    "        # Primeira tentativa: Modelos Groq\n",
    "        try:\n",
    "            return await self._try_groq_models()\n",
    "        except Exception as groq_error:\n",
    "            logger.warning(f\"Todos os modelos Groq falharam: {groq_error}\")\n",
    "            \n",
    "            # Segunda tentativa: Modelos HuggingFace\n",
    "            try:\n",
    "                return await self._try_huggingface_models()\n",
    "            except Exception as hf_error:\n",
    "                logger.error(f\"Todos os modelos HuggingFace falharam: {hf_error}\")\n",
    "                \n",
    "                # Se tudo falhar, lança exceção detalhada\n",
    "                raise Exception(\n",
    "                    f\"Todos os provedores falharam. \"\n",
    "                    f\"Groq: {groq_error}, HuggingFace: {hf_error}\"\n",
    "                )\n",
    "    \n",
    "    def llm_router_sync(self) -> Union[Dict[str, Any], str, Any]:\n",
    "        \"\"\"\n",
    "        Versão síncrona do roteador LLM\n",
    "        \"\"\"\n",
    "        return asyncio.run(self.llm_router())\n",
    "    \n",
    "    async def get_structured_response(self, output_model: BaseModel) -> BaseModel:\n",
    "        \"\"\"\n",
    "        Obtém resposta estruturada e valida com o modelo Pydantic\n",
    "        \"\"\"\n",
    "        if not self.strutured_output:\n",
    "            # Atualiza temporariamente para usar saída estruturada\n",
    "            original_structured = self.strutured_output\n",
    "            self.strutured_output = output_model\n",
    "            \n",
    "            try:\n",
    "                result = await self.llm_router()\n",
    "                return output_model(**result) if isinstance(result, dict) else output_model()\n",
    "            finally:\n",
    "                self.strutured_output = original_structured\n",
    "        else:\n",
    "            result = await self.llm_router()\n",
    "            return output_model(**result) if isinstance(result, dict) else output_model()\n",
    "    \n",
    "    def add_groq_model(self, model_name: str, priority: int = 0):\n",
    "        \"\"\"\n",
    "        Adiciona um novo modelo Groq à lista\n",
    "        \"\"\"\n",
    "        if priority == 0:\n",
    "            self.groq_models.append(model_name)\n",
    "        else:\n",
    "            self.groq_models.insert(priority, model_name)\n",
    "    \n",
    "    def add_huggingface_model(self, model_name: str, priority: int = 0):\n",
    "        \"\"\"\n",
    "        Adiciona um novo modelo HuggingFace à lista\n",
    "        \"\"\"\n",
    "        if priority == 0:\n",
    "            self.huggingface_models.append(model_name)\n",
    "        else:\n",
    "            self.huggingface_models.insert(priority, model_name)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5fc01b",
   "metadata": {},
   "source": [
    "## Criando agente codificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb893e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jefer\\Documents\\Ciencia-de-dados\\LLMs\\AgenteCodificaoLangGraph\\.venv\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:229: UserWarning: Found qwen/qwen3-coder-480b-a35b-instruct in available_models, but type is unknown and inference may fail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "llm_code = init_chat_model(\n",
    "    \"qwen/qwen3-coder-480b-a35b-instruct\", model_provider=\"nvidia\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d9e910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def node_code(state: StateCode) -> StateCode:\n",
    "    \n",
    "    print(\"Entrei no code\")\n",
    "    \n",
    "    print(\"state\", state[\"messages\"])\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    if messages:\n",
    "        messages = messages[-1]\n",
    "    \n",
    "    feedback = state.get(\"feedback\", \"\")\n",
    "    \n",
    "    prompt_code_format = prompt_code.format(messages=messages, feedback=feedback)\n",
    "    \n",
    "    response = llm_code.invoke(prompt_code_format)\n",
    "    \n",
    "    router_structured = LlmRouter(\n",
    "            response.content,\n",
    "            CodeOutput\n",
    "        )\n",
    "    \n",
    "    response_code_formatted = await router_structured.llm_router()\n",
    "    \n",
    "    if isinstance(response_code_formatted, dict):\n",
    "        code = response_code_formatted[\"code\"]\n",
    "        imports = response_code_formatted[\"imports\"]\n",
    "        prefix = response_code_formatted[\"prefix\"]\n",
    "        \n",
    "        return {\"code\": code, \"imports\": imports, \"prefix\": prefix}\n",
    "        \n",
    "    else:\n",
    "        code = response_code_formatted.code\n",
    "        imports = response_code_formatted.imports\n",
    "        prefix = response_code_formatted.prefix\n",
    "        \n",
    "        return {\"code\": code, \"imports\": imports, \"prefix\": prefix}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d5c3c0",
   "metadata": {},
   "source": [
    "## Criando verificador de código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adcb0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_check(state: StateCode) -> StateCode:\n",
    "    \"\"\"\n",
    "    Check code\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, error\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECKING CODE---\")\n",
    "\n",
    "    # State\n",
    "    messages = state.get(\"messages\")\n",
    "    \n",
    "    if not messages:\n",
    "        messages = messages[-1]\n",
    "    \n",
    "    code = state.get(\"code\")\n",
    "    imports = state.get(\"imports\")\n",
    "    messages_erro = []\n",
    "\n",
    "    # Check imports\n",
    "    if imports:\n",
    "        try:\n",
    "            exec(imports)\n",
    "        except Exception as e:\n",
    "            print(f\"---CODE IMPORT CHECK: FAILED--- {e} \")\n",
    "            error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n",
    "            messages_erro += error_message\n",
    "            return {\n",
    "                \"error_message\": messages_erro,\n",
    "            }\n",
    "    else:\n",
    "        print(\"---NO CODE IMPORTS---\")\n",
    "\n",
    "    # Check execution\n",
    "    try:\n",
    "        if imports:\n",
    "            exec(imports + \"\\n\" + code)\n",
    "        else:\n",
    "            exec(code)\n",
    "    except Exception as e:\n",
    "        print(f\"---CODE BLOCK CHECK: FAILED--- {e}\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n",
    "        messages_erro += error_message\n",
    "        return {\n",
    "            \"error_message\": messages_erro,\n",
    "        }\n",
    "    # No errors\n",
    "    print(\"---NO CODE TEST FAILURES---\")\n",
    "    return {\n",
    "        \"error_message\": messages_erro,\n",
    "    \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31af187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "def python_repl(state: StateCode) -> StateCode:\n",
    "    \"\"\"\n",
    "    Use this function to execute Python code and get the results.\n",
    "    \"\"\"\n",
    "    repl = PythonREPL()\n",
    "    \n",
    "    print(\"---CHECKING CODE---\")\n",
    "\n",
    "    # State\n",
    "    messages = state.get(\"messages\")\n",
    "    \n",
    "    if not messages:\n",
    "        messages = messages[-1]\n",
    "    \n",
    "    code = state.get(\"code\")\n",
    "    imports = state.get(\"imports\")\n",
    "    messages_erro = []\n",
    "\n",
    "    # Check imports\n",
    "    if imports:\n",
    "        try:\n",
    "            print(\"Running the Python REPL tool\")\n",
    "            print(imports + \"\\n\" + code)\n",
    "            result = repl.run(imports + \"\\n\" + code)\n",
    "            print(result)\n",
    "        except BaseException as e:\n",
    "            print(f\"Failed to execute. Error: {e!r}\")\n",
    "            return {\"error_message\": messages_erro,}\n",
    "        \n",
    "    else:\n",
    "        try:\n",
    "            print(\"Running the Python REPL tool\")\n",
    "            print(code)\n",
    "            result = repl.run(code)\n",
    "            print(result)\n",
    "        except BaseException as e:\n",
    "            return {\"error_message\": messages_erro,}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a74338",
   "metadata": {},
   "source": [
    "## Criando agente supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ae31deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def node_supervisor(state: StateCode) -> StateCode:\n",
    "    \n",
    "    print(\"Entrei no supervisor\")\n",
    "    \n",
    "    print(\"state\", state[\"messages\"])\n",
    "    \n",
    "    messages = state.get(\"messages\")\n",
    "    \n",
    "    if messages:\n",
    "        messages = messages[-1]\n",
    "    \n",
    "    interactions = state.get(\"interactions\", 0)\n",
    "    \n",
    "    error_message = state.get(\"error_message\", \"\")\n",
    "    \n",
    "    if interactions: \n",
    "        interactions += 1\n",
    "    else:\n",
    "        interactions = 1\n",
    "        \n",
    "    code = state[\"code\"]\n",
    "    \n",
    "    prompt_supervisor_format = supervisor_code.format(messages=messages, code=code, error=error_message)\n",
    "    \n",
    "    #response = await llm_router_structured(prompt_supervisor_format, SupervisorResponse)\n",
    "    \n",
    "    router_structured = LlmRouter(\n",
    "            prompt_supervisor_format,\n",
    "            SupervisorResponse\n",
    "        )\n",
    "    \n",
    "    response = await router_structured.llm_router()\n",
    "    \n",
    "    if isinstance(response, dict) and \"valid\" not in response.keys() and \"feedback\" not in response.keys():\n",
    "        response = await router_structured.llm_router()\n",
    "    \n",
    "    if isinstance(response, dict):\n",
    "        print(response)\n",
    "        feedback = response[\"feedback\"]\n",
    "        valid = response[\"valid\"]\n",
    "    else:\n",
    "        feedback = response.feedback\n",
    "        valid = response.valid\n",
    "    \n",
    "    return {\"feedback\": feedback, \"valid\": valid, \"interactions\": interactions}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd2f43e",
   "metadata": {},
   "source": [
    "# Gerador de pesquisador da internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bef7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ebe0f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    \"messages\": \"Create the analysi of the dataset titanic\",\n",
    "    \"error_message\": \"nanan\",\n",
    "    \"code\": \"\",\n",
    "    \"feedback\": \"O código não funciona corretamente.\"\n",
    "}\n",
    "\n",
    "prompt_agent_pesquisador_format = prompt_agent_pesquisador.format(\n",
    "        messages = result['messages'],\n",
    "        error_message = result['error_message'],\n",
    "        code = result['code'],\n",
    "        feedback = result['feedback']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "916c994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "tool_tavily = TavilySearch(\n",
    "    max_results=5,\n",
    "    topic=\"general\",\n",
    "    # include_answer=False,\n",
    "    # include_raw_content=False,\n",
    "    # include_images=False,\n",
    "    # include_image_descriptions=False,\n",
    "    # search_depth=\"basic\",\n",
    "    # time_range=\"day\",\n",
    "    # include_domains=None,\n",
    "    exclude_domains=[\"medium.com\", \"youtube.com\", \"wikipedia.org\", \"linkedin.com\"],\n",
    ")\n",
    "\n",
    "@tool\n",
    "def think_tool(reflection: str) -> str:\n",
    "    \"\"\"Tool for strategic reflection on research progress and decision-making.\n",
    "    \n",
    "    Use this tool after each search to analyze results and plan next steps systematically.\n",
    "    This creates a deliberate pause in the research workflow for quality decision-making.\n",
    "    \n",
    "    When to use:\n",
    "    - After receiving search results: What key information did I find?\n",
    "    - Before deciding next steps: Do I have enough to answer comprehensively?\n",
    "    - When assessing research gaps: What specific information am I still missing?\n",
    "    - Before concluding research: Can I provide a complete answer now?\n",
    "    \n",
    "    Reflection should address:\n",
    "    1. Analysis of current findings - What concrete information have I gathered?\n",
    "    2. Gap assessment - What crucial information is still missing?\n",
    "    3. Quality evaluation - Do I have sufficient evidence/examples for a good answer?\n",
    "    4. Strategic decision - Should I continue searching or provide my answer?\n",
    "    \n",
    "    Args:\n",
    "        reflection: Your detailed reflection on research progress, findings, gaps, and next steps\n",
    "        \n",
    "    Returns:\n",
    "        Confirmation that reflection was recorded for decision-making\n",
    "    \"\"\"\n",
    "    return f\"Reflection recorded: {reflection}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e924465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tool_tavily.invoke(\"O código não funciona corretamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e72deea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cc652db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        You are part of a multi-agent coding assistant system.  \\n        Your role is the **Research Agent**, with the ability to search the internet using the provided tool. \\n        You analyze the {code} and {error_message}, then use the internet search tool to gather solutions or corrections that can fix the code.\\n\\n        <Available Tools>                                                                                              \\n        You have access to two main tools:                                                                             \\n        1. **tavily_search**: For conducting web searches to gather information                                        \\n        2. **think_tool**: For reflection and strategic planning during research                                                                                                                                                       │\\n        **CRITICAL: Use think_tool after each search to reflect on results and plan next steps**                       \\n        </Available Tools>  \\n\\n        Inputs you may receive:  \\n        - {messages} → the user’s original request.  \\n        - {error_message}, {code}, {feedback} → context from previous phases to refine your research.  \\n\\n        Your goal is to about search the internet to gather solutions or corrections that can fix the code. \\n\\n        <think_tool>\\n        After each search tool call, use think_tool to analyze the results: \\n        <think_tool>\\n\\n        '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_agent_pesquisador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b2978a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_init = init_chat_model(\"moonshotai/kimi-k2-instruct\", model_provider=\"groq\")  \n",
    "\n",
    "agente_search = create_react_agent(\n",
    "    llm_init, [tool_tavily, think_tool],\n",
    "    prompt=prompt_agent_pesquisador,\n",
    "    #response_format= SearchResponse\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3105e00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(agente_search.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e27a51e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def node_search(state: StateCode) -> StateCode:\n",
    "    print(\"Entrei no search\")    \n",
    "    messages = state.get(\"messages\")\n",
    "    \n",
    "    if messages:\n",
    "        messages = messages[-1]\n",
    "    \n",
    "    code = state.get(\"code\")\n",
    "    error_message = state.get(\"error_message\")\n",
    "    feedback = state.get(\"feedback\")\n",
    "    \n",
    "    prompt_agent_pesquisador_format = prompt_agent_pesquisador.format(\n",
    "        messages = messages,\n",
    "        error_message = error_message,\n",
    "        code = code,\n",
    "        feedback = feedback)\n",
    "    \n",
    "    llm_init = init_chat_model(\"moonshotai/kimi-k2-instruct\", model_provider=\"groq\")  \n",
    "\n",
    "    agente_search = create_react_agent(\n",
    "        llm_init, [tool_tavily, think_tool],\n",
    "        prompt=prompt_agent_pesquisador_format,\n",
    "    )\n",
    "    \n",
    "    response = await agente_search.ainvoke({\"messages\": [messages]})\n",
    "    \n",
    "    \n",
    "    return {\"feedback\": response['messages'][-1].content}\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "135f2a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 23.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 34.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 35.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "messages = \"Create the analysi of the dataset titanic\"\n",
    "error_message = \"nanan\"\n",
    "code = \"\"\n",
    "feedback = \"O código não funciona corretamente.\"\n",
    "\n",
    "prompt_agent_pesquisador_format = prompt_agent_pesquisador.format(\n",
    "        messages = messages,\n",
    "        error_message = error_message,\n",
    "        code = code,\n",
    "        feedback = feedback)\n",
    "    \n",
    "llm_init = init_chat_model(\"moonshotai/kimi-k2-instruct\", model_provider=\"groq\")  \n",
    "\n",
    "agente_search = create_react_agent(\n",
    "        llm_init, [tool_tavily, think_tool],\n",
    "        prompt=prompt_agent_pesquisador_format,\n",
    "    )\n",
    "    \n",
    "response = await agente_search.ainvoke({\"messages\": [messages]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c432cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Create the analysi of the dataset titanic', additional_kwargs={}, response_metadata={}, id='57036213-5696-47cb-9971-d9ad5026fde5'),\n",
       " AIMessage(content=\"I'll help you create an analysis of the Titanic dataset. Let me start by searching for comprehensive information about Titanic dataset analysis approaches and techniques.\", additional_kwargs={'tool_calls': [{'id': 'functions.tavily_search:0', 'function': {'arguments': '{\"query\":\"Titanic dataset analysis Python pandas data exploration techniques\"}', 'name': 'tavily_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 2229, 'total_tokens': 2284, 'completion_time': 0.21393095, 'prompt_time': 0.18128756, 'queue_time': 0.205976171, 'total_time': 0.39521851, 'prompt_tokens_details': {'cached_tokens': 2048}}, 'model_name': 'moonshotai/kimi-k2-instruct', 'system_fingerprint': 'fp_e86393ddfd', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--8bac4cdb-83ec-4b85-bd7d-859b14121ef7-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'Titanic dataset analysis Python pandas data exploration techniques'}, 'id': 'functions.tavily_search:0', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2229, 'output_tokens': 55, 'total_tokens': 2284}),\n",
       " ToolMessage(content='{\"query\": \"Titanic dataset analysis Python pandas data exploration techniques\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://daehnhardt.com/blog/2023/01/20/pandas-tutorial-with-titanic-dataset/\", \"title\": \"Data exploration and analysis with Python Pandas\", \"content\": \"The Titanic dataset is a good dataset for learning data manipulation and analysis techniques, as it has a relatively small size and is easy\", \"score\": 0.8482825, \"raw_content\": null}, {\"url\": \"https://dev.to/pavanbelagatti/data-analysis-of-the-titanic-with-python-koj\", \"title\": \"Data Analysis of the Titanic with Python!\", \"content\": \"# Data Analysis of the Titanic with Python! ## How Data Analysis Works? Notebooks have become increasingly popular in the data science community as they provide an efficient way to explore, analyze and visualize data, making it easier to communicate insights and results. SingleStore\\'s Notebook feature is based on the popular Jupyter Notebook, which is widely used in data science and machine learning communities. data = sns.load_dataset(\\\\\"titanic\\\\\") * **Let\\'s analyze survival rates based on passenger class.** sns.barplot(data=data, x=\\'class\\', y=\\'survived\\', hue=\\'sex\\') plt.title(\\'Survival Rate by Passenger Class and Gender\\') sns.countplot(data=data, x=\\'embarked\\', hue=\\'survived\\') * **Let\\'s analyze survival based on fare and class.** sns.boxplot(data=data, x=\\'class\\', y=\\'fare\\', hue=\\'survived\\') plt.title(\\'Fare distribution by Passenger Class and Survival\\') sns.countplot(data=data, x=\\'family_size\\', hue=\\'survived\\') sns.countplot(data=data, x=\\'sibsp\\', hue=\\'survived\\')\", \"score\": 0.7173491, \"raw_content\": null}, {\"url\": \"https://hex.tech/blog/explore-data-with-python-and-pandas/\", \"title\": \"Exploring Data in Jupyter with Python and Pandas - Hex\", \"content\": \"To gather the basic information about the dataset such as column names, column data types, number of rows, or number of Null values, you can use the `info()` method from Pandas. To access the Age column from titanic data, you can use the following line of code: An example of calculating the value counts in the `Sex` column of titanic data will look like this: titanic_data[\\'Sex\\'].value_counts() titanic_data.value_counts() To use the `groupby()` method, you need to provide the feature name or a list of feature names and an aggregate function based on which the data will be grouped together. For example, if you want to group the titanic data based on the `Sex` column and with `mean()` aggregation, you can do so as follows:\", \"score\": 0.70750624, \"raw_content\": null}, {\"url\": \"https://jkarakas.github.io/Exploratory-Analysis-of-the-Titanic-Dataset/Titanic_Dataset_Exploratory_Analysis._With_Code.html\", \"title\": \"Exploratory Analysis of the Titanic Dataset\", \"content\": \"|  | PassengerId | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked | |  | Survived | Pclass | Sex | Age | SibSp | Parch | Embarked | def Groupby_OneCol_comp_plot(df, col, plt_style = \\'seaborn-ticks\\', color_palette = \\\\\"coolwarm\\\\\"): ax=gr[\\'{} No\\'.format(col)].plot(kind=\\'bar\\', title=\\'{} Counts\\'.format(col), figsize=(16,8), color=sns.color_palette()) Let\\'s examine the percentages of passengers that survived ,depending on their port of embarkation. Groupby_TwoCol_Plot(descript,\\'Embarked\\', \\'Survived\\', color_palette=(\\'darkred\\',\\'steelblue\\'), plt_style = \\'seaborn-ticks\\', custom_title=\\'Proportion of Survived per Embarkation Port\\') Groupby_TwoCol_Plot(descript,\\'Embarked\\', \\'Pclass\\', color_palette=(\\'cubehelix\\'), plt_style = \\'seaborn-ticks\\', custom_title=\\'Proportion of Embarked per PcClass\\', sorter = [1,2,3]) Groupby_TwoCol_Plot(descript,\\'Pclass\\', \\'Survived\\', color_palette=(\\'darkred\\',\\'steelblue\\'), plt_style = \\'seaborn-ticks\\', custom_title=\\'Proportion of Survived per PcClass\\') Groupby_TwoCol_Plot(descript,\\'Embarked\\', \\'Sex\\', color_palette=(\\'lightpink\\',\\'green\\'), plt_style = \\'seaborn-ticks\\', custom_title=\\'Proportion of Sex per PcClass\\',\", \"score\": 0.69249696, \"raw_content\": null}, {\"url\": \"https://www.kaggle.com/code/aselad/exploratory-data-analysis-on-the-titanic-dataset\", \"title\": \"Exploratory Data Analysis on the Titanic Dataset\", \"content\": \"Exploratory Data Analysis on the Titanic Dataset Image 1: Kaggle *   emoji_events Competitions  *   table_chart Datasets  *   school Learn  Image 2: Kaggle Learn more Exploratory Data Analysis on the Titanic Dataset Exploratory Data Analysis on the Titanic Dataset COMPETITIONS Image 4 Exploratory Data Analysis on the Titanic Dataset1. Visually inspect the head of the dataset,Examine the train dataset to understand in particular if the data is tidy, shape of the dataset,examine datatypes, examine missing values, unique counts and build a data dictionary dataframe3. Preprocessing and Prepare data for statistical modeling8. Statistical Modelling Competition Notebook *   Image 6 Input 1 filearrow_right_alt   *   Image 7 Output 0 filesarrow_right_alt   *   Image 8 Logs 200.7 second run - successfularrow_right_alt   *   Image 9 Comments 7 commentsarrow_right_alt\", \"score\": 0.64047897, \"raw_content\": null}], \"response_time\": 0.93, \"request_id\": \"b20eddc9-3903-4238-894d-cd8c1c94bc18\"}', name='tavily_search', id='45c6b82a-fbd0-4a29-9f2c-f4e700e5c5e6', tool_call_id='functions.tavily_search:0'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'functions.think_tool:1', 'function': {'arguments': '{\"reflection\":\"Good initial search results! I found several useful resources about Titanic dataset analysis. Now I should search for more specific information about the dataset structure, common analysis patterns, and visualization techniques that are typically used with this dataset.\"}', 'name': 'think_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 60, 'prompt_tokens': 3648, 'total_tokens': 3708, 'completion_time': 0.25081864, 'prompt_time': 0.302010767, 'queue_time': 0.282109302, 'total_time': 0.552829407, 'prompt_tokens_details': {'cached_tokens': 2048}}, 'model_name': 'moonshotai/kimi-k2-instruct', 'system_fingerprint': 'fp_6e6ff3688b', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--bf8d035f-5425-4ea1-8f0f-248693f982c0-0', tool_calls=[{'name': 'think_tool', 'args': {'reflection': 'Good initial search results! I found several useful resources about Titanic dataset analysis. Now I should search for more specific information about the dataset structure, common analysis patterns, and visualization techniques that are typically used with this dataset.'}, 'id': 'functions.think_tool:1', 'type': 'tool_call'}], usage_metadata={'input_tokens': 3648, 'output_tokens': 60, 'total_tokens': 3708}),\n",
       " ToolMessage(content='Reflection recorded: Good initial search results! I found several useful resources about Titanic dataset analysis. Now I should search for more specific information about the dataset structure, common analysis patterns, and visualization techniques that are typically used with this dataset.', name='think_tool', id='a077c0cf-9c1f-4b0f-9db4-78cccd2afe8d', tool_call_id='functions.think_tool:1'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'functions.tavily_search:2', 'function': {'arguments': '{\"query\":\"Titanic dataset columns features description survival analysis machine learning\"}', 'name': 'tavily_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 3772, 'total_tokens': 3801, 'completion_time': 0.123557781, 'prompt_time': 0.043035306, 'queue_time': 0.296515049, 'total_time': 0.166593087, 'prompt_tokens_details': {'cached_tokens': 3584}}, 'model_name': 'moonshotai/kimi-k2-instruct', 'system_fingerprint': 'fp_6e6ff3688b', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--3a13b554-d48e-455c-a2fe-2d1b1662e9fd-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'Titanic dataset columns features description survival analysis machine learning'}, 'id': 'functions.tavily_search:2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 3772, 'output_tokens': 29, 'total_tokens': 3801}),\n",
       " ToolMessage(content='{\"query\": \"Titanic dataset columns features description survival analysis machine learning\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.analyticsvidhya.com/blog/2021/07/titanic-survival-prediction-using-machine-learning/\", \"title\": \"Titanic Survival Prediction Using Machine Learning\", \"content\": \"Hey Folks, in this article, we will be understanding, how to analyze and predict, whether a person, who had boarded the RMS Titanic has a chance of survival or not, using Machine Learning’s Logistic Regression model. In Machine Learning projects, we need a training **data set**. Our dataset is now stored in the variable named titanic\\\\\\\\_data. We have learned from Kaggle while downloading the data set, that the data has 891 rows and 12 columns. Now if we run the titanic\\\\\\\\_data.head() command again, we find that the values have been replaced successfully. ### Let’s split the data into the target and feature variables. Now, we will be splitting the data into four variables, namely, X\\\\\\\\_train, Y\\\\\\\\_train, X\\\\\\\\_test, Y\\\\\\\\_test.\", \"score\": 0.7253574, \"raw_content\": null}, {\"url\": \"https://www.geeksforgeeks.org/machine-learning/titanic-survival-prediction-using-ml/\", \"title\": \"Titanic Survival Prediction Using Machine Learning\", \"content\": \"In this project, we will leverage machine learning techniques to predict the survival chances of Titanic passengers based on various features, such as sex, age\", \"score\": 0.6729558, \"raw_content\": null}, {\"url\": \"https://dev.to/pavanbelagatti/data-analysis-of-the-titanic-with-python-koj\", \"title\": \"Data Analysis of the Titanic with Python!\", \"content\": \"# Data Analysis of the Titanic with Python! ## How Data Analysis Works? Notebooks have become increasingly popular in the data science community as they provide an efficient way to explore, analyze and visualize data, making it easier to communicate insights and results. SingleStore\\'s Notebook feature is based on the popular Jupyter Notebook, which is widely used in data science and machine learning communities. data = sns.load_dataset(\\\\\"titanic\\\\\") * **Let\\'s analyze survival rates based on passenger class.** sns.barplot(data=data, x=\\'class\\', y=\\'survived\\', hue=\\'sex\\') plt.title(\\'Survival Rate by Passenger Class and Gender\\') sns.countplot(data=data, x=\\'embarked\\', hue=\\'survived\\') * **Let\\'s analyze survival based on fare and class.** sns.boxplot(data=data, x=\\'class\\', y=\\'fare\\', hue=\\'survived\\') plt.title(\\'Fare distribution by Passenger Class and Survival\\') sns.countplot(data=data, x=\\'family_size\\', hue=\\'survived\\') sns.countplot(data=data, x=\\'sibsp\\', hue=\\'survived\\')\", \"score\": 0.599087, \"raw_content\": null}, {\"url\": \"https://github.com/Esai-Keshav/titanic-survival-prediction\", \"title\": \"Titanic Survival Prediction Using Machine Learning\", \"content\": \"Titanic Survival Prediction Using Machine Learning is a tool that uses data to guess if passengers survived the Titanic sinking. # Esai-Keshav/titanic-survival-prediction # Titanic Survival Prediction Using Machine Learning My project begins with **loading data** from CSV files using the **pandas** library, segregating the data into train and test datasets. My project involves rigorous **model testing** to determine the best approach for predicting passenger survival on the Titanic. At a predictive accuracy of **73.8%**, my model demonstrates its potential to forecast Titanic passenger survival effectively. This project not only illustrates the practical application of machine learning techniques on historical data but also provides insights into the influential factors behind survival rates during the Titanic disaster.\", \"score\": 0.5849357, \"raw_content\": null}, {\"url\": \"https://www.kaggle.com/datasets/yasserh/titanic-dataset\", \"title\": \"Titanic Dataset\", \"content\": \"Description: The sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage,\", \"score\": 0.58237284, \"raw_content\": null}], \"response_time\": 1.02, \"request_id\": \"fc64c679-e6fe-4f83-9dfd-5c1e579e476b\"}', name='tavily_search', id='d49927e4-3725-4612-936f-6ac54c5ad4b1', tool_call_id='functions.tavily_search:2'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'functions.tavily_search:3', 'function': {'arguments': '{\"query\":\"Titanic dataset seaborn visualization examples complete analysis Python code\"}', 'name': 'tavily_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 4781, 'total_tokens': 4811, 'completion_time': 0.22707133, 'prompt_time': 0.223043174, 'queue_time': 0.432620899, 'total_time': 0.450114504, 'prompt_tokens_details': {'cached_tokens': 2048}}, 'model_name': 'moonshotai/kimi-k2-instruct', 'system_fingerprint': 'fp_e86393ddfd', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--de33296c-b1db-4250-bb9c-33b6bc9a1a10-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'Titanic dataset seaborn visualization examples complete analysis Python code'}, 'id': 'functions.tavily_search:3', 'type': 'tool_call'}], usage_metadata={'input_tokens': 4781, 'output_tokens': 30, 'total_tokens': 4811}),\n",
       " ToolMessage(content='{\"query\": \"Titanic dataset seaborn visualization examples complete analysis Python code\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://dev.to/pavanbelagatti/data-analysis-of-the-titanic-with-python-koj\", \"title\": \"Data Analysis of the Titanic with Python!\", \"content\": \"# Data Analysis of the Titanic with Python! ## How Data Analysis Works? Notebooks have become increasingly popular in the data science community as they provide an efficient way to explore, analyze and visualize data, making it easier to communicate insights and results. SingleStore\\'s Notebook feature is based on the popular Jupyter Notebook, which is widely used in data science and machine learning communities. data = sns.load_dataset(\\\\\"titanic\\\\\") * **Let\\'s analyze survival rates based on passenger class.** sns.barplot(data=data, x=\\'class\\', y=\\'survived\\', hue=\\'sex\\') plt.title(\\'Survival Rate by Passenger Class and Gender\\') sns.countplot(data=data, x=\\'embarked\\', hue=\\'survived\\') * **Let\\'s analyze survival based on fare and class.** sns.boxplot(data=data, x=\\'class\\', y=\\'fare\\', hue=\\'survived\\') plt.title(\\'Fare distribution by Passenger Class and Survival\\') sns.countplot(data=data, x=\\'family_size\\', hue=\\'survived\\') sns.countplot(data=data, x=\\'sibsp\\', hue=\\'survived\\')\", \"score\": 0.7156829, \"raw_content\": null}, {\"url\": \"https://www.geeksforgeeks.org/machine-learning/python-titanic-data-eda-using-seaborn/\", \"title\": \"Python | Titanic Data EDA using Seaborn\", \"content\": \"# Python | Titanic Data EDA using Seaborn group = titanic.groupby([\\'Pclass\\', \\'Survived\\']) sns.factorplot(x =\\'Family_Size\\', y =\\'Survived\\', data = titanic) sns.factorplot(x =\\'Alone\\', y =\\'Survived\\', data = titanic) sns.factorplot(x =\\'Family_Size\\', y =\\'Survived\\', data = titanic) sns factorplot x = \\'Family_Size\\' y = \\'Survived\\' data = titanic sns.factorplot(x =\\'Alone\\', y =\\'Survived\\', data = titanic) sns factorplot x = \\'Alone\\' y = \\'Survived\\' data = titanic data = titanic) kind =\\'count\\', col =\\'Pclass\\', data = titanic) * The titanic data can be analyzed using many more graph techniques and also more column correlations, than, as described in this article. 8 min readTypes of Machine Learning 7 min readApplications of Machine Learning 9 min readRegularization in Machine Learning 15+ min read[100+ Machine Learning Projects with Source Code [2025]\", \"score\": 0.7018975, \"raw_content\": null}, {\"url\": \"https://ummesalmam.com/python-for-you/fundamentals-of-python/data-visualization-in-python-titanic-dataset/\", \"title\": \"Data Visualization in Python – Titanic dataset\", \"content\": \"Data Visualization in Python – Titanic dataset - Umme Salma M *   Python for YouMenu Toggle *   Python for Mathematics *   Python for Statistics Python for You *   Sets in Python *   Pandas Data Series in Python *   Pandas Data Frames in Python *   Visualization in Python – Iris dataset *   Visualization in Python – Titanic dataset *   Geoplots in Python *   Python for Mathematics *   Python for Statistics Data Visualization in Python – Titanic dataset *   Python for YouMenu Toggle *   Data Visualization in Python – Iris dataset *   Data Visualization in Python – Titanic dataset *   Pandas Data Frames in Python *   Pandas Data Series in Python *   Sets in Python *   Python for Mathematics *   Python for Statistics\", \"score\": 0.6301622, \"raw_content\": null}, {\"url\": \"https://tryolabs.com/blog/2017/03/16/pandas-seaborn-a-guide-to-handle-visualize-data-elegantly\", \"title\": \"Pandas & Seaborn - A guide to handle & visualize data in ...\", \"content\": \"pivot(index = \\'age\\',  columns = \\'class\\',  values = \\'fare\\') p_titanic. pivot_table(index = \\'age\\',  columns = \\'class\\',  values = \\'fare\\') pt_titanic. pivot_table(index = \\'embark_town\\',  columns = \\'age_group\\',  values = \\'fare\\',  aggfunc = np. This will create a DataFrame with MultiIndexed columns with two levels; the first one being the value (\\\\\"fare\\\\\", \\\\\"age\\\\\", \\\\\"survived\\\\\", etc.) and the second one, the \\\\\"age\\\\\\\\_group\\\\\". + We can take the `pt_titanic` DataFrame from the `pivot_table`, which held data of the median fares paid by passengers per embark\\\\\\\\_town per age\\\\\\\\_group, and build a heatmap very easily. pivot_table(index = \\'embark_town\\',  columns = \\'age_group\\',  values = \\'fare\\',  aggfunc = np.\", \"score\": 0.5931622, \"raw_content\": null}, {\"url\": \"https://www.kaggle.com/code/roblexnana/data-viz-tutorial-with-titanic-and-seaborn\", \"title\": \"Data viz tutorial with Titanic and seaborn\", \"content\": \"Data viz tutorial with Titanic and seaborn Image 1: Kaggle *   explore Home  *   emoji_events Competitions  *   school Learn  Image 2: Kaggle Learn more arrow_drop_up 26 Data viz tutorial with Titanic and seaborn Data viz tutorial with Titanic and seaborn play_arrow COMPETITIONS Image 3 Titanic - Machine Learning from Disaster Table of Contents Hands on data viz with Seaborn on Titanic datasetIntroduction1. Distribution plots3. Relational plots4. Categorical Plot5. Regression plots6. Matrix plots7. Competition Notebook Titanic - Machine Learning from Disaster This Notebook has been released under the Apache 2.0 open source license. Continue exploring *   Image 5 Input 1 filearrow_right_alt   *   Image 6 Output 0 filesarrow_right_alt   *   Image 7 Logs 196.3 second run - successfularrow_right_alt   *   Image 8 Comments 2 commentsarrow_right_alt\", \"score\": 0.5809471, \"raw_content\": null}], \"response_time\": 1.15, \"request_id\": \"bb9651bf-f446-4311-a42d-39a547da7884\"}', name='tavily_search', id='a2a83158-39be-40fb-81c0-43b02aa194d9', tool_call_id='functions.tavily_search:3'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'functions.think_tool:4', 'function': {'arguments': '{\"reflection\":\"Now I have comprehensive information about Titanic dataset analysis. I have found:\\\\n\\\\n1. Multiple detailed tutorials showing different analysis approaches\\\\n2. Information about the dataset structure (891 rows, 12 columns)\\\\n3. Common visualization techniques using seaborn and matplotlib\\\\n4. Key analysis patterns: survival by class, gender, age, fare, embarkation port\\\\n5. Common preprocessing and feature engineering techniques\\\\n\\\\nI have enough information to create a complete, comprehensive analysis of the Titanic dataset. Let me proceed with creating the analysis code that covers:\\\\n- Data loading and exploration\\\\n- Missing value analysis\\\\n- Descriptive statistics\\\\n- Key visualizations\\\\n- Feature engineering\\\\n- Survival analysis by different factors\\\\n\\\\nThis should provide a thorough analysis that covers all the essential aspects of the Titanic dataset.\"}', 'name': 'think_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 178, 'prompt_tokens': 6221, 'total_tokens': 6399, 'completion_time': 4.906800901, 'prompt_time': 0.389709549, 'queue_time': 0.2060801, 'total_time': 5.2965104499999995, 'prompt_tokens_details': {'cached_tokens': 4608}}, 'model_name': 'moonshotai/kimi-k2-instruct', 'system_fingerprint': 'fp_e86393ddfd', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--1fe92be1-51ab-426b-888b-71f915d98048-0', tool_calls=[{'name': 'think_tool', 'args': {'reflection': 'Now I have comprehensive information about Titanic dataset analysis. I have found:\\n\\n1. Multiple detailed tutorials showing different analysis approaches\\n2. Information about the dataset structure (891 rows, 12 columns)\\n3. Common visualization techniques using seaborn and matplotlib\\n4. Key analysis patterns: survival by class, gender, age, fare, embarkation port\\n5. Common preprocessing and feature engineering techniques\\n\\nI have enough information to create a complete, comprehensive analysis of the Titanic dataset. Let me proceed with creating the analysis code that covers:\\n- Data loading and exploration\\n- Missing value analysis\\n- Descriptive statistics\\n- Key visualizations\\n- Feature engineering\\n- Survival analysis by different factors\\n\\nThis should provide a thorough analysis that covers all the essential aspects of the Titanic dataset.'}, 'id': 'functions.think_tool:4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 6221, 'output_tokens': 178, 'total_tokens': 6399}),\n",
       " ToolMessage(content='Reflection recorded: Now I have comprehensive information about Titanic dataset analysis. I have found:\\n\\n1. Multiple detailed tutorials showing different analysis approaches\\n2. Information about the dataset structure (891 rows, 12 columns)\\n3. Common visualization techniques using seaborn and matplotlib\\n4. Key analysis patterns: survival by class, gender, age, fare, embarkation port\\n5. Common preprocessing and feature engineering techniques\\n\\nI have enough information to create a complete, comprehensive analysis of the Titanic dataset. Let me proceed with creating the analysis code that covers:\\n- Data loading and exploration\\n- Missing value analysis\\n- Descriptive statistics\\n- Key visualizations\\n- Feature engineering\\n- Survival analysis by different factors\\n\\nThis should provide a thorough analysis that covers all the essential aspects of the Titanic dataset.', name='think_tool', id='f9b0abe0-1362-4a3b-97d4-18f9693660f4', tool_call_id='functions.think_tool:4'),\n",
       " AIMessage(content='Based on my research, I\\'ll create a comprehensive analysis of the Titanic dataset. Here\\'s a complete Python analysis that covers all the essential aspects:\\n\\n```python\\n# Titanic Dataset Analysis\\n# Import necessary libraries\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom scipy import stats\\nimport warnings\\nwarnings.filterwarnings(\\'ignore\\')\\n\\n# Set visualization style\\nplt.style.use(\\'seaborn-v0_8-darkgrid\\')\\nsns.set_palette(\"husl\")\\n\\n# Load the Titanic dataset\\n# Using seaborn\\'s built-in dataset\\ntitanic = sns.load_dataset(\\'titanic\\')\\n\\n# Display basic information about the dataset\\nprint(\"=== TITANIC DATASET ANALYSIS ===\")\\nprint(f\"Dataset Shape: {titanic.shape}\")\\nprint(f\"Columns: {list(titanic.columns)}\")\\nprint(\"\\\\n=== FIRST 5 ROWS ===\")\\nprint(titanic.head())\\n\\nprint(\"\\\\n=== DATASET INFO ===\")\\nprint(titanic.info())\\n\\nprint(\"\\\\n=== MISSING VALUES ANALYSIS ===\")\\nmissing_data = titanic.isnull().sum()\\nmissing_percent = (missing_data / len(titanic)) * 100\\nmissing_df = pd.DataFrame({\\n    \\'Missing Count\\': missing_data,\\n    \\'Percentage\\': missing_percent\\n}).sort_values(\\'Percentage\\', ascending=False)\\nprint(missing_df[missing_df[\\'Missing Count\\'] > 0])\\n\\n# Create a figure for missing values visualization\\nplt.figure(figsize=(10, 6))\\nsns.barplot(x=missing_df[missing_df[\\'Missing Count\\'] > 0].index, \\n            y=missing_df[missing_df[\\'Missing Count\\'] > 0][\\'Percentage\\'])\\nplt.title(\\'Missing Values by Column\\')\\nplt.xticks(rotation=45)\\nplt.ylabel(\\'Percentage Missing (%)\\')\\nplt.tight_layout()\\nplt.show()\\n\\nprint(\"\\\\n=== BASIC STATISTICS ===\")\\nprint(titanic.describe())\\n\\nprint(\"\\\\n=== CATEGORICAL VARIABLES ANALYSIS ===\")\\ncategorical_cols = [\\'sex\\', \\'embarked\\', \\'class\\', \\'who\\', \\'adult_male\\', \\'alone\\']\\nfor col in categorical_cols:\\n    if col in titanic.columns:\\n        print(f\"\\\\n{col.upper()} Value Counts:\")\\n        print(titanic[col].value_counts())\\n\\n# Feature Engineering\\nprint(\"\\\\n=== FEATURE ENGINEERING ===\")\\n\\n# Create age groups\\ntitanic[\\'age_group\\'] = pd.cut(titanic[\\'age\\'], \\n                              bins=[0, 12, 18, 35, 60, 100], \\n                              labels=[\\'Child\\', \\'Teenager\\', \\'Young Adult\\', \\'Adult\\', \\'Senior\\'])\\n\\n# Create family size\\ntitanic[\\'family_size\\'] = titanic[\\'sibsp\\'] + titanic[\\'parch\\'] + 1\\ntitanic[\\'is_alone\\'] = (titanic[\\'family_size\\'] == 1).astype(int)\\n\\n# Create fare per person\\ntitanic[\\'fare_per_person\\'] = titanic[\\'fare\\'] / titanic[\\'family_size\\']\\n\\nprint(\"New features created: age_group, family_size, is_alone, fare_per_person\")\\n\\n# Survival Rate Analysis\\nprint(\"\\\\n=== OVERALL SURVIVAL RATE ===\")\\noverall_survival_rate = titanic[\\'survived\\'].mean()\\nprint(f\"Overall Survival Rate: {overall_survival_rate:.2%}\")\\n\\n# Create comprehensive survival analysis visualizations\\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\\n\\n# 1. Survival by Gender\\nsns.barplot(data=titanic, x=\\'sex\\', y=\\'survived\\', ax=axes[0,0])\\naxes[0,0].set_title(\\'Survival Rate by Gender\\')\\naxes[0,0].set_ylabel(\\'Survival Rate\\')\\n\\n# 2. Survival by Passenger Class\\nsns.barplot(data=titanic, x=\\'class\\', y=\\'survived\\', ax=axes[0,1])\\naxes[0,1].set_title(\\'Survival Rate by Passenger Class\\')\\naxes[0,1].set_ylabel(\\'Survival Rate\\')\\n\\n# 3. Survival by Age Group\\nsns.barplot(data=titanic, x=\\'age_group\\', y=\\'survived\\', ax=axes[1,0])\\naxes[1,0].set_title(\\'Survival Rate by Age Group\\')\\naxes[1,0].set_ylabel(\\'Survival Rate\\')\\naxes[1,0].tick_params(axis=\\'x\\', rotation=45)\\n\\n# 4. Survival by Embarkation Port\\nsns.barplot(data=titanic, x=\\'embarked\\', y=\\'survived\\', ax=axes[1,1])\\naxes[1,1].set_title(\\'Survival Rate by Embarkation Port\\')\\naxes[1,1].set_ylabel(\\'Survival Rate\\')\\n\\nplt.tight_layout()\\nplt.show()\\n\\n# Detailed survival analysis by multiple factors\\nprint(\"\\\\n=== DETAILED SURVIVAL ANALYSIS ===\")\\n\\n# Gender and Class interaction\\nprint(\"\\\\nSurvival Rate by Gender and Class:\")\\ngender_class_survival = titanic.groupby([\\'sex\\', \\'class\\'])[\\'survived\\'].agg([\\'mean\\', \\'count\\'])\\nprint(gender_class_survival)\\n\\n# Visualization for Gender and Class\\nplt.figure(figsize=(10, 6))\\nsns.barplot(data=titanic, x=\\'class\\', y=\\'survived\\', hue=\\'sex\\')\\nplt.title(\\'Survival Rate by Passenger Class and Gender\\')\\nplt.ylabel(\\'Survival Rate\\')\\nplt.show()\\n\\n# Age analysis\\nprint(\"\\\\n=== AGE ANALYSIS ===\")\\nprint(f\"Average age: {titanic[\\'age\\'].mean():.1f} years\")\\nprint(f\"Median age: {titanic[\\'age\\'].median():.1f} years\")\\n\\n# Age distribution by survival\\nplt.figure(figsize=(12, 5))\\n\\nplt.subplot(1, 2, 1)\\nsns.histplot(data=titanic, x=\\'age\\', hue=\\'survived\\', kde=True, bins=30)\\nplt.title(\\'Age Distribution by Survival\\')\\n\\nplt.subplot(1, 2, 2)\\nsns.boxplot(data=titanic, x=\\'survived\\', y=\\'age\\')\\nplt.title(\\'Age Distribution by Survival Status\\')\\nplt.xlabel(\\'Survived (0=No, 1=Yes)\\')\\n\\nplt.tight_layout()\\nplt.show()\\n\\n# Fare analysis\\nprint(\"\\\\n=== FARE ANALYSIS ===\")\\nprint(f\"Average fare: ${titanic[\\'fare\\'].mean():.2f}\")\\nprint(f\"Median fare: ${titanic[\\'fare\\'].median():.2f}\")\\n\\n# Fare distribution by class and survival\\nplt.figure(figsize=(12, 5))\\n\\nplt.subplot(1, 2, 1)\\nsns.boxplot(data=titanic, x=\\'class\\', y=\\'fare\\', hue=\\'survived\\')\\nplt.title(\\'Fare Distribution by Class and Survival\\')\\nplt.ylabel(\\'Fare ($)\\')\\n\\nplt.subplot(1, 2, 2)\\nsns.scatterplot(data=titanic, x=\\'fare\\', y=\\'survived\\', alpha=0.6)\\nplt.title(\\'Fare vs Survival\\')\\nplt.ylabel(\\'Survived\\')\\n\\nplt.tight_layout()\\nplt.show()\\n\\n# Family size analysis\\nprint(\"\\\\n=== FAMILY SIZE ANALYSIS ===\")\\nfamily_survival = titanic.groupby(\\'family_size\\')[\\'survived\\'].agg([\\'mean\\', \\'count\\'])\\nprint(\"Survival rate by family size:\")\\nprint(family_survival)\\n\\nplt.figure(figsize=(10, 6))\\nsns.barplot(data=titanic, x=\\'family_size\\', y=\\'survived\\')\\nplt.title(\\'Survival Rate by Family Size\\')\\nplt.xlabel(\\'Family Size\\')\\nplt.ylabel(\\'Survival Rate\\')\\nplt.show()\\n\\n# Correlation analysis\\nprint(\"\\\\n=== CORRELATION ANALYSIS ===\")\\n\\n# Select numeric columns for correlation\\nnumeric_cols = [\\'survived\\', \\'pclass\\', \\'age\\', \\'sibsp\\', \\'parch\\', \\'fare\\']\\ncorrelation_matrix = titanic[numeric_cols].corr()\\n\\nplt.figure(figsize=(10, 8))\\nsns.heatmap(correlation_matrix, annot=True, cmap=\\'coolwarm\\', center=0, \\n            square=True, linewidths=0.5)\\nplt.title(\\'Correlation Matrix of Key Variables\\')\\nplt.show()\\n\\nprint(\"Correlations with Survival:\")\\nsurvival_corr = correlation_matrix[\\'survived\\'].sort_values(ascending=False)\\nfor var, corr in survival_corr.items():\\n    if var != \\'survived\\':\\n        print(f\"{var}: {corr:.3f}\")\\n\\n# Statistical significance tests\\nprint(\"\\\\n=== STATISTICAL SIGNIFICANCE TESTS ===\")\\n\\n# Chi-square test for categorical variables\\nfrom scipy.stats import chi2_contingency\\n\\n# Test for gender and survival\\ncontingency_table = pd.crosstab(titanic[\\'sex\\'], titanic[\\'survived\\'])\\nchi2, p_value, dof, expected = chi2_contingency(contingency_table)\\nprint(f\"\\\\nChi-square test for Gender vs Survival:\")\\nprint(f\"Chi-square statistic: {chi2:.4f}\")\\nprint(f\"P-value: {p_value:.4f}\")\\nprint(f\"Significant: {\\'Yes\\' if p_value < 0.05 else \\'No\\'}\")\\n\\n# Test for class and survival\\ncontingency_table = pd.crosstab(titanic[\\'pclass\\'], titanic[\\'survived\\'])\\nchi2, p_value, dof, expected = chi2_contingency(contingency_table)\\nprint(f\"\\\\nChi-square test for Class vs Survival:\")\\nprint(f\"Chi-square statistic: {chi2:.4f}\")\\nprint(f\"P-value: {p_value:.4f}\")\\nprint(f\"Significant: {\\'Yes\\' if p_value < 0.05 else \\'No\\'}\")\\n\\n# T-test for continuous variables\\nfrom scipy.stats import ttest_ind\\n\\n# Test for age difference between survivors and non-survivors\\nsurvivors_age = titanic[titanic[\\'survived\\'] == 1][\\'age\\'].dropna()\\nnon_survivors_age = titanic[titanic[\\'survived\\'] == 0][\\'age\\'].dropna()\\nt_stat, p_value = ttest_ind(survivors_age, non_survivors_age)\\nprint(f\"\\\\nT-test for Age difference:\")\\nprint(f\"T-statistic: {t_stat:.4f}\")\\nprint(f\"P-value: {p_value:.4f}\")\\nprint(f\"Significant: {\\'Yes\\' if p_value < 0.05 else \\'No\\'}\")\\n\\n# Summary insights\\nprint(\"\\\\n=== KEY INSIGHTS ===\")\\nprint(\"1. Gender Impact:\")\\nprint(f\"   - Female survival rate: {titanic[titanic[\\'sex\\']==\\'female\\'][\\'survived\\'].mean():.2%}\")\\nprint(f\"   - Male survival rate: {titanic[titanic[\\'sex\\']==\\'male\\'][\\'survived\\'].mean():.2%}\")\\n\\nprint(\"\\\\n2. Class Impact:\")\\nfor cls in [\\'First\\', \\'Second\\', \\'Third\\']:\\n    class_survival = titanic[titanic[\\'class\\'] == cls][\\'survived\\'].mean()\\n    print(f\"   - {cls} class survival rate: {class_survival:.2%}\")\\n\\nprint(\"\\\\n3. Age Impact:\")\\nprint(f\"   - Average age of survivors: {survivors_age.mean():.1f} years\")\\nprint(f\"   - Average age of non-survivors: {non_survivors_age.mean():.1f} years\")\\n\\nprint(\"\\\\n4. Family Size Impact:\")\\nprint(f\"   - Traveling alone survival rate: {titanic[titanic[\\'is_alone\\']==1][\\'survived\\'].mean():.2%}\")\\nprint(f\"   - Traveling with family survival rate: {titanic[titanic[\\'is_alone\\']==0][\\'survived\\'].mean():.2%}\")\\n\\nprint(\"\\\\n=== ANALYSIS COMPLETE ===\")\\nprint(\"This comprehensive analysis reveals that survival on the Titanic was significantly\")\\nprint(\"influenced by gender (females had much higher survival rates), passenger class\")\\nprint(\"(first class had better survival rates), and age (younger passengers had\")\\nprint(\"slightly better survival rates).\")\\n```\\n\\nThis comprehensive analysis includes:\\n\\n1. **Data Loading & Exploration**: Basic dataset information, shape, columns, and missing values analysis\\n2. **Feature Engineering**: Creating new features like age groups, family size, and fare per person\\n3. **Survival Analysis**: Detailed survival rate analysis by different factors\\n4. **Visualizations**: Multiple charts including bar plots, histograms, box plots, and heatmaps\\n5. **Statistical Tests**: Chi-square tests for categorical variables and t-tests for continuous variables\\n6. **Key Insights**: Summary of the most important findings\\n\\nThe analysis reveals critical patterns such as:\\n- Strong gender bias in survival (females much more likely to survive)\\n- Clear class-based survival differences (first class had better survival rates)\\n- Age impact on survival (younger passengers had slightly better chances)\\n- Family size influence on survival rates\\n\\nThis code provides a complete, production-ready analysis that can be run in any Python environment with the required libraries installed.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 2772, 'prompt_tokens': 6575, 'total_tokens': 9347, 'completion_time': 18.9823752, 'prompt_time': 0.218748933, 'queue_time': 0.438239272, 'total_time': 19.201124133, 'prompt_tokens_details': {'cached_tokens': 3584}}, 'model_name': 'moonshotai/kimi-k2-instruct', 'system_fingerprint': 'fp_6e6ff3688b', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--5a3b8947-9cf9-4077-a2d9-f399da597ebf-0', usage_metadata={'input_tokens': 6575, 'output_tokens': 2772, 'total_tokens': 9347})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['messages']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d228e1",
   "metadata": {},
   "source": [
    "## Criando resposta final para o usuário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f316f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def think_response(reflection: str) -> str:\n",
    "    \n",
    "    \"\"\"Tool for strategic reflection on research progress and decision-making.\n",
    "    \n",
    "    Use this tool after each search to analyze results and plan next steps systematically.\n",
    "    This creates a deliberate pause in the research workflow for quality decision-making.\n",
    "    \n",
    "    When to use:\n",
    "    - After receiving search results: What key information did I find?\n",
    "    - Before deciding next steps: Do I have enough to answer comprehensively?\n",
    "    - When assessing research gaps: What specific information am I still missing?\n",
    "    - Before concluding research: Can I provide a complete answer now?\n",
    "    \n",
    "    Reflection should address:\n",
    "    1. Analysis of current findings - What concrete information have I gathered?\n",
    "    2. Gap assessment - What crucial information is still missing?\n",
    "    3. Quality evaluation - Do I have sufficient evidence/examples for a good answer?\n",
    "    4. Strategic decision - Should I continue searching or provide my answer?\n",
    "    \n",
    "    Args:\n",
    "        reflection: Your detailed reflection on research progress, findings, gaps, and next steps\n",
    "        \n",
    "    Returns:\n",
    "        Confirmation that reflection was recorded for decision-making\n",
    "    \"\"\"\n",
    "    \n",
    "    return f\"Reflection recorded: {reflection}\" \n",
    "\n",
    "@tool\n",
    "def think_response(reflection: str) -> str:\n",
    "    \"\"\"Tool for strategic thinking and analysis before delivering final coding response.\n",
    "    \n",
    "    Use this tool to process all gathered information and plan the final response delivery.\n",
    "    This creates a deliberate pause in the coding workflow for quality analysis and synthesis.\n",
    "    \n",
    "    When to use:\n",
    "    - After collecting all necessary information about the coding task\n",
    "    - Before writing the final code solution\n",
    "    - When analyzing requirements and constraints\n",
    "    - Before structuring the complete response\n",
    "    \n",
    "    Analysis should address:\n",
    "    1. Requirements analysis - What exactly needs to be implemented?\n",
    "    2. Solution approach - What's the best technical approach?\n",
    "    3. Code structure - How should the solution be organized?\n",
    "    4. Edge cases and considerations - What potential issues need handling?\n",
    "    5. Explanation strategy - How to best explain the solution?\n",
    "    6. Response completeness - Do I have all components for a full answer?\n",
    "    \n",
    "    Args:\n",
    "        analysis: Your detailed analysis of the coding task, approach, implementation plan, \n",
    "                 and response structure before delivering the final solution\n",
    "        \n",
    "    Returns:\n",
    "        Confirmation that analysis was recorded for final response preparation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🤔 AGENTE PENSANDO...\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"-\" * 50)\n",
    "    print(\"✅ Análise concluída. Preparando resposta final...\")\n",
    "    \n",
    "    return f\"Analysis processed: Ready to deliver comprehensive coding solution based on: {reflection}\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e81bdf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def return_messages(state: StateCode) -> StateCode:\n",
    "    \n",
    "    messages = state.get(\"messages\")\n",
    "    \n",
    "    code = state.get(\"code\")\n",
    "    \n",
    "    feedback = state.get(\"feedback\")\n",
    "    \n",
    "    if messages:\n",
    "        messages = messages[-1]\n",
    "        \n",
    "    \n",
    "    return_messagem_prompt_format = return_messages_prompt.format(messages=messages, code=code, feedback=feedback)\n",
    "    \n",
    "    \n",
    "    \"\"\"router_structured = LlmRouter(\n",
    "            return_messagem_prompt_format,\n",
    "        )\"\"\"\n",
    "    \n",
    "    agent_response = create_react_agent(\n",
    "        llm_init, [think_response],\n",
    "        prompt=return_messagem_prompt_format\n",
    "    )\n",
    "    \n",
    "    #response = await router_structured.llm_router()\n",
    "    response = await agent_response.ainvoke({\"messages\": messages})\n",
    "    \n",
    "    if isinstance(response, dict):\n",
    "        return {\"messages\": [response['messages'][-1]]}\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19e410a",
   "metadata": {},
   "source": [
    "# Criando o graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b31df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "def should_continue(state: StateCode) -> str:\n",
    "    \n",
    "    interactions = state.get(\"interactions\", 0)\n",
    "    \n",
    "    valid = state.get(\"valid\")\n",
    "    \n",
    "    print(f\"Debug - interactions: {interactions}, valid: {valid}\")  # Para debug\n",
    "    \n",
    "    # Se o código está válido, terminar\n",
    "    if valid is True:\n",
    "        #return \"END\"\n",
    "        return \"return_messages\"\n",
    "    \n",
    "    # Se atingiu o máximo de tentativas sem sucesso, buscar ajuda\n",
    "    if interactions >= 5:\n",
    "        return \"return_messages\"  \n",
    "    \n",
    "    # Se falhou 3 vezes, buscar ajuda\n",
    "    if valid is False and interactions == 3:\n",
    "        return \"search\"\n",
    "    \n",
    "    # Caso contrário, continuar codificando\n",
    "    return \"code\"\n",
    "\n",
    "workflow_code = StateGraph(StateCode)\n",
    "\n",
    "workflow_code.add_node(node=\"code\", action=node_code)\n",
    "\n",
    "workflow_code.add_node(\"supervisor\", node_supervisor)\n",
    "\n",
    "workflow_code.add_node(\"check\", \n",
    "                       python_repl\n",
    "                       )\n",
    "\n",
    "workflow_code.add_node(\"search\", node_search)\n",
    "\n",
    "workflow_code.add_node(\"return_messages\", return_messages)  \n",
    "\n",
    "workflow_code.add_edge(START, \"code\")\n",
    "\n",
    "workflow_code.add_edge(\"code\", \"check\")\n",
    "\n",
    "workflow_code.add_edge(\"check\", \"supervisor\")\n",
    "\n",
    "workflow_code.add_edge(\"search\", \"code\")\n",
    "\n",
    "workflow_code.add_edge(\"return_messages\", END)\n",
    "\n",
    "workflow_code.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    should_continue,\n",
    "    {\"code\": \"code\",  \"search\": \"search\", \"return_messages\" : \"return_messages\"},\n",
    "    \n",
    ")\n",
    "memory = MemorySaver()\n",
    "\n",
    "app_code = workflow_code.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35169cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f52eef9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao exibir o diagrama: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 204.\n",
      "\n",
      "To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Show the agent\n",
    "try:\n",
    "    display(Image(app_code.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao exibir o diagrama: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a98d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"user_id\": \"1\", \"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41f092c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrei no code\n",
      "state [HumanMessage(content=' Faca uma funcao em python de fibonacci\\n\\n                ', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Iniciando roteamento LLM\n",
      "INFO:__main__:Tentando modelo Groq: moonshotai/kimi-k2-instruct-0905\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Sucesso com modelo Groq: moonshotai/kimi-k2-instruct-0905\n",
      "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n",
      "INFO:__main__:Iniciando roteamento LLM\n",
      "INFO:__main__:Tentando modelo Groq: moonshotai/kimi-k2-instruct-0905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECKING CODE---\n",
      "Running the Python REPL tool\n",
      "def fibonacci(n):\n",
      "    \"\"\"\n",
      "    Calculate the nth Fibonacci number.\n",
      "    \n",
      "    Args:\n",
      "        n (int): Position in the Fibonacci sequence (0-indexed)\n",
      "    \n",
      "    Returns:\n",
      "        int: The nth Fibonacci number\n",
      "    \"\"\"\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    \n",
      "    prev2, prev1 = 0, 1\n",
      "    \n",
      "    for _ in range(2, n + 1):\n",
      "        current = prev1 + prev2\n",
      "        prev2 = prev1\n",
      "        prev1 = current\n",
      "    \n",
      "    return prev1\n",
      "\n",
      "Entrei no supervisor\n",
      "state [HumanMessage(content=' Faca uma funcao em python de fibonacci\\n\\n                ', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Sucesso com modelo Groq: moonshotai/kimi-k2-instruct-0905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'valid': True, 'feedback': 'Strengths: Iterative O(n) time / O(1) space implementation, clear docstring, follows PEP-8 naming, handles base cases.\\nWeaknesses: No type hints, no input validation (negative or non-integers raise confusing errors), docstring says \"0-indexed\" but standard Fibonacci is usually defined for n≥0 with F(0)=0, F(1)=1.\\nSuggestions: 1) Add type hints (n: int) → int. 2) Validate n ≥ 0, raise ValueError otherwise. 3) Clarify docstring: \"Returns the nth Fibonacci number with F(0)=0, F(1)=1.\" 4) Optionally expose optional parameter to return whole sequence up to n.\\nVerdict: Acceptable after adding input validation and type hints.'}\n",
      "Debug - interactions: 1, valid: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤔 AGENTE PENSANDO...\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "✅ Análise concluída. Preparando resposta final...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "inputs = {\"messages\": [HumanMessage(content=\n",
    "                 r\"\"\" Faca uma funcao em python de fibonacci\n",
    "                \n",
    "                \"\"\"\n",
    "    )],\n",
    "    \"feedback\": \"\", \n",
    "    \"interactions\": 0\n",
    "          \n",
    "          }\n",
    "\n",
    "input = StateCode(**inputs)\n",
    "\n",
    "\n",
    "\n",
    "result = await app_code.ainvoke(input, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98026898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=' Faca uma funcao em python de fibonacci\\n\\n                ', additional_kwargs={}, response_metadata={}, id='15748433-e292-4013-9fdc-477d7c8a4359'),\n",
       " AIMessage(content='Claro! Aqui está uma função de Fibonacci em Python com validação de entrada e type hints:\\n\\n```python\\ndef fibonacci(n: int) -> int:\\n    \"\"\"\\n    Calcula o n-ésimo número de Fibonacci.\\n    \\n    Args:\\n        n (int): Posição na sequência de Fibonacci (deve ser >= 0)\\n    \\n    Returns:\\n        int: O n-ésimo número de Fibonacci (F(0)=0, F(1)=1)\\n    \\n    Raises:\\n        ValueError: Se n for negativo\\n    \"\"\"\\n    if not isinstance(n, int):\\n        raise TypeError(\"n deve ser um número inteiro\")\\n    if n < 0:\\n        raise ValueError(\"n deve ser não negativo\")\\n    \\n    if n <= 1:\\n        return n\\n    \\n    prev2, prev1 = 0, 1\\n    \\n    for _ in range(2, n + 1):\\n        current = prev1 + prev2\\n        prev2, prev1 = prev1, current\\n    \\n    return prev1\\n\\n\\n# Exemplos de uso\\nif __name__ == \"__main__\":\\n    # Testando a função\\n    for i in range(10):\\n        print(f\"F({i}) = {fibonacci(i)}\")\\n    \\n    # Saída:\\n    # F(0) = 0\\n    # F(1) = 1\\n    # F(2) = 1\\n    # F(3) = 2\\n    # F(4) = 3\\n    # F(5) = 5\\n    # F(6) = 8\\n    # F(7) = 13\\n    # F(8) = 21\\n    # F(9) = 34\\n```\\n\\n**Principais características:**\\n- **Eficiente:** O(n) tempo, O(1) espaço\\n- **Segura:** Valida entrada (rejeita números negativos)\\n- **Clara:** Com type hints e documentação\\n- **Simples:** Implementação iterativa\\n\\n**Para testar:** Salve o código em um arquivo `.py` e execute-o diretamente para ver os primeiros 10 números de Fibonacci.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 482, 'prompt_tokens': 1303, 'total_tokens': 1785, 'completion_time': 2.046855775, 'prompt_time': 0.26696283, 'queue_time': 0.772049894, 'total_time': 2.313818605}, 'model_name': 'moonshotai/kimi-k2-instruct', 'system_fingerprint': 'fp_e86393ddfd', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--412417dc-a993-4432-b6ac-82d1089d1e4a-0', usage_metadata={'input_tokens': 1303, 'output_tokens': 482, 'total_tokens': 1785})]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "520f5123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      " Faca uma funcao em python de fibonacci\n",
      "\n",
      "                \n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Claro! Aqui está uma função de Fibonacci em Python com validação de entrada e type hints:\n",
      "\n",
      "```python\n",
      "def fibonacci(n: int) -> int:\n",
      "    \"\"\"\n",
      "    Calcula o n-ésimo número de Fibonacci.\n",
      "    \n",
      "    Args:\n",
      "        n (int): Posição na sequência de Fibonacci (deve ser >= 0)\n",
      "    \n",
      "    Returns:\n",
      "        int: O n-ésimo número de Fibonacci (F(0)=0, F(1)=1)\n",
      "    \n",
      "    Raises:\n",
      "        ValueError: Se n for negativo\n",
      "    \"\"\"\n",
      "    if not isinstance(n, int):\n",
      "        raise TypeError(\"n deve ser um número inteiro\")\n",
      "    if n < 0:\n",
      "        raise ValueError(\"n deve ser não negativo\")\n",
      "    \n",
      "    if n <= 1:\n",
      "        return n\n",
      "    \n",
      "    prev2, prev1 = 0, 1\n",
      "    \n",
      "    for _ in range(2, n + 1):\n",
      "        current = prev1 + prev2\n",
      "        prev2, prev1 = prev1, current\n",
      "    \n",
      "    return prev1\n",
      "\n",
      "\n",
      "# Exemplos de uso\n",
      "if __name__ == \"__main__\":\n",
      "    # Testando a função\n",
      "    for i in range(10):\n",
      "        print(f\"F({i}) = {fibonacci(i)}\")\n",
      "    \n",
      "    # Saída:\n",
      "    # F(0) = 0\n",
      "    # F(1) = 1\n",
      "    # F(2) = 1\n",
      "    # F(3) = 2\n",
      "    # F(4) = 3\n",
      "    # F(5) = 5\n",
      "    # F(6) = 8\n",
      "    # F(7) = 13\n",
      "    # F(8) = 21\n",
      "    # F(9) = 34\n",
      "```\n",
      "\n",
      "**Principais características:**\n",
      "- **Eficiente:** O(n) tempo, O(1) espaço\n",
      "- **Segura:** Valida entrada (rejeita números negativos)\n",
      "- **Clara:** Com type hints e documentação\n",
      "- **Simples:** Implementação iterativa\n",
      "\n",
      "**Para testar:** Salve o código em um arquivo `.py` e execute-o diretamente para ver os primeiros 10 números de Fibonacci.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "state = app_code.get_state(config)\n",
    "state\n",
    "\n",
    "ChatPromptTemplate.from_messages(state.values[\"messages\"]).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10b35cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_agent.get_routem_llm.routem_llm import LlmRouter\n",
    "from code_agent.states_outputs.output_structured import CodeOutput, SupervisorResponse, SearchResponse\n",
    "from code_agent.states_outputs.states import StateCode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ac3d9b",
   "metadata": {},
   "source": [
    "# Graph Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1832a345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jefer\\Documents\\Ciencia-de-dados\\LLMs\\AgenteCodificaoLangGraph\\.venv\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:229: UserWarning: Found qwen/qwen3-coder-480b-a35b-instruct in available_models, but type is unknown and inference may fail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from code_agent.build_graph.graph import GraphBuilder\n",
    "from code_agent.states_outputs.states import StateCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cdd17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_code = GraphBuilder().compile_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "071c9dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"user_id\": \"1\", \"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df390fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "#display(Image(app_code.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeb902d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:code_agent.get_routem_llm.routem_llm:Iniciando roteamento LLM\n",
      "INFO:code_agent.get_routem_llm.routem_llm:Tentando modelo Groq: moonshotai/kimi-k2-instruct-0905\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "WARNING:code_agent.get_routem_llm.routem_llm:Falha no modelo Groq moonshotai/kimi-k2-instruct-0905: Error code: 400 - {'error': {'message': \"Failed to generate JSON. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'json_validate_failed', 'failed_generation': 'max completion tokens reached before generating a valid document'}}\n",
      "INFO:code_agent.get_routem_llm.routem_llm:Tentando modelo Groq: moonshotai/kimi-k2-instruct\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:code_agent.get_routem_llm.routem_llm:Sucesso com modelo Groq: moonshotai/kimi-k2-instruct\n",
      "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n",
      "INFO:code_agent.get_routem_llm.routem_llm:Iniciando roteamento LLM\n",
      "INFO:code_agent.get_routem_llm.routem_llm:Tentando modelo Groq: moonshotai/kimi-k2-instruct-0905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECKING CODE---\n",
      "Running the Python REPL tool\n",
      "def fibonacci_iterative(n):\n",
      "    \"\"\"\n",
      "    Calculate the nth Fibonacci number using iteration.\n",
      "    \n",
      "    Args:\n",
      "        n (int): Position in the Fibonacci sequence (0-indexed)\n",
      "    \n",
      "    Returns:\n",
      "        int: The nth Fibonacci number\n",
      "    \n",
      "    Time Complexity: O(n)\n",
      "    Space Complexity: O(1)\n",
      "    \"\"\"\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Fibonacci is not defined for negative numbers\")\n",
      "    \n",
      "    if n <= 1:\n",
      "        return n\n",
      "    \n",
      "    prev2, prev1 = 0, 1\n",
      "    for _ in range(2, n + 1):\n",
      "        current = prev1 + prev2\n",
      "        prev2, prev1 = prev1, current\n",
      "    \n",
      "    return prev1\n",
      "\n",
      "# Test the function\n",
      "test_cases = [(0, 0), (1, 1), (2, 1), (3, 2), (4, 3), (5, 5), (6, 8), (7, 13), (10, 55)]\n",
      "for n, expected in test_cases:\n",
      "    result = fibonacci_iterative(n)\n",
      "    print(f\"F({n}) = {result} (expected {expected})\")\n",
      "F(0) = 0 (expected 0)\n",
      "F(1) = 1 (expected 1)\n",
      "F(2) = 1 (expected 1)\n",
      "F(3) = 2 (expected 2)\n",
      "F(4) = 3 (expected 3)\n",
      "F(5) = 5 (expected 5)\n",
      "F(6) = 8 (expected 8)\n",
      "F(7) = 13 (expected 13)\n",
      "F(10) = 55 (expected 55)\n",
      "\n",
      "Entrei no supervisor\n",
      "state [HumanMessage(content=' \\n                 Faca uma funcao em python de fibonacci\\n                 ', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:code_agent.get_routem_llm.routem_llm:Sucesso com modelo Groq: moonshotai/kimi-k2-instruct-0905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug - interactions: 1, valid: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "inputs = {\"messages\": [HumanMessage(content=\n",
    "                 r\"\"\" \n",
    "                 Faca uma funcao em python de fibonacci\n",
    "                 \"\"\"\n",
    "                 \n",
    "    )],\n",
    "    \"feedback\": \"\", \n",
    "    \"interactions\": 0\n",
    "          \n",
    "          }\n",
    "\n",
    "input = StateCode(**inputs)\n",
    "\n",
    "\n",
    "\n",
    "result = await app_code.ainvoke(input, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d01da7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      " \n",
      "                 Faca uma funcao em python de fibonacci\n",
      "                 \n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "state = app_code.get_state(config)\n",
    "state\n",
    "\n",
    "ChatPromptTemplate.from_messages(state.values[\"messages\"]).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6795d696",
   "metadata": {},
   "source": [
    "# Request API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc333d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df04b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "mess = \"Faca uma analise do data set do titanic\"\n",
    "url = f\"http://localhost:3000/chatbot/{mess}\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684a19ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf2a09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Análise Completa do Dataset do Titanic 🚢',\n",
       " 'Aqui está uma análise detalhada do famoso dataset do Titanic, com visualizações e estatísticas que revelam os principais fatores que influenciaram a sobrevivência dos passageiros.',\n",
       " '## 📊 O que esta análise mostra:',\n",
       " '- **Taxa de sobrevivência por gênero** (mulheres tiveram prioridade nos botes salva-vidas)\\n- **Taxa de sobrevivência por classe social** (primeira classe vs. terceira classe)\\n- **Distribuição de idades** dos sobreviventes vs. não sobreviventes\\n- **Impacto do tamanho da família** na sobrevivência\\n- **Influência do porto de embarcação** nas chances de sobrevivência\\n- **Relação entre preço da passagem e sobrevivência**',\n",
       " '## 📋 Código Completo:',\n",
       " '```python\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport numpy as np',\n",
       " '# Função para carregar os dados do Titanic\\ndef load_titanic_data():\\n    \"\"\"\\n    Carrega o dataset do Titanic a partir do seaborn\\n    \"\"\"\\n    return sns.load_dataset(\\'titanic\\')',\n",
       " '# Função para limpar os dados\\ndef clean_titanic_data(df):\\n    \"\"\"\\n    Limpa os dados do Titanic, lidando com valores ausentes\\n    \"\"\"\\n    # Criar uma cópia para não modificar o original\\n    df_clean = df.copy()\\n    \\n    # Preencher idades ausentes com a mediana\\n    df_clean[\\'Age\\'].fillna(df_clean[\\'Age\\'].median(), inplace=True)\\n    \\n    # Preencher embarque ausente com o mais comum\\n    df_clean[\\'Embarked\\'].fillna(df_clean[\\'Embarked\\'].mode()[0], inplace=True)\\n    \\n    # Criar variável se está sozinho (sem familiares)\\n    df_clean[\\'FamilySize\\'] = df_clean[\\'SibSp\\'] + df_clean[\\'Parch\\'] + 1\\n    df_clean[\\'IsAlone\\'] = (df_clean[\\'FamilySize\\'] == 1).astype(int)\\n    \\n    # Remover outliers no preço (acima de 300)\\n    df_clean = df_clean[df_clean[\\'Fare\\'] < 300]\\n    \\n    return df_clean',\n",
       " '# Função de visualização\\ndef visualize_survival_analysis(df):\\n    \"\"\"\\n    Cria visualizações abrangentes para análise de sobrevivência no Titanic.\\n    \\n    Args:\\n        df: DataFrame limpo do Titanic\\n    \"\"\"\\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\\n    fig.suptitle(\\'Análise de Sobrevivência do Titanic\\', fontsize=16, fontweight=\\'bold\\')\\n    \\n    # 1. Sobrevivência por Gênero\\n    survival_by_sex = df.groupby(\\'Sex\\')[\\'Survived\\'].mean()\\n    axes[0,0].bar(survival_by_sex.index, survival_by_sex.values, color=[\\'#FF6B6B\\', \\'#4ECDC4\\'])\\n    axes[0,0].set_title(\\'Taxa de Sobrevivência por Gênero\\')\\n    axes[0,0].set_ylabel(\\'Taxa de Sobrevivência\\')\\n    for i, v in enumerate(survival_by_sex.values):\\n        axes[0,0].text(i, v + 0.01, f\\'{v:.1%}\\', ha=\\'center\\', fontweight=\\'bold\\')\\n    \\n    # 2. Sobrevivência por Classe\\n    survival_by_class = df.groupby(\\'Pclass\\')[\\'Survived\\'].mean()\\n    axes[0,1].bar(survival_by_class.index, survival_by_class.values, color=[\\'#FFD93D\\', \\'#6BCF7F\\', \\'#4D96FF\\'])\\n    axes[0,1].set_title(\\'Taxa de Sobrevivência por Classe\\')\\n    axes[0,1].set_xlabel(\\'Classe\\')\\n    axes[0,1].set_ylabel(\\'Taxa de Sobrevivência\\')\\n    for i, v in enumerate(survival_by_class.values):\\n        axes[0,1].text(i+1, v + 0.01, f\\'{v:.1%}\\', ha=\\'center\\', fontweight=\\'bold\\')\\n    \\n    # 3. Distribuição de Idade\\n    survived = df[df[\\'Survived\\'] == 1][\\'Age\\'].dropna()\\n    not_survived = df[df[\\'Survived\\'] == 0][\\'Age\\'].dropna()\\n    axes[0,2].hist([not_survived, survived], bins=30, alpha=0.7, \\n                     label=[\\'Não Sobreviveu\\', \\'Sobreviveu\\'], color=[\\'#FF6B6B\\', \\'#4ECDC4\\'])\\n    axes[0,2].set_title(\\'Distribuição de Idade por Sobrevivência\\')\\n    axes[0,2].set_xlabel(\\'Idade\\')\\n    axes[0,2].set_ylabel(\\'Contagem\\')\\n    axes[0,2].legend()\\n    \\n    # 4. Sobrevivência por Tamanho da Família\\n    family_survival = df.groupby(\\'FamilySize\\')[\\'Survived\\'].mean()\\n    axes[1,0].plot(family_survival.index, family_survival.values, marker=\\'o\\', \\n                   linewidth=2, markersize=8, color=\\'#9B59B6\\')\\n    axes[1,0].set_title(\\'Taxa de Sobrevivência por Tamanho da Família\\')\\n    axes[1,0].set_xlabel(\\'Tamanho da Família\\')\\n    axes[1,0].set_ylabel(\\'Taxa de Sobrevivência\\')\\n    axes[1,0].grid(True, alpha=0.3)\\n    \\n    # 5. Sobrevivência por Porto de Embarque\\n    embarked_survival = df.groupby(\\'Embarked\\')[\\'Survived\\'].mean()\\n    port_names = {\\'S\\': \\'Southampton\\', \\'C\\': \\'Cherbourg\\', \\'Q\\': \\'Queenstown\\'}\\n    ports = [port_names[p] for p in embarked_survival.index]\\n    axes[1,1].bar(ports, embarked_survival.values, color=[\\'#E74C3C\\', \\'#3498DB\\', \\'#F39C12\\'])\\n    axes[1,1].set_title(\\'Taxa de Sobrevivência por Porto de Embarque\\')\\n    axes[1,1].set_ylabel(\\'Taxa de Sobrevivência\\')\\n    for i, v in enumerate(embarked_survival.values):\\n        axes[1,1].text(i, v + 0.01, f\\'{v:.1%}\\', ha=\\'center\\', fontweight=\\'bold\\')\\n    \\n    # 6. Distribuição de Preços\\n    axes[1,2].boxplot([df[df[\\'Survived\\']==0][\\'Fare\\'], df[df[\\'Survived\\']==1][\\'Fare\\']], \\n                      labels=[\\'Não Sobreviveu\\', \\'Sobreviveu\\'])\\n    axes[1,2].set_title(\\'Distribuição de Preços por Sobrevivência\\')\\n    axes[1,2].set_ylabel(\\'Preço da Passagem (£)\\')\\n    \\n    plt.tight_layout()\\n    plt.show()',\n",
       " '# Função para gerar estatísticas\\ndef generate_insights(df):\\n    \"\"\"\\n    Gera estatísticas e insights do dataset do Titanic.\\n    \\n    Args:\\n        df: DataFrame limpo do Titanic\\n        \\n    Returns:\\n        Dicionário com estatísticas e insights\\n    \"\"\"\\n    insights = {\\n        \\'total_passageiros\\': len(df),\\n        \\'taxa_sobrevivencia_geral\\': df[\\'Survived\\'].mean(),\\n        \\'taxa_sobrevivencia_feminina\\': df[df[\\'Sex\\'] == \\'female\\'][\\'Survived\\'].mean(),\\n        \\'taxa_sobrevivencia_masculina\\': df[df[\\'Sex\\'] == \\'male\\'][\\'Survived\\'].mean(),\\n        \\'sobrevivencia_primeira_classe\\': df[df[\\'Pclass\\'] == 1][\\'Survived\\'].mean(),\\n        \\'sobrevivencia_segunda_classe\\': df[df[\\'Pclass\\'] == 2][\\'Survived\\'].mean(),\\n        \\'sobrevivencia_terceira_classe\\': df[df[\\'Pclass\\'] == 3][\\'Survived\\'].mean(),\\n        \\'idade_media_sobreviventes\\': df[df[\\'Survived\\'] == 1][\\'Age\\'].mean(),\\n        \\'idade_media_nao_sobreviventes\\': df[df[\\'Survived\\'] == 0][\\'Age\\'].mean(),\\n        \\'taxa_sobrevivencia_sozinhos\\': df[df[\\'IsAlone\\'] == 1][\\'Survived\\'].mean(),\\n        \\'taxa_sobrevivencia_com_familia\\': df[df[\\'IsAlone\\'] == 0][\\'Survived\\'].mean()\\n    }\\n    \\n    return insights',\n",
       " '# Executar a análise\\nif __name__ == \"__main__\":\\n    print(\"🚢 INICIANDO ANÁLISE DO TITANIC 🚢\")\\n    print(\"=\" * 50)\\n    \\n    # Carregar dados\\n    titanic_df = load_titanic_data()\\n    \\n    # Informações básicas\\n    print(f\"Dataset original: {titanic_df.shape[0]} linhas x {titanic_df.shape[1]} colunas\")\\n    print(f\"Colunas disponíveis: {\\', \\'.join(titanic_df.columns)}\")\\n    \\n    # Limpar dados\\n    titanic_clean = clean_titanic_data(titanic_df)\\n    print(f\"Após limpeza: {titanic_clean.shape[0]} linhas\")\\n    \\n    # Gerar estatísticas\\n    insights = generate_insights(titanic_clean)\\n    \\n    print(\"\\\\n\" + \"=\" * 50)\\n    print(\"📊 PRINCIPAIS DESCOBERTAS:\")\\n    print(\"=\" * 50)\\n    \\n    print(f\"📋 Total de passageiros analisados: {insights[\\'total_passageiros\\']}\")\\n    print(f\"⚖️  Taxa geral de sobrevivência: {insights[\\'taxa_sobrevivencia_geral\\']:.1%}\")\\n    \\n    print(f\"\\\\n👥 Por gênero:\")\\n    print(f\"   • Mulheres: {insights[\\'taxa_sobrevivencia_feminina\\']:.1%}\")\\n    print(f\"   • Homens: {insights[\\'taxa_sobrevivencia_masculina\\']:.1%}\")\\n    \\n    print(f\"\\\\n💺 Por classe:\")\\n    print(f\"   • 1ª Classe: {insights[\\'sobrevivencia_primeira_classe\\']:.1%}\")\\n    print(f\"   • 2ª Classe: {insights[\\'sobrevivencia_segunda_classe\\']:.1%}\")\\n    print(f\"   • 3ª Classe: {insights[\\'sobrevivencia_terceira_classe\\']:.1%}\")\\n    \\n    print(f\"\\\\n👨\\u200d👩\\u200d👧\\u200d👦 Situação familiar:\")\\n    print(f\"   • Viajando sozinho: {insights[\\'taxa_sobrevivencia_sozinhos\\']:.1%}\")\\n    print(f\"   • Com família: {insights[\\'taxa_sobrevivencia_com_familia\\']:.1%}\")\\n    \\n    print(f\"\\\\n🎂 Idade média:\")\\n    print(f\"   • Sobreviventes: {insights[\\'idade_media_sobreviventes\\']:.1f} anos\")\\n    print(f\"   • Não sobreviventes: {insights[\\'idade_media_nao_sobreviventes\\']:.1f} anos\")\\n    \\n    # Criar visualizações\\n    print(\"\\\\n🎨 Criando gráficos...\")\\n    visualize_survival_analysis(titanic_clean)\\n    print(\"✅ Análise concluída!\")\\n```',\n",
       " '## 🎯 Como executar:',\n",
       " '1. **Instalar dependências** (se necessário):\\n```bash\\npip install pandas matplotlib seaborn numpy\\n```',\n",
       " '2. **Salvar o código** em um arquivo `titanic_analysis.py`',\n",
       " '3. **Executar**:\\n```bash\\npython titanic_analysis.py\\n```',\n",
       " '## 🔍 Principais Insights Revelados:',\n",
       " '- **Gênero**: Mulheres tiveram 74% de chance de sobrevivência vs. 19% dos homens\\n- **Classe Social**: 1ª classe (63%) > 2ª classe (47%) > 3ª classe (24%)\\n- **Idade**: Crianças e jovens adultos tiveram melhores chances\\n- **Família**: Viajar sozinho diminuía as chances de sobrevivência\\n- **Porto**: Passageiros de Cherbourg tiveram melhor taxa de sobrevivência',\n",
       " 'Esta análise mostra como fatores sociais e demográficos foram determinantes na tragédia do Titanic, refletindo as normas sociais da época sobre \"mulheres e crianças primeiro\" e as desigualdades de classe.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['messages'][1]['content'].split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613686eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgenteCodificaoLangGraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

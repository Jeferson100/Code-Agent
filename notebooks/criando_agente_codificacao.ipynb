{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "179e03a0",
   "metadata": {},
   "source": [
    "# Agente de codificacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "129c806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "from IPython.display import Image, display\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb77182",
   "metadata": {},
   "source": [
    "## Criando prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7d6c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_code = \"\"\"You are a **Coding Assistant Agent**.  \n",
    "Your mission is to help the user with programming tasks across different languages and frameworks.  \n",
    "You must:  \n",
    "1. Write clean, efficient, and well-documented code.  \n",
    "2. Explain your reasoning and key choices in simple terms.  \n",
    "3. Suggest improvements when reviewing code (performance, readability, best practices).  \n",
    "4. When debugging, identify the root cause of errors and provide fixes with explanations.  \n",
    "5. Adapt your coding style to match the user’s context (e.g., beginner-friendly explanations or advanced optimizations).  \n",
    "6. When there are multiple solutions, compare trade-offs clearly.  \n",
    "7. Always follow security, scalability, and maintainability principles.  \n",
    "8. Never just drop code — guide the user through understanding it.  \n",
    "\n",
    "Primary objectives:  \n",
    "- Generate working code examples.  \n",
    "- Provide step-by-step explanations.  \n",
    "- Support iterative development and testing.  \n",
    "\n",
    "You are not just a code generator — you are a **teacher, reviewer, and collaborator**.  \n",
    "\n",
    "{messages}\n",
    "\n",
    "{feedback}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "supervisor_code= \"\"\"\n",
    "\n",
    "                You are a **Code Supervisor Agent**.  \n",
    "                Your mission is to **review and evaluate** code generated by other coding agents. \n",
    "                You do not write new solutions unless necessary — your role is to **analyze, critique, and guide**. \n",
    "                \n",
    "                The question of user was this:\n",
    "                {messages}\n",
    "                \n",
    "                The code generated to other agent was this:\n",
    "            \n",
    "                {code}\n",
    "                \n",
    "                You receive on message of test excution:\n",
    "                \n",
    "                {error}\n",
    "\n",
    "                Your responsibilities:  \n",
    "                1. **Correctness** → Verify if the code works as intended, handles inputs properly, and avoids logical errors.  \n",
    "                2. **Quality** → Check readability, structure, naming conventions, and adherence to best practices (e.g., PEP8 for Python).  \n",
    "                3. **Efficiency** → Assess performance, memory usage, and scalability. Suggest optimizations if beneficial.  \n",
    "                4. **Security** → Identify potential vulnerabilities, unsafe practices, or missing validations.  \n",
    "                5. **Maintainability** → Ensure the code is modular, well-documented, and easy to extend or debug later.  \n",
    "                6. **Clarity of explanation** → Confirm that the original agent provided sufficient reasoning and that a user could understand the choices made.  \n",
    "                7. **Constructive feedback** → Provide concise, actionable improvements (not just criticism).  \n",
    "                8. **Decision-making** → If there are multiple valid approaches, compare pros and cons clearly.  \n",
    "\n",
    "                Evaluation format:  \n",
    "                - **Strengths**: What is good about the code.  \n",
    "                - **Weaknesses**: Problems, risks, or bad practices.  \n",
    "                - **Suggestions**: Clear, prioritized improvements.  \n",
    "                - **Verdict**: Acceptable as-is or requires changes.  \n",
    "\n",
    "                You are not a coder — you are a **mentor, auditor, and quality gatekeeper**.  \n",
    "                Your goal is to ensure the code is **correct, safe, efficient, and teachable**.  \n",
    "                \n",
    "                \n",
    "                \"\"\"\n",
    "\n",
    "return_messages_prompt = \"\"\"\"\n",
    "You are the **Respondent Agent** in a coding assistant system.  \n",
    "Your mission is to communicate with the user in a clear, helpful, and professional way.  \n",
    "\n",
    "<Available Tools>\n",
    "You have access to one so tools: \n",
    "- **think_response**: Use this tool to reflect and analyze before crafting your final response.\n",
    "</Available Tools>\n",
    "\n",
    "\n",
    "<inputs>\n",
    "You receive two inputs:  \n",
    "- {messages} → the user’s request or question.  \n",
    "- {code} → the code generated, reviewed, or corrected by other agents.  \n",
    "- {feedback} → the supervisor agent's feedback on the code is working.\n",
    "</inputs>\n",
    "\n",
    "Your responsibilities:  \n",
    "1. **Clarity** → Present the response in a way that the user can easily understand, regardless of skill level.  \n",
    "2. **Contextualization** → Connect the code to the user’s original request ({messages}).  \n",
    "3. **Explanation** → Summarize what the code does and highlight key points or improvements.  \n",
    "4. **Actionability** → Provide direct instructions or next steps if needed (how to run, test, or adapt the code).  \n",
    "5. **Tone** → Be concise, friendly, and collaborative — never overly technical without explanation.  \n",
    "6. **Formatting** → Use proper code blocks, bullet points, and sections for readability.  \n",
    "\n",
    "Your goal:  \n",
    "Deliver the **final, user-facing answer**, combining the code with clear explanations so the user can immediately apply it.  \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt_agent_pesquisador = \"\"\"\n",
    "        You are part of a multi-agent coding assistant system.  \n",
    "        Your role is the **Research Agent**, with the ability to search the internet using the provided tool. \n",
    "        You analyze the {code} and {error_message}, then use the internet search tool to gather solutions or corrections that can fix the code.\n",
    "        \n",
    "        <Available Tools>                                                                                              \n",
    "        You have access to two main tools:                                                                             \n",
    "        1. **tavily_search**: For conducting web searches to gather information                                        \n",
    "        2. **think_tool**: For reflection and strategic planning during research                                                                                                                                                       │\n",
    "        **CRITICAL: Use think_tool after each search to reflect on results and plan next steps**                       \n",
    "        </Available Tools>  \n",
    "         \n",
    "        Inputs you may receive:  \n",
    "        - {messages} → the user’s original request.  \n",
    "        - {error_message}, {code}, {feedback} → context from previous phases to refine your research.  \n",
    "\n",
    "        Your goal is to about search the internet to gather solutions or corrections that can fix the code. \n",
    "        \n",
    "        <think_tool>\n",
    "        After each search tool call, use think_tool to analyze the results: \n",
    "        <think_tool>\n",
    "        \n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d481ab11",
   "metadata": {},
   "source": [
    "## Criando estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "695eed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated\n",
    "from typing import Literal, TypedDict, Tuple, List\n",
    "import operator\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class StateCode(TypedDict):\n",
    "    messages: Annotated[List, operator.add]\n",
    "    code: str\n",
    "    feedback: str\n",
    "    valid: Literal[True, False]\n",
    "    interactions: int\n",
    "    error_message : str\n",
    "    \n",
    "\n",
    "class SupervisorResponse(BaseModel):\n",
    "    valid: Literal[True, False] = Field(description=\"Valid from the supervisor, if not is need more interactions return True, else return False\")\n",
    "    feedback: str = Field(description=\"Feedback from the supervisor\")\n",
    "    \n",
    "class CodeOutput(BaseModel):\n",
    "    \"\"\"Schema for code solutions to questions about LCEL.\"\"\"\n",
    "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "    imports: str = Field(description=\"Code block import statements, if there are imports, leave it empty\")\n",
    "    code: str = Field(description=\"Code block not including import statements\")\n",
    "    \n",
    "class SearchResponse(BaseModel):\n",
    "    \"\"\"Response of agent that search in the internet\"\"\"\n",
    "    \n",
    "    feedback: str = Field(description= \"Fedeback of agent that search the internet for agent of code\")\n",
    "    code : str = Field(description= \"Code generated by the search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61b4494",
   "metadata": {},
   "source": [
    "## Criando roteador de LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60908863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "from groq import AsyncGroq\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import asyncio\n",
    "from typing import Optional, Dict, Any, Union\n",
    "import logging\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# client = Groq()\n",
    "client = AsyncGroq()\n",
    "\n",
    "\n",
    "class GetLlmResponse:\n",
    "    \"\"\"\n",
    "    Classe para obter respostas de diferentes provedores LLM\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, messages: str, model_llm: str, strutured_output: Optional[BaseModel] = None):\n",
    "        self.messages = messages\n",
    "        self.strutured_output = strutured_output\n",
    "        self.model_llm = model_llm\n",
    "    \n",
    "    async def llm_structured_groq(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Chama modelo Groq com saída estruturada\n",
    "        \"\"\"\n",
    "        response = await client.chat.completions.create(\n",
    "            model=self.model_llm,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": self.messages},  # Corrigido: usar self.messages\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"structured_response\",\n",
    "                    \"schema\": self.strutured_output.model_json_schema(),\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        content = response.choices[0].message.content or \"{}\"\n",
    "        return json.loads(content)\n",
    "    \n",
    "    async def llm_groq(self) -> str:\n",
    "        \"\"\"\n",
    "        Chama modelo Groq sem saída estruturada\n",
    "        \"\"\"\n",
    "        response = await client.chat.completions.create(\n",
    "            model=self.model_llm,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": self.messages},  # Corrigido: usar self.messages\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content or \"\"\n",
    "    \n",
    "    async def llm_pydantic_structured(self) -> Any:\n",
    "        \"\"\"\n",
    "        Chama modelo HuggingFace via Pydantic AI com saída estruturada\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from pydantic_ai import Agent\n",
    "            from pydantic_ai.models.huggingface import HuggingFaceModel\n",
    "            \n",
    "            model = HuggingFaceModel(self.model_llm)\n",
    "            agent = Agent(model, output_type=self.strutured_output)\n",
    "            \n",
    "            async with agent.run_stream(self.messages) as response:  # Corrigido: usar self.messages\n",
    "                async for profile in response.stream_output():\n",
    "                    return profile\n",
    "                    \n",
    "        except ImportError:\n",
    "            raise ImportError(\"pydantic_ai não está instalado. Instale com: pip install pydantic-ai\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro no llm_pydantic_structured: {e}\")\n",
    "            raise\n",
    "    \n",
    "    async def llm_pydantic(self) -> Any:\n",
    "        \"\"\"\n",
    "        Chama modelo HuggingFace via Pydantic AI sem saída estruturada\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from pydantic_ai import Agent\n",
    "            from pydantic_ai.models.huggingface import HuggingFaceModel\n",
    "            \n",
    "            model = HuggingFaceModel(self.model_llm)\n",
    "            agent = Agent(model)\n",
    "            \n",
    "            async with agent.run_stream(self.messages) as response:  # Corrigido: usar self.messages\n",
    "                async for profile in response.stream_output():\n",
    "                    return profile\n",
    "                    \n",
    "        except ImportError:\n",
    "            raise ImportError(\"pydantic_ai não está instalado. Instale com: pip install pydantic-ai\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro no llm_pydantic: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "class LlmRouter:\n",
    "    \"\"\"\n",
    "    Classe para roteamento automático entre diferentes modelos LLM com fallback\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, messages: str, strutured_output: Optional[BaseModel] = None):\n",
    "        self.messages = messages\n",
    "        self.strutured_output = strutured_output\n",
    "        \n",
    "        # Lista de modelos em ordem de prioridade\n",
    "        self.groq_models = [\n",
    "            \"moonshotai/kimi-k2-instruct-0905\",\n",
    "            \"moonshotai/kimi-k2-instruct\",\n",
    "            \"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "            \"openai/gpt-oss-120b\",\n",
    "            \n",
    "        ]\n",
    "        \n",
    "        self.huggingface_models = [\n",
    "            \"Qwen/Qwen3-235B-A22B-Instruct-2507\",\n",
    "            \"microsoft/DialoGPT-large\",\n",
    "            \"meta-llama/Llama-4-Maverick-17B-128E\"\n",
    "        ]\n",
    "    \n",
    "    async def _try_groq_models(self) -> Union[Dict[str, Any], str]:\n",
    "        \"\"\"\n",
    "        Tenta usar modelos Groq em ordem de prioridade\n",
    "        \"\"\"\n",
    "        for model in self.groq_models:\n",
    "            try:\n",
    "                logger.info(f\"Tentando modelo Groq: {model}\")\n",
    "                llm_response = GetLlmResponse(self.messages, model, self.strutured_output)\n",
    "                \n",
    "                if self.strutured_output:\n",
    "                    result = await llm_response.llm_structured_groq()\n",
    "                else:\n",
    "                    result = await llm_response.llm_groq()\n",
    "                \n",
    "                logger.info(f\"Sucesso com modelo Groq: {model}\")\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Falha no modelo Groq {model}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        raise Exception(\"Todos os modelos Groq falharam\")\n",
    "    \n",
    "    async def _try_huggingface_models(self) -> Any:\n",
    "        \"\"\"\n",
    "        Tenta usar modelos HuggingFace em ordem de prioridade\n",
    "        \"\"\"\n",
    "        for model in self.huggingface_models:\n",
    "            try:\n",
    "                logger.info(f\"Tentando modelo HuggingFace: {model}\")\n",
    "                llm_response = GetLlmResponse(self.messages, model, self.strutured_output)\n",
    "                \n",
    "                if self.strutured_output:\n",
    "                    result = await llm_response.llm_pydantic_structured()\n",
    "                else:\n",
    "                    result = await llm_response.llm_pydantic()\n",
    "                \n",
    "                logger.info(f\"Sucesso com modelo HuggingFace: {model}\")\n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Falha no modelo HuggingFace {model}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        raise Exception(\"Todos os modelos HuggingFace falharam\")\n",
    "    \n",
    "    async def llm_router(self) -> Union[Dict[str, Any], str, Any]:\n",
    "        \"\"\"\n",
    "        Método principal que implementa o sistema de fallback\n",
    "        \"\"\"\n",
    "        logger.info(\"Iniciando roteamento LLM\")\n",
    "        \n",
    "        # Primeira tentativa: Modelos Groq\n",
    "        try:\n",
    "            return await self._try_groq_models()\n",
    "        except Exception as groq_error:\n",
    "            logger.warning(f\"Todos os modelos Groq falharam: {groq_error}\")\n",
    "            \n",
    "            # Segunda tentativa: Modelos HuggingFace\n",
    "            try:\n",
    "                return await self._try_huggingface_models()\n",
    "            except Exception as hf_error:\n",
    "                logger.error(f\"Todos os modelos HuggingFace falharam: {hf_error}\")\n",
    "                \n",
    "                # Se tudo falhar, lança exceção detalhada\n",
    "                raise Exception(\n",
    "                    f\"Todos os provedores falharam. \"\n",
    "                    f\"Groq: {groq_error}, HuggingFace: {hf_error}\"\n",
    "                )\n",
    "    \n",
    "    def llm_router_sync(self) -> Union[Dict[str, Any], str, Any]:\n",
    "        \"\"\"\n",
    "        Versão síncrona do roteador LLM\n",
    "        \"\"\"\n",
    "        return asyncio.run(self.llm_router())\n",
    "    \n",
    "    async def get_structured_response(self, output_model: BaseModel) -> BaseModel:\n",
    "        \"\"\"\n",
    "        Obtém resposta estruturada e valida com o modelo Pydantic\n",
    "        \"\"\"\n",
    "        if not self.strutured_output:\n",
    "            # Atualiza temporariamente para usar saída estruturada\n",
    "            original_structured = self.strutured_output\n",
    "            self.strutured_output = output_model\n",
    "            \n",
    "            try:\n",
    "                result = await self.llm_router()\n",
    "                return output_model(**result) if isinstance(result, dict) else output_model()\n",
    "            finally:\n",
    "                self.strutured_output = original_structured\n",
    "        else:\n",
    "            result = await self.llm_router()\n",
    "            return output_model(**result) if isinstance(result, dict) else output_model()\n",
    "    \n",
    "    def add_groq_model(self, model_name: str, priority: int = 0):\n",
    "        \"\"\"\n",
    "        Adiciona um novo modelo Groq à lista\n",
    "        \"\"\"\n",
    "        if priority == 0:\n",
    "            self.groq_models.append(model_name)\n",
    "        else:\n",
    "            self.groq_models.insert(priority, model_name)\n",
    "    \n",
    "    def add_huggingface_model(self, model_name: str, priority: int = 0):\n",
    "        \"\"\"\n",
    "        Adiciona um novo modelo HuggingFace à lista\n",
    "        \"\"\"\n",
    "        if priority == 0:\n",
    "            self.huggingface_models.append(model_name)\n",
    "        else:\n",
    "            self.huggingface_models.insert(priority, model_name)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5fc01b",
   "metadata": {},
   "source": [
    "## Criando agente codificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb893e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jefer\\Documents\\Ciencia-de-dados\\LLMs\\AgenteCodificaoLangGraph\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\jefer\\Documents\\Ciencia-de-dados\\LLMs\\AgenteCodificaoLangGraph\\.venv\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:229: UserWarning: Found qwen/qwen3-coder-480b-a35b-instruct in available_models, but type is unknown and inference may fail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "llm_code = init_chat_model(\n",
    "    \"qwen/qwen3-coder-480b-a35b-instruct\", model_provider=\"nvidia\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d9e910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def node_code(state: StateCode) -> StateCode:\n",
    "    \n",
    "    print(\"Entrei no code\")\n",
    "    \n",
    "    print(\"state\", state[\"messages\"])\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    if messages:\n",
    "        messages = messages[-1]\n",
    "    \n",
    "    feedback = state.get(\"feedback\", \"\")\n",
    "    \n",
    "    prompt_code_format = prompt_code.format(messages=messages, feedback=feedback)\n",
    "    \n",
    "    response = llm_code.invoke(prompt_code_format)\n",
    "    \n",
    "    router_structured = LlmRouter(\n",
    "            response.content,\n",
    "            CodeOutput\n",
    "        )\n",
    "    \n",
    "    response_code_formatted = await router_structured.llm_router()\n",
    "    \n",
    "    if isinstance(response_code_formatted, dict):\n",
    "        code = response_code_formatted[\"code\"]\n",
    "        imports = response_code_formatted[\"imports\"]\n",
    "        prefix = response_code_formatted[\"prefix\"]\n",
    "        \n",
    "        return {\"code\": code, \"imports\": imports, \"prefix\": prefix}\n",
    "        \n",
    "    else:\n",
    "        code = response_code_formatted.code\n",
    "        imports = response_code_formatted.imports\n",
    "        prefix = response_code_formatted.prefix\n",
    "        \n",
    "        return {\"code\": code, \"imports\": imports, \"prefix\": prefix}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d5c3c0",
   "metadata": {},
   "source": [
    "## Criando verificador de código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adcb0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_check(state: StateCode) -> StateCode:\n",
    "    \"\"\"\n",
    "    Check code\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, error\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECKING CODE---\")\n",
    "\n",
    "    # State\n",
    "    messages = state.get(\"messages\")\n",
    "    \n",
    "    if not messages:\n",
    "        messages = messages[-1]\n",
    "    \n",
    "    code = state.get(\"code\")\n",
    "    imports = state.get(\"imports\")\n",
    "    messages_erro = []\n",
    "\n",
    "    # Check imports\n",
    "    if imports:\n",
    "        try:\n",
    "            exec(imports)\n",
    "        except Exception as e:\n",
    "            print(f\"---CODE IMPORT CHECK: FAILED--- {e} \")\n",
    "            error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n",
    "            messages_erro += error_message\n",
    "            return {\n",
    "                \"error_message\": messages_erro,\n",
    "            }\n",
    "    else:\n",
    "        print(\"---NO CODE IMPORTS---\")\n",
    "\n",
    "    # Check execution\n",
    "    try:\n",
    "        if imports:\n",
    "            exec(imports + \"\\n\" + code)\n",
    "        else:\n",
    "            exec(code)\n",
    "    except Exception as e:\n",
    "        print(f\"---CODE BLOCK CHECK: FAILED--- {e}\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n",
    "        messages_erro += error_message\n",
    "        return {\n",
    "            \"error_message\": messages_erro,\n",
    "        }\n",
    "    # No errors\n",
    "    print(\"---NO CODE TEST FAILURES---\")\n",
    "    return {\n",
    "        \"error_message\": messages_erro,\n",
    "    \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31af187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "def python_repl(state: StateCode) -> StateCode:\n",
    "    \"\"\"\n",
    "    Use this function to execute Python code and get the results.\n",
    "    \"\"\"\n",
    "    repl = PythonREPL()\n",
    "    \n",
    "    print(\"---CHECKING CODE---\")\n",
    "\n",
    "    # State\n",
    "    messages = state.get(\"messages\")\n",
    "    \n",
    "    if not messages:\n",
    "        messages = messages[-1]\n",
    "    \n",
    "    code = state.get(\"code\")\n",
    "    imports = state.get(\"imports\")\n",
    "    messages_erro = []\n",
    "\n",
    "    # Check imports\n",
    "    if imports:\n",
    "        try:\n",
    "            print(\"Running the Python REPL tool\")\n",
    "            print(imports + \"\\n\" + code)\n",
    "            result = repl.run(imports + \"\\n\" + code)\n",
    "            print(result)\n",
    "        except BaseException as e:\n",
    "            print(f\"Failed to execute. Error: {e!r}\")\n",
    "            return {\"error_message\": messages_erro,}\n",
    "        \n",
    "    else:\n",
    "        try:\n",
    "            print(\"Running the Python REPL tool\")\n",
    "            print(code)\n",
    "            result = repl.run(code)\n",
    "            print(result)\n",
    "        except BaseException as e:\n",
    "            return {\"error_message\": messages_erro,}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a74338",
   "metadata": {},
   "source": [
    "## Criando agente supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ae31deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def node_supervisor(state: StateCode) -> StateCode:\n",
    "    \n",
    "    print(\"Entrei no supervisor\")\n",
    "    \n",
    "    print(\"state\", state[\"messages\"])\n",
    "    \n",
    "    messages = state.get(\"messages\")\n",
    "    \n",
    "    if messages:\n",
    "        messages = messages[-1]\n",
    "    \n",
    "    interactions = state.get(\"interactions\", 0)\n",
    "    \n",
    "    error_message = state.get(\"error_message\", \"\")\n",
    "    \n",
    "    if interactions: \n",
    "        interactions += 1\n",
    "    else:\n",
    "        interactions = 1\n",
    "        \n",
    "    code = state[\"code\"]\n",
    "    \n",
    "    prompt_supervisor_format = supervisor_code.format(messages=messages, code=code, error=error_message)\n",
    "    \n",
    "    #response = await llm_router_structured(prompt_supervisor_format, SupervisorResponse)\n",
    "    \n",
    "    router_structured = LlmRouter(\n",
    "            prompt_supervisor_format,\n",
    "            SupervisorResponse\n",
    "        )\n",
    "    \n",
    "    response = await router_structured.llm_router()\n",
    "    \n",
    "    if isinstance(response, dict) and \"valid\" not in response.keys() and \"feedback\" not in response.keys():\n",
    "        response = await router_structured.llm_router()\n",
    "    \n",
    "    if isinstance(response, dict):\n",
    "        print(response)\n",
    "        feedback = response[\"feedback\"]\n",
    "        valid = response[\"valid\"]\n",
    "    else:\n",
    "        feedback = response.feedback\n",
    "        valid = response.valid\n",
    "    \n",
    "    return {\"feedback\": feedback, \"valid\": valid, \"interactions\": interactions}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd2f43e",
   "metadata": {},
   "source": [
    "# Gerador de pesquisador da internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bef7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ebe0f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    \"messages\": \"Create the analysi of the dataset titanic\",\n",
    "    \"error_message\": \"nanan\",\n",
    "    \"code\": \"\",\n",
    "    \"feedback\": \"O código não funciona corretamente.\"\n",
    "}\n",
    "\n",
    "prompt_agent_pesquisador_format = prompt_agent_pesquisador.format(\n",
    "        messages = result['messages'],\n",
    "        error_message = result['error_message'],\n",
    "        code = result['code'],\n",
    "        feedback = result['feedback']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "916c994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "tool_tavily = TavilySearch(\n",
    "    max_results=5,\n",
    "    topic=\"general\",\n",
    "    # include_answer=False,\n",
    "    # include_raw_content=False,\n",
    "    # include_images=False,\n",
    "    # include_image_descriptions=False,\n",
    "    # search_depth=\"basic\",\n",
    "    # time_range=\"day\",\n",
    "    # include_domains=None,\n",
    "    exclude_domains=[\"medium.com\", \"youtube.com\", \"wikipedia.org\", \"linkedin.com\"],\n",
    ")\n",
    "\n",
    "\n",
    "def think_tool(reflection: str) -> str:\n",
    "    \"\"\"Tool for strategic reflection on research progress and decision-making.\n",
    "    \n",
    "    Use this tool after each search to analyze results and plan next steps systematically.\n",
    "    This creates a deliberate pause in the research workflow for quality decision-making.\n",
    "    \n",
    "    When to use:\n",
    "    - After receiving search results: What key information did I find?\n",
    "    - Before deciding next steps: Do I have enough to answer comprehensively?\n",
    "    - When assessing research gaps: What specific information am I still missing?\n",
    "    - Before concluding research: Can I provide a complete answer now?\n",
    "    \n",
    "    Reflection should address:\n",
    "    1. Analysis of current findings - What concrete information have I gathered?\n",
    "    2. Gap assessment - What crucial information is still missing?\n",
    "    3. Quality evaluation - Do I have sufficient evidence/examples for a good answer?\n",
    "    4. Strategic decision - Should I continue searching or provide my answer?\n",
    "    \n",
    "    Args:\n",
    "        reflection: Your detailed reflection on research progress, findings, gaps, and next steps\n",
    "        \n",
    "    Returns:\n",
    "        Confirmation that reflection was recorded for decision-making\n",
    "    \"\"\"\n",
    "    return f\"Reflection recorded: {reflection}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e924465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tool_tavily.invoke(\"O código não funciona corretamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e72deea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cc652db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        You are part of a multi-agent coding assistant system.  \\n        Your role is the **Research Agent**, with the ability to search the internet using the provided tool. \\n        You analyze the {code} and {error_message}, then use the internet search tool to gather solutions or corrections that can fix the code.\\n\\n        <Available Tools>                                                                                              \\n        You have access to two main tools:                                                                             \\n        1. **tavily_search**: For conducting web searches to gather information                                        \\n        2. **think_tool**: For reflection and strategic planning during research                                                                                                                                                       │\\n        **CRITICAL: Use think_tool after each search to reflect on results and plan next steps**                       \\n        </Available Tools>  \\n\\n        Inputs you may receive:  \\n        - {messages} → the user’s original request.  \\n        - {error_message}, {code}, {feedback} → context from previous phases to refine your research.  \\n\\n        Your goal is to about search the internet to gather solutions or corrections that can fix the code. \\n\\n        <think_tool>\\n        After each search tool call, use think_tool to analyze the results: \\n        <think_tool>\\n\\n        '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_agent_pesquisador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b2978a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_init = init_chat_model(\"moonshotai/kimi-k2-instruct\", model_provider=\"groq\")  \n",
    "\n",
    "agente_search = create_react_agent(\n",
    "    llm_init, [tool_tavily, think_tool],\n",
    "    prompt=prompt_agent_pesquisador,\n",
    "    #response_format= SearchResponse\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3105e00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(agente_search.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e27a51e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def node_search(state: StateCode) -> StateCode:\n",
    "    print(\"Entrei no search\")    \n",
    "    messages = state.get(\"messages\")\n",
    "    \n",
    "    if messages:\n",
    "        messages = messages[-1]\n",
    "    \n",
    "    code = state.get(\"code\")\n",
    "    error_message = state.get(\"error_message\")\n",
    "    feedback = state.get(\"feedback\")\n",
    "    \n",
    "    prompt_agent_pesquisador_format = prompt_agent_pesquisador.format(\n",
    "        messages = messages,\n",
    "        error_message = error_message,\n",
    "        code = code,\n",
    "        feedback = feedback)\n",
    "    \n",
    "    llm_init = init_chat_model(\"moonshotai/kimi-k2-instruct\", model_provider=\"groq\")  \n",
    "\n",
    "    agente_search = create_react_agent(\n",
    "        llm_init, [tool_tavily, think_tool],\n",
    "        prompt=prompt_agent_pesquisador_format,\n",
    "    )\n",
    "    \n",
    "    response = await agente_search.ainvoke({\"messages\": [messages]})\n",
    "    \n",
    "    \n",
    "    return {\"feedback\": response['messages'][-1].content}\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "135f2a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 3.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 37.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 42.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "messages = \"Create the analysi of the dataset titanic\"\n",
    "error_message = \"nanan\"\n",
    "code = \"\"\n",
    "feedback = \"O código não funciona corretamente.\"\n",
    "\n",
    "prompt_agent_pesquisador_format = prompt_agent_pesquisador.format(\n",
    "        messages = messages,\n",
    "        error_message = error_message,\n",
    "        code = code,\n",
    "        feedback = feedback)\n",
    "    \n",
    "llm_init = init_chat_model(\"moonshotai/kimi-k2-instruct\", model_provider=\"groq\")  \n",
    "\n",
    "agente_search = create_react_agent(\n",
    "        llm_init, [tool_tavily, think_tool],\n",
    "        prompt=prompt_agent_pesquisador_format,\n",
    "    )\n",
    "    \n",
    "response = await agente_search.ainvoke({\"messages\": [messages]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c432cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Create the analysi of the dataset titanic', additional_kwargs={}, response_metadata={}, id='423eca93-ce16-4e7f-bf02-e670a4e421dc'),\n",
       " AIMessage(content=\"I'll help you create an analysis of the Titanic dataset. Let me start by searching for comprehensive information about the Titanic dataset and common analysis approaches.\", additional_kwargs={'tool_calls': [{'id': 'functions.tavily_search:0', 'function': {'arguments': '{\"query\":\"Titanic dataset analysis Python pandas data science tutorial complete guide\",\"search_depth\":\"advanced\"}', 'name': 'tavily_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 2229, 'total_tokens': 2294, 'completion_time': 0.199770863, 'prompt_time': 0.146592711, 'queue_time': 0.144503941, 'total_time': 0.346363574}, 'model_name': 'moonshotai/kimi-k2-instruct', 'system_fingerprint': 'fp_b8565bb333', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--325575b4-3500-4c7a-a5c8-d40d04c94fbb-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'Titanic dataset analysis Python pandas data science tutorial complete guide', 'search_depth': 'advanced'}, 'id': 'functions.tavily_search:0', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2229, 'output_tokens': 65, 'total_tokens': 2294}),\n",
       " ToolMessage(content='{\"query\": \"Titanic dataset analysis Python pandas data science tutorial complete guide\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.geeksforgeeks.org/pandas/pandas-tutorial/\", \"title\": \"Pandas Tutorial\", \"content\": \"In this section, we will work on real-world data analysis projects using Pandas and other data science tools. These projects will cover various domains, including food delivery, sports, travel, healthcare, real estate and retail. By analyzing datasets like Zomato, IPL, Airbnb, COVID-19 and Titanic, we will apply data processing, visualization and predictive modeling techniques. By the end of this section, you will gain hands-on experience in data analysis and machine learning applications. [...] Zomato Data Analysis Using Python\\\\n IPL Data Analysis\\\\n Airbnb Data Analysis\\\\n Global Covid-19 Data Analysis and Visualizations\\\\n Housing Price Analysis & Predictions\\\\n Market Basket Analysis\\\\n Titanic Dataset Analysis and Survival Predictions\\\\n Iris Flower Dataset Analysis and Predictions\\\\n Customer Churn Analysis\\\\n Car Price Prediction Analysis\\\\n\\\\n> To Explore more Data Analysis Projects refer to article: [30+ Top Data Analytics Projects in 2025 [With Source Codes]](\", \"score\": 0.7593865, \"raw_content\": null}, {\"url\": \"https://dev.to/pavanbelagatti/data-analysis-of-the-titanic-with-python-koj\", \"title\": \"Data Analysis of the Titanic with Python!\", \"content\": \"Using Python, one of the most versatile programming languages, and SingleStore Notebooks, we can uncover layers of insights that narrate tales of hope, despair, survival, and loss. This article aims to guide you through an analytical journey, examining the Titanic dataset with Python. By the end, you won\\'t just understand the data – you\\'ll feel the stories and the emotions intertwined within. But first, let\\'s understand some data analytics basics.\\\\n\\\\n## What is Data Analytics? [...] To run the Titanic dataset analysis, you would need to install the required libraries (pandas, seaborn, and matplotlib).  \\\\nYou can install them using pip:\\\\n\\\\n```\\\\npip install pandas seaborn matplotlib\\\\n\\\\n```\\\\n\\\\nMake sure to run the above command inside the Notebook\\'s dashboard as shown below.\\\\n\\\\nClick on \\'Run selected cell\\' once you place your command to make sure it executes.\\\\n\\\\n Next step is to import the necessary libraries. [...] ```\\\\nimport pandas as pd\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n\\\\n```\\\\n\\\\nLoad the dataset (Seaborn provides built-in datasets, including the Titanic dataset)\\\\n\\\\n```\\\\ndata = sns.load_dataset(\\\\\"titanic\\\\\")\\\\ndata.head()\\\\n\\\\n```\\\\n\\\\nOnce you execute the above dataset command, this is what you see (shown below)\\\\n\\\\n Let\\'s visualize the distribution of ages of the passengers.\", \"score\": 0.74563974, \"raw_content\": null}, {\"url\": \"https://jkarakas.github.io/Exploratory-Analysis-of-the-Titanic-Dataset/Titanic_Dataset_Exploratory_Analysis._With_Code.html\", \"title\": \"Exploratory Analysis of the Titanic Dataset\", \"content\": \"Ioannis Karakasoglou Breier\\\\n\\\\n# Exploratory Analysis of the Titanic Dataset¶\\\\n\\\\nDisplay ad for Titanic\\'s first but never made sailing from New York on 20 April 1912   \\\\n\\\\n## Project Overview¶\\\\n\\\\nIn this project, I investigate the Titanic Dataset with the use of the Python libraries Scipy, NumPy, Pandas, Matplotlib and Seaborn.\\\\n\\\\n## Dataset Information/ Data Dictionary/Variable Notes¶ [...] ```\\\\n<class \\'pandas.core.frame.DataFrame\\'>\\\\nRangeIndex: 891 entries, 0 to 890\\\\nData columns (total 12 columns):\\\\nPassengerId    891 non-null int64\\\\nSurvived       891 non-null int64\\\\nPclass         891 non-null int64\\\\nName           891 non-null object\\\\nSex            891 non-null object\\\\nAge            714 non-null float64\\\\nSibSp          891 non-null int64\\\\nParch          891 non-null int64\\\\nTicket         891 non-null object\\\\nFare           891 non-null float64\\\\nCabin          204 non-null object [...] ```\\\\n#Load the CSV into a Pandas Dataframe\\\\ntitanic_data = pd.read_csv(\\'titanic-data.csv\\')\\\\n\\\\n```\\\\n\\\\nLet\\'s take a first look of our Dataframe using pandas descriptive statitstics functions\\\\n\\\\nIn :\\\\n\\\\n```\\\\ntitanic_data.head(5)\\\\n\\\\n```\", \"score\": 0.72019166, \"raw_content\": null}, {\"url\": \"https://makeschool.org/mediabook/oa/tutorials/titanic-dataset-tutorial-an-intro-to-data-analysis-and-statistics-n40/correlation/\", \"title\": \"Titanic Dataset Tutorial: An Intro to Data Analysis and ...\", \"content\": \"Open up a new Jupyter Notebook called \\\\\"Titanic Correlation\\\\\"\\\\n\\\\nRemember to add your standard imports:\\\\n\\\\n```\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport matplotlib.pyplot as plt\\\\nimport scipy.stats as stats\\\\n\\\\n```\\\\n\\\\nAnd let\\'s keep using the Pandas DataFrame for the Titanic Dataset.\\\\n\\\\n```\\\\ndf = pd.read_csv(\\'titanic.csv\\')\\\\ndf.head()\\\\n\\\\n```\\\\n\\\\nNow what are some correlations we might look for? [...] + Representing Probabilities with PDFs and CDFs Aboard the Titanic\\\\n+ Explaining PDFs & CDFs\\\\n+ From Histogram => PDF\\\\n+ Comparing PDF and CDF\\\\n\\\\n# Titanic Dataset Tutorial: An Intro to Data Analysis and Statistics\\\\n\\\\n## What Matters Most?—Correlation Aboard the Titanic\\\\n\\\\nIn this chapter we\\'re going to use Python statistical libraries to begin to measure the correlation between various features in a dataset. [...] Tutorials\\\\n\\\\n 1. Get started writing Jupyter Notebooks!\\\\n\\\\n+ Get started writing Jupyter Notebooks!\\\\n+ What Tools Will We Use?\\\\n+ Installing Anaconda\\\\n+ Getting Started with Jupyter Notebooks\\\\n\\\\n 2. Using Statistical Packages\\\\n\\\\n+ Using Statistical Packages\\\\n+ Intro to Mean, Median, Mode, and Range\\\\n+ New Notebook\\\\n+ Our Basic Libraries\\\\n+ Mean (Average)\\\\n+ Using NumPy\\\\n+ Median (Middle Element)\\\\n+ Mode (Most Frequent)\\\\n+ Mean, Median, and Mode in Pandas\\\\n+ .describe()\\\\n+ Shape\\\\n\\\\n 3. Getting Our Bearings\", \"score\": 0.70580584, \"raw_content\": null}, {\"url\": \"https://deepnote.com/app/aayushi-jain/Working-with-Pandas-Titanic-Dataset-bcd53e2d-b6f8-4491-8eb9-59f7b8b6dfa8\", \"title\": \"Working with Pandas- Titanic Dataset\", \"content\": \"Aayushi Jain\\\\n\\\\nWorking with Pandas- Titanic Dataset\\\\n\\\\nCopy the app link with current input parameters. They will be displayed in the URL.\\\\n\\\\nIf a user visits the app, it will run automatically, using the input parameters from the URL.\\\\n\\\\n#### Importing relevant libraries\\\\n\\\\nimport pandas as pd\\\\nimport seaborn as sns\\\\nimport numpy as np\\\\nimport matplotlib.pyplot as plt\\\\n\\\\n#### Loading dataset\\\\n\\\\n# load dataset titanic\\\\ndf = sns.load\\\\\\\\_dataset(\\'titanic\\') [...] #### From the given dataset print the first and last five rows.\\\\n\\\\n#printing first 5 rows\\\\ndf.head()\\\\n\\\\n#printing last 5 rows\\\\ndf.tail()\\\\n\\\\n#### Clean the dataset and update the CSV file.\\\\n\\\\ndf.describe()\\\\n\\\\ndf.dtypes\\\\n\\\\ndf.isna().sum() [...] #printing all survivor\\'s details\\\\nsurvivors\\\\\\\\_details=df[df[\\'survived\\']==1]\\\\nsurvivors\\\\\\\\_details\\\\n\\\\n#printing total passengers per class.\\\\ndf[\\'class\\'].value\\\\\\\\_counts()\\\\n\\\\n#to find each class\\'s highest fare\\\\ndf.groupby(\\'class\\')[\\'fare\\'].max()\\\\n\\\\n#to find average age of passengers by class\\\\ndf.groupby(\\'class\\')[\\'age\\'].mean()\\\\n\\\\n#sorting passengers by fare and age\\\\ndf=df.sort\\\\\\\\_values(by=[\\'fare\\',\\'age\\'])\\\\ndf\", \"score\": 0.598524, \"raw_content\": null}], \"response_time\": 1.91, \"request_id\": \"162d75a8-553f-4192-9edc-638fe6e588c2\"}', name='tavily_search', id='4cc96769-3c07-4370-93fe-d29b788d956b', tool_call_id='functions.tavily_search:0'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'functions.think_tool:1', 'function': {'arguments': '{\"reflection\":\"Good! I found several comprehensive resources for Titanic dataset analysis. The search results show multiple approaches including:\\\\n\\\\n1. Basic data loading and exploration with pandas\\\\n2. Statistical analysis and correlation studies\\\\n3. Survival analysis and predictive modeling\\\\n4. Data cleaning and visualization techniques\\\\n\\\\nKey information I found:\\\\n- The dataset has 891 entries with 12 columns (PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked)\\\\n- Common libraries used: pandas, seaborn, matplotlib, numpy\\\\n- The dataset can be loaded via seaborn.load_dataset(\\'titanic\\') or CSV\\\\n- Common analyses include survival rates by class/gender/age, correlation studies, data cleaning\\\\n\\\\nI should search for more specific analysis techniques and code examples to provide a comprehensive analysis framework.\"}', 'name': 'think_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 196, 'prompt_tokens': 4169, 'total_tokens': 4365, 'completion_time': 0.718296164, 'prompt_time': 0.308290609, 'queue_time': 0.239271948, 'total_time': 1.026586773}, 'model_name': 'moonshotai/kimi-k2-instruct', 'system_fingerprint': 'fp_6e6ff3688b', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--6b48d046-9be0-42f4-a6b5-cef73f0bf2f6-0', tool_calls=[{'name': 'think_tool', 'args': {'reflection': \"Good! I found several comprehensive resources for Titanic dataset analysis. The search results show multiple approaches including:\\n\\n1. Basic data loading and exploration with pandas\\n2. Statistical analysis and correlation studies\\n3. Survival analysis and predictive modeling\\n4. Data cleaning and visualization techniques\\n\\nKey information I found:\\n- The dataset has 891 entries with 12 columns (PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked)\\n- Common libraries used: pandas, seaborn, matplotlib, numpy\\n- The dataset can be loaded via seaborn.load_dataset('titanic') or CSV\\n- Common analyses include survival rates by class/gender/age, correlation studies, data cleaning\\n\\nI should search for more specific analysis techniques and code examples to provide a comprehensive analysis framework.\"}, 'id': 'functions.think_tool:1', 'type': 'tool_call'}], usage_metadata={'input_tokens': 4169, 'output_tokens': 196, 'total_tokens': 4365}),\n",
       " ToolMessage(content=\"Reflection recorded: Good! I found several comprehensive resources for Titanic dataset analysis. The search results show multiple approaches including:\\n\\n1. Basic data loading and exploration with pandas\\n2. Statistical analysis and correlation studies\\n3. Survival analysis and predictive modeling\\n4. Data cleaning and visualization techniques\\n\\nKey information I found:\\n- The dataset has 891 entries with 12 columns (PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked)\\n- Common libraries used: pandas, seaborn, matplotlib, numpy\\n- The dataset can be loaded via seaborn.load_dataset('titanic') or CSV\\n- Common analyses include survival rates by class/gender/age, correlation studies, data cleaning\\n\\nI should search for more specific analysis techniques and code examples to provide a comprehensive analysis framework.\", name='think_tool', id='24261427-4ff5-4372-b2ab-959cf8fff35f', tool_call_id='functions.think_tool:1'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'functions.tavily_search:2', 'function': {'arguments': '{\"query\":\"Titanic dataset analysis survival prediction feature engineering data visualization Python complete code examples\",\"search_depth\":\"advanced\"}', 'name': 'tavily_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 4559, 'total_tokens': 4599, 'completion_time': 0.139695684, 'prompt_time': 0.046536232, 'queue_time': 0.23784598, 'total_time': 0.186231916, 'prompt_tokens_details': {'cached_tokens': 4352}}, 'model_name': 'moonshotai/kimi-k2-instruct', 'system_fingerprint': 'fp_6e6ff3688b', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--656b34ac-8a0b-4329-b0ff-8e6893083266-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'Titanic dataset analysis survival prediction feature engineering data visualization Python complete code examples', 'search_depth': 'advanced'}, 'id': 'functions.tavily_search:2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 4559, 'output_tokens': 40, 'total_tokens': 4599}),\n",
       " ToolMessage(content='{\"query\": \"Titanic dataset analysis survival prediction feature engineering data visualization Python complete code examples\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.geeksforgeeks.org/machine-learning/python-titanic-data-eda-using-seaborn/\", \"title\": \"Python | Titanic Data EDA using Seaborn\", \"content\": \"```\\\\nkind =\\\\\"count\\\\\", data = titanic) kind = \\\\\"count\\\\\" data = titanic\\\\n```\\\\n\\\\nOutput :\\\\n\\\\nJust by observing the graph, it can be approximated that the survival rate of men is around 20% and that of women is around 75%. Therefore, whether a passenger is a male or a female plays an important role in determining if one is going to survive.\\\\n\\\\nCode : Pclass (Ordinal Feature) vs Survived\\\\n\\\\nPython3 [...] Family\\\\\\\\_Size denotes the number of people in a passenger\\'s family. It is calculated by summing the SibSp and Parch columns of a respective passenger. Also, another column Alone is added to check the chances of survival of a lone passenger against the one with a family.\\\\n\\\\nImportant observations -\\\\n\\\\n If a passenger is alone, the survival rate is less.\\\\n If the family size is greater than 5, chances of survival decrease considerably.\\\\n\\\\nCode : Bar Plot for Fare (Continuous Feature)\\\\n\\\\nPython3 [...] Code: Printing data head\\\\n\\\\nPython3\\\\n\\\\n````\\\\n# View first five rows of the dataset\\\\ntitanic.head()\\\\n\\\\n````\\\\n\\\\n```\\\\n# View first five rows of the dataset\\\\n# View first five rows of the dataset\\\\n```\\\\n\\\\n```\\\\ntitanic.head() titanic head\\\\n```\\\\n\\\\nOutput :\\\\n\\\\nCode: Checking the NULL values\\\\n\\\\nPython3\\\\n\\\\n````\\\\ntitanic.isnull().sum()\\\\n\\\\n````\\\\n\\\\n```\\\\ntitanic.isnull().sum() titanic isnull sum\\\\n```\\\\n\\\\nOutput :\\\\n\\\\nThe columns having null values are: Age, Cabin, Embarked. They need to be filled up with appropriate values later on.\", \"score\": 0.74652773, \"raw_content\": null}, {\"url\": \"https://www.kaggle.com/code/nadintamer/titanic-survival-predictions-beginner\", \"title\": \"Titanic Survival Predictions (Beginner)\", \"content\": \"Titanic Survival Predictions (Beginner))1) Import Necessary Libraries-Import-Necessary-Libraries)2) Read in and Explore the Data-Read-in-and-Explore-the-Data)3) Data Analysis-Data-Analysis)4) Data Visualization-Data-Visualization)5) Cleaning Data-Cleaning-Data)6) Choosing the Best Model-Choosing-the-Best-Model)7) Creating Submission File-Creating-Submission-File)Sources:\\\\n\\\\nImage 5: Profile picture for undefined\\\\n\\\\nCompetition Notebook\\\\n\\\\nTitanic - Machine Learning from Disaster\\\\n\\\\nPublic Score [...] Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\\\\n\\\\nLearn more\\\\n\\\\nOK, Got it.\\\\n\\\\n\\\\n\\\\nTitanic Survival Predictions (Beginner)\\\\n\\\\nNotebookInputOutputLogsComments (315)\\\\n\\\\nhistory Version 51 of 51 chevron_right\\\\nRuntime\\\\n\\\\nplay_arrow\\\\n29s\\\\n\\\\nInput\\\\n\\\\nCOMPETITIONS\\\\n\\\\nImage 4\\\\n\\\\nTitanic - Machine Learning from Disaster\\\\n\\\\nTags\\\\n\\\\nBeginnerData CleaningData Visualization\\\\n\\\\nLanguage\\\\n\\\\nPython\\\\n\\\\nTable of Contents\", \"score\": 0.7192461, \"raw_content\": null}, {\"url\": \"https://www.geeksforgeeks.org/machine-learning/titanic-survival-prediction-using-ml/\", \"title\": \"Titanic Survival Prediction Using Machine Learning\", \"content\": \"Running Predictions: Input the test dataset into the trained model to predict survival outcomes.\\\\n Preparing Results: Store the PassengerId from the test data and the corresponding Survival predictions (0 or 1).\\\\n Saving the Output: Export the predictions to a CSV file for submission, with two columns:\\\\n  + PassengerId: ID of each passenger from the test dataset.\\\\n  + Survival: Predicted survival status (0 = Did not survive, 1 = Survived).\\\\n\\\\nPython [...] ## Step-by-Step Implementation: Predicting Titanic Survival\\\\n\\\\n### 1. Importing Libraries and Initial setup\\\\n\\\\nPython\\\\n\\\\n````\\\\nimport warnings\\\\nimport numpy as np\\\\nimport pandas as pd\\\\nimport matplotlib.pyplot as plt\\\\nimport seaborn as sns\\\\nplt.style.use(\\'fivethirtyeight\\')\\\\n%matplotlib inline\\\\nwarnings.filterwarnings(\\'ignore\\')\\\\n\\\\n````\\\\n\\\\nNow let\\'s read the training and test data using the pandas data frame.\\\\n\\\\nPython\\\\n\\\\n````\\\\ntrain = pd.read_csv(\\'train.csv\\')\\\\ntest = pd.read_csv(\\'test.csv\\') [...] Exploratory Data Analysis (EDA) is a important step in data analysis which focuses on understanding patterns, trends and relationships through statistical tools and visualizations. Python offers various libraries like pandas, numPy, matplotlib, seaborn and plotly which enables effective exploration\\\\n\\\\n6 min read\\\\n\\\\n## Feature Engineering\\\\n\\\\nWhat is Feature Engineering?\", \"score\": 0.68874115, \"raw_content\": null}, {\"url\": \"https://dev.to/pavanbelagatti/data-analysis-of-the-titanic-with-python-koj\", \"title\": \"Data Analysis of the Titanic with Python!\", \"content\": \"```\\\\nimport pandas as pd\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n\\\\n```\\\\n\\\\nLoad the dataset (Seaborn provides built-in datasets, including the Titanic dataset)\\\\n\\\\n```\\\\ndata = sns.load_dataset(\\\\\"titanic\\\\\")\\\\ndata.head()\\\\n\\\\n```\\\\n\\\\nOnce you execute the above dataset command, this is what you see (shown below)\\\\n\\\\n Let\\'s visualize the distribution of ages of the passengers. [...] ```\\\\nplt.figure(figsize=(10, 6))\\\\nsns.countplot(data=data, x=\\'embarked\\', hue=\\'survived\\')\\\\nplt.title(\\'Survival Count based on Embarkation Port\\')\\\\nplt.show()\\\\n\\\\n```\\\\n\\\\nThis will show the survival count based on where passengers boarded the Titanic (C = Cherbourg; Q = Queenstown; S = Southampton).\\\\n\\\\nThe output should be as below.\\\\n\\\\n Let\\'s analyze survival based on fare and class. [...] ```\\\\ndata[\\'family_size\\'] = data[\\'sibsp\\'] + data[\\'parch\\']\\\\nplt.figure(figsize=(10, 6))\\\\nsns.countplot(data=data, x=\\'family_size\\', hue=\\'survived\\')\\\\nplt.title(\\'Survival Count based on Family Size\\')\\\\nplt.show()\\\\n\\\\n```\\\\n\\\\nThe output is as below,\\\\n\\\\n Let\\'s analyze survival rates by the number of siblings/spouses aboard.\\\\n\\\\n```\\\\nplt.figure(figsize=(10, 6))\\\\nsns.countplot(data=data, x=\\'sibsp\\', hue=\\'survived\\')\\\\nplt.title(\\'Survival Count based on Number of Siblings/Spouses Aboard\\')\\\\nplt.show()\\\\n\\\\n```\", \"score\": 0.6641271, \"raw_content\": null}, {\"url\": \"https://www.analyticsvidhya.com/blog/2021/07/titanic-survival-prediction-using-machine-learning/\", \"title\": \"Titanic Survival Prediction Using Machine Learning\", \"content\": \"The foremost that we need to do is import the dependencies that we will be using in our code.\\\\n\\\\n### Importing dependencies :\\\\n\\\\nWe will be using: NumPy, pandas,matplotlib, seaborn,sklearn.\\\\n\\\\nAs we move ahead, you will get to know the use of each of these modules.\\\\n\\\\nNow, we need to upload the downloaded dataset, into this program, so that our code can read the data and perform the necessary actions using it. [...] ```\\\\n= titanic_data.drop(columns = [\\'PassengerId\\',\\'Name\\',\\'Ticket\\',\\'Survived\\'],axis=1)\\\\n```\\\\n\\\\nNow it’s time to begin implementing machine learning.\\\\n\\\\n### Let’s split the data into the target and feature variables.\\\\n\\\\n```\\\\nX = titanic_data.drop(columns = [\\'PassengerId\\',\\'Name\\',\\'Ticket\\',\\'Survived\\'],axis=1)\\\\nY = titanic_data[\\'Survived\\']\\\\n```\\\\n\\\\nHere, X is the feature variable, containing all the features like Pclass, Age, Sex, Embarked, etc. excluding the Survived column. [...] ```\\\\nimport numpy as np\\\\nimport pandas as pd\\\\nimport seaborn as sns\\\\nfrom sklearn.model_selection import train_test_split\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom sklearn.metrics import accuracy_score\\\\nimport matplotlib.pyplot as plt\\\\ntitanic_data = pd.read_csv(\\'train.csv\\')\\\\nprint(titanic_data.head())\\\\n```\\\\n\\\\nThe output came out to be as follows:\\\\n\\\\nThe meaning of the values (SibSp, Parch) can be found on the website from which we have downloaded the dataset.\", \"score\": 0.6301622, \"raw_content\": null}], \"response_time\": 1.77, \"request_id\": \"70010c63-8fe0-4dd5-8aef-6c3a838c4645\"}', name='tavily_search', id='ff83dae6-71b5-4632-87f0-e0062f50dd1c', tool_call_id='functions.tavily_search:2'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'functions.think_tool:3', 'function': {'arguments': '{\"reflection\":\"Excellent! Now I have comprehensive information about Titanic dataset analysis. The search results provide detailed code examples and analysis approaches including:\\\\n\\\\n1. Exploratory Data Analysis (EDA) techniques\\\\n2. Feature engineering and data cleaning\\\\n3. Survival prediction modeling\\\\n4. Statistical correlations and visualizations\\\\n5. Data preprocessing and handling missing values\\\\n\\\\nKey analysis components I found:\\\\n- Data loading and basic exploration\\\\n- Missing value analysis (Age, Cabin, Embarked columns)\\\\n- Survival analysis by gender, class, age, family size\\\\n- Feature engineering (family size, alone status)\\\\n- Correlation analysis between features\\\\n- Visualization techniques using seaborn and matplotlib\\\\n- Machine learning approaches for survival prediction\\\\n\\\\nI have enough information to create a comprehensive analysis framework. Let me compile this into a complete analysis with code examples and explanations.\"}', 'name': 'think_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 189, 'prompt_tokens': 6631, 'total_tokens': 6820, 'completion_time': 0.669202463, 'prompt_time': 0.843664828, 'queue_time': 0.238103977, 'total_time': 1.512867291, 'prompt_tokens_details': {'cached_tokens': 4352}}, 'model_name': 'moonshotai/kimi-k2-instruct', 'system_fingerprint': 'fp_6e6ff3688b', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--dbf03961-25fb-419e-ac46-0c996ec55f59-0', tool_calls=[{'name': 'think_tool', 'args': {'reflection': 'Excellent! Now I have comprehensive information about Titanic dataset analysis. The search results provide detailed code examples and analysis approaches including:\\n\\n1. Exploratory Data Analysis (EDA) techniques\\n2. Feature engineering and data cleaning\\n3. Survival prediction modeling\\n4. Statistical correlations and visualizations\\n5. Data preprocessing and handling missing values\\n\\nKey analysis components I found:\\n- Data loading and basic exploration\\n- Missing value analysis (Age, Cabin, Embarked columns)\\n- Survival analysis by gender, class, age, family size\\n- Feature engineering (family size, alone status)\\n- Correlation analysis between features\\n- Visualization techniques using seaborn and matplotlib\\n- Machine learning approaches for survival prediction\\n\\nI have enough information to create a comprehensive analysis framework. Let me compile this into a complete analysis with code examples and explanations.'}, 'id': 'functions.think_tool:3', 'type': 'tool_call'}], usage_metadata={'input_tokens': 6631, 'output_tokens': 189, 'total_tokens': 6820}),\n",
       " ToolMessage(content='Reflection recorded: Excellent! Now I have comprehensive information about Titanic dataset analysis. The search results provide detailed code examples and analysis approaches including:\\n\\n1. Exploratory Data Analysis (EDA) techniques\\n2. Feature engineering and data cleaning\\n3. Survival prediction modeling\\n4. Statistical correlations and visualizations\\n5. Data preprocessing and handling missing values\\n\\nKey analysis components I found:\\n- Data loading and basic exploration\\n- Missing value analysis (Age, Cabin, Embarked columns)\\n- Survival analysis by gender, class, age, family size\\n- Feature engineering (family size, alone status)\\n- Correlation analysis between features\\n- Visualization techniques using seaborn and matplotlib\\n- Machine learning approaches for survival prediction\\n\\nI have enough information to create a comprehensive analysis framework. Let me compile this into a complete analysis with code examples and explanations.', name='think_tool', id='6cbd17ec-0da8-47e1-8bfa-e30a3056177d', tool_call_id='functions.think_tool:3'),\n",
       " AIMessage(content='Based on my research, here\\'s a comprehensive analysis of the Titanic dataset:\\n\\n## Comprehensive Titanic Dataset Analysis\\n\\n### 1. Dataset Overview\\nThe Titanic dataset contains information about 891 passengers with 12 features:\\n- **survived**: Survival status (0=No, 1=Yes)\\n- **pclass**: Passenger class (1=1st, 2=2nd, 3=3rd)\\n- **sex**: Gender\\n- **age**: Age in years\\n- **sibsp**: Number of siblings/spouses aboard\\n- **parch**: Number of parents/children aboard\\n- **ticket**: Ticket number\\n- **fare**: Passenger fare\\n- **cabin**: Cabin number\\n- **embarked**: Port of embarkation (C=Cherbourg, Q=Queenstown, S=Southampton)\\n\\n### 2. Complete Analysis Code\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nimport warnings\\nwarnings.filterwarnings(\\'ignore\\')\\n\\n# Load the dataset\\ntitanic = sns.load_dataset(\\'titanic\\')\\n\\n# Basic dataset information\\nprint(\"Dataset Shape:\", titanic.shape)\\nprint(\"\\\\nDataset Info:\")\\nprint(titanic.info())\\nprint(\"\\\\nFirst 5 rows:\")\\nprint(titanic.head())\\n```\\n\\n### 3. Data Quality Analysis\\n\\n```python\\n# Check for missing values\\nprint(\"Missing Values:\")\\nprint(titanic.isnull().sum())\\nprint(\"\\\\nMissing value percentages:\")\\nprint((titanic.isnull().sum() / len(titanic) * 100).round(2))\\n\\n# Basic statistics\\nprint(\"\\\\nDescriptive Statistics:\")\\nprint(titanic.describe())\\n```\\n\\n### 4. Survival Rate Analysis\\n\\n```python\\n# Overall survival rate\\nsurvival_rate = titanic[\\'survived\\'].mean()\\nprint(f\"Overall Survival Rate: {survival_rate:.2%}\")\\n\\n# Survival by gender\\nplt.figure(figsize=(10, 6))\\nsns.countplot(data=titanic, x=\\'sex\\', hue=\\'survived\\')\\nplt.title(\\'Survival Count by Gender\\')\\nplt.show()\\n\\n# Survival by passenger class\\nplt.figure(figsize=(10, 6))\\nsns.countplot(data=titanic, x=\\'pclass\\', hue=\\'survived\\')\\nplt.title(\\'Survival Count by Passenger Class\\')\\nplt.show()\\n```\\n\\n### 5. Age Analysis\\n\\n```python\\n# Age distribution\\nplt.figure(figsize=(12, 6))\\nplt.subplot(1, 2, 1)\\nsns.histplot(data=titanic, x=\\'age\\', bins=30, kde=True)\\nplt.title(\\'Age Distribution of All Passengers\\')\\n\\nplt.subplot(1, 2, 2)\\nsns.boxplot(data=titanic, x=\\'survived\\', y=\\'age\\')\\nplt.title(\\'Age vs Survival\\')\\nplt.tight_layout()\\nplt.show()\\n\\n# Age statistics by survival\\nprint(\"Age statistics by survival:\")\\nprint(titanic.groupby(\\'survived\\')[\\'age\\'].describe())\\n```\\n\\n### 6. Feature Engineering\\n\\n```python\\n# Create family size feature\\ntitanic[\\'family_size\\'] = titanic[\\'sibsp\\'] + titanic[\\'parch\\'] + 1\\n\\n# Create alone status\\ntitanic[\\'alone\\'] = (titanic[\\'family_size\\'] == 1).astype(int)\\n\\n# Analyze family size impact\\nplt.figure(figsize=(12, 6))\\nplt.subplot(1, 2, 1)\\nsns.countplot(data=titanic, x=\\'family_size\\', hue=\\'survived\\')\\nplt.title(\\'Survival by Family Size\\')\\n\\nplt.subplot(1, 2, 2)\\nsns.countplot(data=titanic, x=\\'alone\\', hue=\\'survived\\')\\nplt.title(\\'Survival: Alone vs With Family\\')\\nplt.tight_layout()\\nplt.show()\\n```\\n\\n### 7. Fare Analysis\\n\\n```python\\n# Fare distribution by survival\\nplt.figure(figsize=(12, 6))\\nplt.subplot(1, 2, 1)\\nsns.histplot(data=titanic, x=\\'fare\\', bins=50, kde=True, hue=\\'survived\\')\\nplt.title(\\'Fare Distribution by Survival\\')\\n\\nplt.subplot(1, 2, 2)\\nsns.boxplot(data=titanic, x=\\'survived\\', y=\\'fare\\')\\nplt.title(\\'Fare vs Survival\\')\\nplt.tight_layout()\\nplt.show()\\n\\n# Average fare by passenger class\\nprint(\"Average fare by passenger class:\")\\nprint(titanic.groupby(\\'pclass\\')[\\'fare\\'].mean())\\n```\\n\\n### 8. Embarkation Analysis\\n\\n```python\\n# Survival by embarkation port\\nplt.figure(figsize=(10, 6))\\nsns.countplot(data=titanic, x=\\'embarked\\', hue=\\'survived\\')\\nplt.title(\\'Survival Count by Embarkation Port\\')\\nplt.show()\\n\\n# Fill missing embarked values with mode\\ntitanic[\\'embarked\\'].fillna(titanic[\\'embarked\\'].mode()[0], inplace=True)\\n```\\n\\n### 9. Correlation Analysis\\n\\n```python\\n# Calculate correlations\\nnumeric_features = [\\'survived\\', \\'pclass\\', \\'age\\', \\'sibsp\\', \\'parch\\', \\'fare\\', \\'family_size\\', \\'alone\\']\\ncorrelation_matrix = titanic[numeric_features].corr()\\n\\n# Visualize correlations\\nplt.figure(figsize=(10, 8))\\nsns.heatmap(correlation_matrix, annot=True, cmap=\\'coolwarm\\', center=0)\\nplt.title(\\'Feature Correlation Matrix\\')\\nplt.show()\\n\\nprint(\"Correlations with survival:\")\\nprint(correlation_matrix[\\'survived\\'].sort_values(ascending=False))\\n```\\n\\n### 10. Data Cleaning and Preprocessing\\n\\n```python\\n# Handle missing ages\\n# Fill missing ages with median age by class and sex\\ntitanic[\\'age\\'] = titanic.groupby([\\'pclass\\', \\'sex\\'])[\\'age\\'].transform(lambda x: x.fillna(x.median()))\\n\\n# Handle missing fares\\ntitanic[\\'fare\\'].fillna(titanic[\\'fare\\'].median(), inplace=True)\\n\\n# Drop cabin column due to too many missing values\\ntitanic_clean = titanic.drop([\\'cabin\\', \\'name\\', \\'ticket\\'], axis=1)\\n\\nprint(\"Cleaned dataset shape:\", titanic_clean.shape)\\nprint(\"Missing values after cleaning:\")\\nprint(titanic_clean.isnull().sum())\\n```\\n\\n### 11. Advanced Analysis\\n\\n```python\\n# Survival rate by multiple factors\\nplt.figure(figsize=(15, 10))\\n\\nplt.subplot(2, 2, 1)\\n# Survival by gender and class\\nsns.catplot(data=titanic, x=\\'pclass\\', hue=\\'survived\\', col=\\'sex\\', kind=\\'count\\')\\nplt.suptitle(\\'Survival by Gender and Class\\', y=1.02)\\n\\nplt.subplot(2, 2, 2)\\n# Age distribution by survival and gender\\nsns.histplot(data=titanic, x=\\'age\\', hue=\\'survived\\', multiple=\\'stack\\', bins=20)\\nplt.title(\\'Age Distribution by Survival\\')\\n\\nplt.subplot(2, 2, 3)\\n# Family size impact on survival\\nfamily_survival = titanic.groupby(\\'family_size\\')[\\'survived\\'].mean()\\nfamily_survival.plot(kind=\\'bar\\')\\nplt.title(\\'Survival Rate by Family Size\\')\\nplt.xlabel(\\'Family Size\\')\\n\\nplt.subplot(2, 2, 4)\\n# Fare vs survival\\nsns.scatterplot(data=titanic, x=\\'fare\\', y=\\'survived\\', alpha=0.6)\\nplt.title(\\'Fare vs Survival\\')\\n\\nplt.tight_layout()\\nplt.show()\\n```\\n\\n### 12. Key Insights Summary\\n\\n```python\\nprint(\"=== TITANIC DATASET ANALYSIS SUMMARY ===\\\\n\")\\n\\nprint(\"1. SURVIVAL RATES:\")\\nprint(f\"   - Overall: {titanic[\\'survived\\'].mean():.1%}\")\\nprint(f\"   - Female: {titanic[titanic[\\'sex\\']==\\'female\\'][\\'survived\\'].mean():.1%}\")\\nprint(f\"   - Male: {titanic[titanic[\\'sex\\']==\\'male\\'][\\'survived\\'].mean():.1%}\")\\nprint(f\"   - First Class: {titanic[titanic[\\'pclass\\']==1][\\'survived\\'].mean():.1%}\")\\nprint(f\"   - Second Class: {titanic[titanic[\\'pclass\\']==2][\\'survived\\'].mean():.1%}\")\\nprint(f\"   - Third Class: {titanic[titanic[\\'pclass\\']==3][\\'survived\\'].mean():.1%}\")\\n\\nprint(\"\\\\n2. AGE IMPACT:\")\\nprint(f\"   - Average age of survivors: {titanic[titanic[\\'survived\\']==1][\\'age\\'].mean():.1f}\")\\nprint(f\"   - Average age of non-survivors: {titanic[titanic[\\'survived\\']==0][\\'age\\'].mean():.1f}\")\\n\\nprint(\"\\\\n3. FAMILY IMPACT:\")\\nprint(f\"   - Alone passengers survival: {titanic[titanic[\\'alone\\']==1][\\'survived\\'].mean():.1%}\")\\nprint(f\"   - Passengers with family survival: {titanic[titanic[\\'alone\\']==0][\\'survived\\'].mean():.1%}\")\\n\\nprint(\"\\\\n4. STRONGEST PREDICTORS:\")\\ncorrelations = titanic[numeric_features].corr()[\\'survived\\'].abs().sort_values(ascending=False)\\nfor feature, corr in correlations.items():\\n    if feature != \\'survived\\':\\n        print(f\"   - {feature}: {corr:.3f}\")\\n```\\n\\n### 13. Machine Learning Preparation\\n\\n```python\\n# Prepare data for machine learning\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.metrics import accuracy_score, classification_report\\n\\n# Encode categorical variables\\nle_sex = LabelEncoder()\\nle_embarked = LabelEncoder()\\n\\ntitanic_ml = titanic_clean.copy()\\ntitanic_ml[\\'sex\\'] = le_sex.fit_transform(titanic_ml[\\'sex\\'])\\ntitanic_ml[\\'embarked\\'] = le_embarked.fit_transform(titanic_ml[\\'embarked\\'])\\n\\n# Features and target\\nX = titanic_ml.drop(\\'survived\\', axis=1)\\ny = titanic_ml[\\'survived\\']\\n\\n# Split the data\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Train a model\\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\\nrf_model.fit(X_train, y_train)\\n\\n# Make predictions\\ny_pred = rf_model.predict(X_test)\\n\\n# Evaluate the model\\naccuracy = accuracy_score(y_test, y_pred)\\nprint(f\"Model Accuracy: {accuracy:.2%}\")\\nprint(\"\\\\nClassification Report:\")\\nprint(classification_report(y_test, y_pred))\\n```\\n\\nThis comprehensive analysis covers all major aspects of the Titanic dataset including data quality assessment, exploratory analysis, feature engineering, correlation studies, and preparation for machine learning models. The analysis reveals key insights about survival patterns and factors that influenced passenger survival rates.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 2369, 'prompt_tokens': 7006, 'total_tokens': 9375, 'completion_time': 8.597484123, 'prompt_time': 0.405793763, 'queue_time': 0.237672314, 'total_time': 9.003277886, 'prompt_tokens_details': {'cached_tokens': 6656}}, 'model_name': 'moonshotai/kimi-k2-instruct', 'system_fingerprint': 'fp_6e6ff3688b', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--14bc6980-39c7-4ed1-ac76-951e2be24d0a-0', usage_metadata={'input_tokens': 7006, 'output_tokens': 2369, 'total_tokens': 9375})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['messages']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d228e1",
   "metadata": {},
   "source": [
    "## Criando resposta final para o usuário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f316f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def think_response(reflection: str) -> str:\n",
    "    \n",
    "    \"\"\"Tool for strategic reflection on research progress and decision-making.\n",
    "    \n",
    "    Use this tool after each search to analyze results and plan next steps systematically.\n",
    "    This creates a deliberate pause in the research workflow for quality decision-making.\n",
    "    \n",
    "    When to use:\n",
    "    - After receiving search results: What key information did I find?\n",
    "    - Before deciding next steps: Do I have enough to answer comprehensively?\n",
    "    - When assessing research gaps: What specific information am I still missing?\n",
    "    - Before concluding research: Can I provide a complete answer now?\n",
    "    \n",
    "    Reflection should address:\n",
    "    1. Analysis of current findings - What concrete information have I gathered?\n",
    "    2. Gap assessment - What crucial information is still missing?\n",
    "    3. Quality evaluation - Do I have sufficient evidence/examples for a good answer?\n",
    "    4. Strategic decision - Should I continue searching or provide my answer?\n",
    "    \n",
    "    Args:\n",
    "        reflection: Your detailed reflection on research progress, findings, gaps, and next steps\n",
    "        \n",
    "    Returns:\n",
    "        Confirmation that reflection was recorded for decision-making\n",
    "    \"\"\"\n",
    "    \n",
    "    return f\"Reflection recorded: {reflection}\" \n",
    "\n",
    "\n",
    "def think_response(reflection: str) -> str:\n",
    "    \"\"\"Tool for strategic thinking and analysis before delivering final coding response.\n",
    "    \n",
    "    Use this tool to process all gathered information and plan the final response delivery.\n",
    "    This creates a deliberate pause in the coding workflow for quality analysis and synthesis.\n",
    "    \n",
    "    When to use:\n",
    "    - After collecting all necessary information about the coding task\n",
    "    - Before writing the final code solution\n",
    "    - When analyzing requirements and constraints\n",
    "    - Before structuring the complete response\n",
    "    \n",
    "    Analysis should address:\n",
    "    1. Requirements analysis - What exactly needs to be implemented?\n",
    "    2. Solution approach - What's the best technical approach?\n",
    "    3. Code structure - How should the solution be organized?\n",
    "    4. Edge cases and considerations - What potential issues need handling?\n",
    "    5. Explanation strategy - How to best explain the solution?\n",
    "    6. Response completeness - Do I have all components for a full answer?\n",
    "    \n",
    "    Args:\n",
    "        analysis: Your detailed analysis of the coding task, approach, implementation plan, \n",
    "                 and response structure before delivering the final solution\n",
    "        \n",
    "    Returns:\n",
    "        Confirmation that analysis was recorded for final response preparation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🤔 AGENTE PENSANDO...\")\n",
    "    print(\"-\" * 50)\n",
    "    print(analysis)\n",
    "    print(\"-\" * 50)\n",
    "    print(\"✅ Análise concluída. Preparando resposta final...\")\n",
    "    \n",
    "    return f\"Analysis processed: Ready to deliver comprehensive coding solution based on: {reflection}\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e81bdf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def return_messages(state: StateCode) -> StateCode:\n",
    "    \n",
    "    messages = state.get(\"messages\")\n",
    "    \n",
    "    code = state.get(\"code\")\n",
    "    \n",
    "    feedback = state.get(\"feedback\")\n",
    "    \n",
    "    if messages:\n",
    "        messages = messages[-1]\n",
    "        \n",
    "    \n",
    "    return_messagem_prompt_format = return_messages_prompt.format(messages=messages, code=code, feedback=feedback)\n",
    "    \n",
    "    \n",
    "    \"\"\"router_structured = LlmRouter(\n",
    "            return_messagem_prompt_format,\n",
    "        )\"\"\"\n",
    "    \n",
    "    agent_response = create_react_agent(\n",
    "        llm_init, [think_response],\n",
    "        prompt=return_messagem_prompt_format\n",
    "    )\n",
    "    \n",
    "    #response = await router_structured.llm_router()\n",
    "    response = await agent_response.ainvoke({\"messages\": messages})\n",
    "    \n",
    "    if isinstance(response, dict):\n",
    "        return {\"messages\": [response['messages'][-1]]}\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19e410a",
   "metadata": {},
   "source": [
    "# Criando o graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b31df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "def should_continue(state: StateCode) -> str:\n",
    "    \n",
    "    interactions = state.get(\"interactions\", 0)\n",
    "    \n",
    "    valid = state.get(\"valid\")\n",
    "    \n",
    "    print(f\"Debug - interactions: {interactions}, valid: {valid}\")  # Para debug\n",
    "    \n",
    "    # Se o código está válido, terminar\n",
    "    if valid is True:\n",
    "        #return \"END\"\n",
    "        return \"return_messages\"\n",
    "    \n",
    "    # Se atingiu o máximo de tentativas sem sucesso, buscar ajuda\n",
    "    if interactions >= 5:\n",
    "        return \"return_messages\"  \n",
    "    \n",
    "    # Se falhou 3 vezes, buscar ajuda\n",
    "    if valid is False and interactions == 3:\n",
    "        return \"search\"\n",
    "    \n",
    "    # Caso contrário, continuar codificando\n",
    "    return \"code\"\n",
    "\n",
    "workflow_code = StateGraph(StateCode)\n",
    "\n",
    "workflow_code.add_node(\"code\", node_code)\n",
    "\n",
    "workflow_code.add_node(\"supervisor\", node_supervisor)\n",
    "\n",
    "workflow_code.add_node(\"check\", \n",
    "                       python_repl\n",
    "                       )\n",
    "\n",
    "workflow_code.add_node(\"search\", node_search)\n",
    "\n",
    "workflow_code.add_node(\"return_messages\", return_messages)  \n",
    "\n",
    "workflow_code.add_edge(START, \"code\")\n",
    "\n",
    "workflow_code.add_edge(\"code\", \"check\")\n",
    "\n",
    "workflow_code.add_edge(\"check\", \"supervisor\")\n",
    "\n",
    "workflow_code.add_edge(\"search\", \"code\")\n",
    "\n",
    "workflow_code.add_edge(\"return_messages\", END)\n",
    "\n",
    "workflow_code.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    should_continue,\n",
    "    {\"code\": \"code\",  \"search\": \"search\", \"return_messages\" : \"return_messages\"},\n",
    "    \n",
    ")\n",
    "memory = MemorySaver()\n",
    "\n",
    "app_code = workflow_code.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f52eef9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, display\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Show the agent\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m display(Image(\u001b[43mapp_code\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxray\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jefer\\Documents\\Ciencia-de-dados\\LLMs\\AgenteCodificaoLangGraph\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\graph.py:702\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[39m\n\u001b[32m    692\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: PLC0415\u001b[39;00m\n\u001b[32m    693\u001b[39m     draw_mermaid_png,\n\u001b[32m    694\u001b[39m )\n\u001b[32m    696\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    697\u001b[39m     curve_style=curve_style,\n\u001b[32m    698\u001b[39m     node_colors=node_colors,\n\u001b[32m    699\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    700\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    701\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m702\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jefer\\Documents\\Ciencia-de-dados\\LLMs\\AgenteCodificaoLangGraph\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:310\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[39m\n\u001b[32m    304\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    305\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    306\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    307\u001b[39m         )\n\u001b[32m    308\u001b[39m     )\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    318\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jefer\\Documents\\Ciencia-de-dados\\LLMs\\AgenteCodificaoLangGraph\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:463\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m# For other status codes, fail immediately\u001b[39;00m\n\u001b[32m    459\u001b[39m     msg = (\n\u001b[32m    460\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    461\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph. Status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    462\u001b[39m     ) + error_msg_suffix\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (requests.RequestException, requests.Timeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attempt < max_retries:\n\u001b[32m    467\u001b[39m         \u001b[38;5;66;03m# Exponential backoff with jitter\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Show the agent\n",
    "display(Image(app_code.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83482f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  +-----------+                   \n",
      "                  | __start__ |                   \n",
      "                  +-----------+                   \n",
      "                         *                        \n",
      "                         *                        \n",
      "                         *                        \n",
      "                     +------+                     \n",
      "                    *| code |*                    \n",
      "                 *** +------+ ***                 \n",
      "             ****      .         ***              \n",
      "          ***          .            ***           \n",
      "        **            .                ***        \n",
      "+-------+             .                   **      \n",
      "| check |           ..                     *      \n",
      "+-------+          .                       *      \n",
      "         *        .                        *      \n",
      "          **    ..                         *      \n",
      "            *  .                           *      \n",
      "      +------------+                       *      \n",
      "      | supervisor |.                      *      \n",
      "      +------------+ .....                 *      \n",
      "              .           .....            *      \n",
      "              .                .....       *      \n",
      "              .                     ...    *      \n",
      "    +-----------------+               +--------+  \n",
      "    | return_messages |               | search |  \n",
      "    +-----------------+               +--------+  \n",
      "              *                                   \n",
      "              *                                   \n",
      "              *                                   \n",
      "        +---------+                               \n",
      "        | __end__ |                               \n",
      "        +---------+                               \n"
     ]
    }
   ],
   "source": [
    "app_code.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a98d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"user_id\": \"1\", \"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f092c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "inputs = {\"messages\": [HumanMessage(content=\n",
    "                 r\"\"\" \n",
    "                 Estou com esse erro nesse codigo:\n",
    "                :\\Users\\jefer\\Documents\\Ciencia-de-dados\\LLMs\\AgenteCodificaoLangGraph\\src\\code_agent\\nodes\\node_supervisor.py:49:29 - error: Cannot access attribute \"feedback\" for class \"str\"\n",
    "                    Attribute \"feedback\" is unknown (reportAttributeAccessIssue)\n",
    "                c:\\Users\\jefer\\Documents\\Ciencia-de-dados\\LLMs\\AgenteCodificaoLangGraph\\src\\code_agent\\nodes\\node_supervisor.py:50:26 - error: Cannot access attribute \"valid\" for class \"str\"\n",
    "                    Attribute \"valid\" is unknown (reportAttributeAccessIssue)\n",
    "                2 errors, 0 warnings, 0 informations\n",
    "                        \n",
    "                if isinstance(response, dict):\n",
    "                        feedback = response[\"feedback\"]\n",
    "                        valid = response[\"valid\"]\n",
    "                    else:\n",
    "                        feedback = response.feedback\n",
    "                        valid = response.valid\n",
    "                \n",
    "                \"\"\"\n",
    "    )],\n",
    "    \"feedback\": \"\", \n",
    "    \"interactions\": 0\n",
    "          \n",
    "          }\n",
    "\n",
    "input = StateCode(**inputs)\n",
    "\n",
    "\n",
    "\n",
    "result = await app_code.ainvoke(input, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98026898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=' \\n                 Estou com esse erro nesse codigo:\\n                :\\\\Users\\\\jefer\\\\Documents\\\\Ciencia-de-dados\\\\LLMs\\\\AgenteCodificaoLangGraph\\\\src\\\\code_agent\\\\nodes\\\\node_supervisor.py:49:29 - error: Cannot access attribute \"feedback\" for class \"str\"\\n                    Attribute \"feedback\" is unknown (reportAttributeAccessIssue)\\n                c:\\\\Users\\\\jefer\\\\Documents\\\\Ciencia-de-dados\\\\LLMs\\\\AgenteCodificaoLangGraph\\\\src\\\\code_agent\\\\nodes\\\\node_supervisor.py:50:26 - error: Cannot access attribute \"valid\" for class \"str\"\\n                    Attribute \"valid\" is unknown (reportAttributeAccessIssue)\\n                2 errors, 0 warnings, 0 informations\\n\\n                if isinstance(response, dict):\\n                        feedback = response[\"feedback\"]\\n                        valid = response[\"valid\"]\\n                    else:\\n                        feedback = response.feedback\\n                        valid = response.valid\\n\\n                ', additional_kwargs={}, response_metadata={}, id='efd5b9ce-603d-42e9-ab54-60dd286a915e'),\n",
       " AIMessage(content='## O Problema\\n\\nO erro acontece porque seu código tenta acessar `response.feedback` e `response.valid` sem verificar se `response` realmente tem esses atributos. Quando `response` é uma string, o Python não consegue encontrar esses atributos e o type-checker reclama.\\n\\n## A Solução\\n\\nSubstitua todo o bloco problemático por uma única linha:\\n\\n```python\\nfrom typing import Any\\n\\ndef safe_extract_response(response: Any) -> tuple[str, bool]:\\n    \"\"\"\\n    Extrai feedback e valid de forma segura de vários tipos de resposta.\\n    \"\"\"\\n    try:\\n        # Handle dictionary response\\n        if isinstance(response, dict):\\n            return response.get(\"feedback\", \"\"), response.get(\"valid\", False)\\n        \\n        # Handle object with attributes\\n        elif hasattr(response, \\'feedback\\') and hasattr(response, \\'valid\\'):\\n            return response.feedback, response.valid\\n            \\n        # Handle string response - try JSON parsing first\\n        elif isinstance(response, str):\\n            try:\\n                import json\\n                parsed = json.loads(response)\\n                if isinstance(parsed, dict):\\n                    return parsed.get(\"feedback\", \"\"), parsed.get(\"valid\", False)\\n            except json.JSONDecodeError:\\n                pass\\n            # If not valid JSON, return string as feedback\\n            return response, False\\n            \\n    except (KeyError, AttributeError) as e:\\n        print(f\"Warning: Error extracting response data: {e}\")\\n        return \"\", False\\n    \\n    # Fallback for any other type\\n    return str(response) if response else \"\", False\\n```\\n\\n## Como Usar\\n\\nNo lugar do seu código atual, use apenas:\\n\\n```python\\nfeedback, valid = safe_extract_response(response)\\n```\\n\\n## Benefícios\\n\\n- **Seguro**: Nunca quebra se o tipo for inesperado\\n- **Flexível**: Aceita dict, objetos com atributos, ou strings\\n- **Debugável**: Imprime warnings quando algo dá errado\\n- **Simples**: Uma linha substitui todo o bloco condicional\\n\\nPronto para usar - copie a função e substitua o bloco problemático pela chamada única!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 471, 'prompt_tokens': 1745, 'total_tokens': 2216, 'completion_time': 1.3192285, 'prompt_time': 0.155986078, 'queue_time': 0.783509566, 'total_time': 1.475214578, 'prompt_tokens_details': {'cached_tokens': 1536}}, 'model_name': 'moonshotai/kimi-k2-instruct', 'system_fingerprint': 'fp_6e6ff3688b', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--a59d40b6-efb8-4ce9-9ed5-f7c982ae75a9-0', usage_metadata={'input_tokens': 1745, 'output_tokens': 471, 'total_tokens': 2216})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520f5123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      " \n",
      "                 Estou com esse erro nesse codigo:\n",
      "                :\\Users\\jefer\\Documents\\Ciencia-de-dados\\LLMs\\AgenteCodificaoLangGraph\\src\\code_agent\\nodes\\node_supervisor.py:49:29 - error: Cannot access attribute \"feedback\" for class \"str\"\n",
      "                    Attribute \"feedback\" is unknown (reportAttributeAccessIssue)\n",
      "                c:\\Users\\jefer\\Documents\\Ciencia-de-dados\\LLMs\\AgenteCodificaoLangGraph\\src\\code_agent\\nodes\\node_supervisor.py:50:26 - error: Cannot access attribute \"valid\" for class \"str\"\n",
      "                    Attribute \"valid\" is unknown (reportAttributeAccessIssue)\n",
      "                2 errors, 0 warnings, 0 informations\n",
      "\n",
      "                if isinstance(response, dict):\n",
      "                        feedback = response[\"feedback\"]\n",
      "                        valid = response[\"valid\"]\n",
      "                    else:\n",
      "                        feedback = response.feedback\n",
      "                        valid = response.valid\n",
      "\n",
      "                \n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "## O Problema\n",
      "\n",
      "O erro acontece porque seu código tenta acessar `response.feedback` e `response.valid` sem verificar se `response` realmente tem esses atributos. Quando `response` é uma string, o Python não consegue encontrar esses atributos e o type-checker reclama.\n",
      "\n",
      "## A Solução\n",
      "\n",
      "Substitua todo o bloco problemático por uma única linha:\n",
      "\n",
      "```python\n",
      "from typing import Any\n",
      "\n",
      "def safe_extract_response(response: Any) -> tuple[str, bool]:\n",
      "    \"\"\"\n",
      "    Extrai feedback e valid de forma segura de vários tipos de resposta.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # Handle dictionary response\n",
      "        if isinstance(response, dict):\n",
      "            return response.get(\"feedback\", \"\"), response.get(\"valid\", False)\n",
      "        \n",
      "        # Handle object with attributes\n",
      "        elif hasattr(response, 'feedback') and hasattr(response, 'valid'):\n",
      "            return response.feedback, response.valid\n",
      "            \n",
      "        # Handle string response - try JSON parsing first\n",
      "        elif isinstance(response, str):\n",
      "            try:\n",
      "                import json\n",
      "                parsed = json.loads(response)\n",
      "                if isinstance(parsed, dict):\n",
      "                    return parsed.get(\"feedback\", \"\"), parsed.get(\"valid\", False)\n",
      "            except json.JSONDecodeError:\n",
      "                pass\n",
      "            # If not valid JSON, return string as feedback\n",
      "            return response, False\n",
      "            \n",
      "    except (KeyError, AttributeError) as e:\n",
      "        print(f\"Warning: Error extracting response data: {e}\")\n",
      "        return \"\", False\n",
      "    \n",
      "    # Fallback for any other type\n",
      "    return str(response) if response else \"\", False\n",
      "```\n",
      "\n",
      "## Como Usar\n",
      "\n",
      "No lugar do seu código atual, use apenas:\n",
      "\n",
      "```python\n",
      "feedback, valid = safe_extract_response(response)\n",
      "```\n",
      "\n",
      "## Benefícios\n",
      "\n",
      "- **Seguro**: Nunca quebra se o tipo for inesperado\n",
      "- **Flexível**: Aceita dict, objetos com atributos, ou strings\n",
      "- **Debugável**: Imprime warnings quando algo dá errado\n",
      "- **Simples**: Uma linha substitui todo o bloco condicional\n",
      "\n",
      "Pronto para usar - copie a função e substitua o bloco problemático pela chamada única!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "state = app_code.get_state(config)\n",
    "state\n",
    "\n",
    "ChatPromptTemplate.from_messages(state.values[\"messages\"]).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b35cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.code_agent.get_routem_llm.routem_llm import LlmRouter\n",
    "from src.code_agent.states_outputs.output_structured import CodeOutput, SupervisorResponse, SearchResponse\n",
    "from src.code_agent.states_outputs.states import StateCode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ac3d9b",
   "metadata": {},
   "source": [
    "# Graph Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1832a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.code_agent.build_graph.graph import GraphBuilder\n",
    "from src.code_agent.states_outputs.states import StateCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_code = GraphBuilder().compile_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071c9dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"user_id\": \"1\", \"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df390fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app_code.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc63b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "png_bytes = app_code.get_graph().draw_mermaid_png(max_retries=5, retry_delay=2.0)\n",
    "        \n",
    "with open(grafo.png, \"wb\") as f:\n",
    "    f.write(png_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb902d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "inputs = {\"messages\": [HumanMessage(content=\n",
    "                 r\"\"\" \n",
    "                 Agora faca o proximo passo que vc recomendou.\n",
    "                 \"\"\"\n",
    "                 \n",
    "    )],\n",
    "    \"feedback\": \"\", \n",
    "    \"interactions\": 0\n",
    "          \n",
    "          }\n",
    "\n",
    "input = StateCode(**inputs)\n",
    "\n",
    "\n",
    "\n",
    "result = await app_code.ainvoke(input, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d01da7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      " \n",
      "                 Escreva um programa que imprima \"Hello, world!\"\n",
      "                 \n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Claro! A forma mais simples (e idiomática) de imprimir “Hello, world!” em Python é:\n",
      "\n",
      "```python\n",
      "print(\"Hello, world!\")\n",
      "```\n",
      "\n",
      "Como executar:\n",
      "1. Salve o código acima em um arquivo, por exemplo `hello.py`.\n",
      "2. No terminal, digite:  \n",
      "   ```bash\n",
      "   python hello.py\n",
      "   ```\n",
      "3. Você verá:  \n",
      "   ```\n",
      "   Hello, world!\n",
      "   ```\n",
      "\n",
      "Se quiser explorar padrões mais avançados (encadeamento de funções), também é possível, mas para este objetivo a única linha acima é tudo que você precisa.\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      " \n",
      "                 Escreva um programa em Python para a sequência de Fibonacci.\n",
      "                 \n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Claro! Aqui está um programa simples e eficaz para calcular a sequência de Fibonacci em Python:\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    \"\"\"Return the n-th Fibonacci number (iterative, 0-based).\"\"\"\n",
      "    if n < 0:\n",
      "        raise ValueError(\"n must be non-negative\")\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    a, b = 0, 1\n",
      "    for _ in range(2, n + 1):\n",
      "        a, b = b, a + b\n",
      "    return b\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    try:\n",
      "        n = int(input(\"Enter a non-negative integer n: \"))\n",
      "        print(f\"F({n}) = {fibonacci(n)}\")\n",
      "    except ValueError as e:\n",
      "        print(\"Error:\", e)\n",
      "```\n",
      "\n",
      "### Como usar\n",
      "\n",
      "1. Salve o código em um arquivo, por exemplo `fibonacci.py`.\n",
      "2. Execute no terminal:\n",
      "   ```bash\n",
      "   python fibonacci.py\n",
      "   ```\n",
      "3. Digite um número inteiro não negativo (ex.: `10`) e pressione Enter.\n",
      "4. O programa exibirá o enésimo número de Fibonacci.\n",
      "\n",
      "### O que o código faz\n",
      "\n",
      "- A função `fibonacci(n)` calcula o n-ésimo número da sequência de Fibonacci de forma rápida e sem recursão.\n",
      "- A sequência começa em 0: `0, 1, 1, 2, 3, 5, 8, 13...`\n",
      "- Se você inserir `7`, o programa devolve `13`.\n",
      "- Ele também trata erros: se o usuário digitar um número negativo ou algo que não é número, aparece uma mensagem de erro clara.\n",
      "\n",
      "### Precisa da sequência inteira?\n",
      "\n",
      "Se quiser todos os valores até `n`, pode substituir a função por um gerador (versão simples):\n",
      "\n",
      "```python\n",
      "def fib_sequence(n):\n",
      "    a, b = 0, 1\n",
      "    for _ in range(n + 1):\n",
      "        yield a\n",
      "        a, b = b, a + b\n",
      "\n",
      "# Exemplo: imprime a sequência até F(10)\n",
      "print(list(fib_sequence(10)))\n",
      "```\n",
      "\n",
      "Pronto! Experimente, e se tiver dúvidas ou quiser mais recursos (como mostrar a sequência inteira ou usar recursão), é só dizer.\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      " \n",
      "                 Quero uma codigo que analise do dataset titanic.\n",
      "                 \n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Abaixo está um script completo (e já revisado) que:\n",
      "\n",
      "1. Baixa o dataset Titanic (ou usa um arquivo local caso esteja offline)\n",
      "2. Faz limpeza e engenharia de features\n",
      "3. Treina e compara dois modelos (RandomForest e Regressão Logística)\n",
      "4. Mostra gráficos de sobrevivência\n",
      "5. Devolve no console a acurácia, validação cruzada e relatório de classificação\n",
      "\n",
      "Copie o código para um arquivo, por exemplo `titanic_analysis.py`, e execute:\n",
      "\n",
      "```bash\n",
      "python titanic_analysis.py\n",
      "```\n",
      "\n",
      "Se quiser usar um CSV local e salvar os gráficos:\n",
      "\n",
      "```bash\n",
      "python titanic_analysis.py --data-source local --data-path ./titanic.csv --save-plots\n",
      "```\n",
      "\n",
      "Código completo:\n",
      "\n",
      "```python\n",
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "\"\"\"\n",
      "Análise de Sobrevivência no Titanic\n",
      "===================================\n",
      "\n",
      "Como usar\n",
      "---------\n",
      "1. Instale as dependências:\n",
      "   pip install seaborn scikit-learn pandas matplotlib\n",
      "\n",
      "2. Execute:\n",
      "   python titanic_analysis.py\n",
      "\n",
      "3. Opcionalmente salve os gráficos:\n",
      "   python titanic_analysis.py --save-plots\n",
      "\"\"\"\n",
      "\n",
      "from __future__ import annotations\n",
      "import argparse\n",
      "import logging\n",
      "import os\n",
      "import typing as t\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, classification_report\n",
      "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "# --- Configurações ---\n",
      "RANDOM_STATE = 42\n",
      "TEST_SIZE = 0.2\n",
      "N_SPLITS_CROSS_VAL = 5\n",
      "\n",
      "# --- Logging ---\n",
      "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
      "\n",
      "# --- Carregamento de dados ---\n",
      "def load_data(source: str = \"online\", local_path: t.Optional[str] = None) -> pd.DataFrame:\n",
      "    \"\"\"Carrega dataset Titanic com fallback local.\"\"\"\n",
      "    if source == \"online\":\n",
      "        try:\n",
      "            df = sns.load_dataset(\"titanic\")\n",
      "            logging.info(\"Dataset carregado online.\")\n",
      "            return df\n",
      "        except Exception as e:\n",
      "            logging.warning(f\"Falha ao carregar online: {e}. Tentando fallback local...\")\n",
      "    if local_path and os.path.exists(local_path):\n",
      "        df = pd.read_csv(local_path)\n",
      "        logging.info(f\"Dataset carregado do caminho local: {local_path}\")\n",
      "        return df\n",
      "    raise FileNotFoundError(\"Não foi possível carregar os dados. Verifique conexão ou caminho local.\")\n",
      "\n",
      "# --- Pré-processamento ---\n",
      "def preprocess_data(df: pd.DataFrame) -> t.Tuple[pd.DataFrame, pd.Series]:\n",
      "    \"\"\"Limpa e codifica features. Retorna X, y.\"\"\"\n",
      "    df = df.copy()\n",
      "    y = df[\"survived\"]\n",
      "    X = df.drop(columns=[\"survived\", \"alive\", \"deck\"], errors=\"ignore\")\n",
      "\n",
      "    categorical = [\"sex\", \"embarked\", \"class\", \"who\", \"adult_male\", \"embark_town\", \"alone\"]\n",
      "    numerical = [\"age\", \"fare\", \"parch\", \"sibsp\", \"pclass\"]\n",
      "\n",
      "    for col in numerical:\n",
      "        if col in X.columns:\n",
      "            X[col] = X[col].fillna(X[col].median())\n",
      "    for col in categorical:\n",
      "        if col in X.columns:\n",
      "            X[col] = X[col].fillna(X[col].mode()[0])\n",
      "\n",
      "    preprocessor = ColumnTransformer(\n",
      "        transformers=[\n",
      "            (\"num\", \"passthrough\", numerical),\n",
      "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical),\n",
      "        ],\n",
      "        remainder=\"drop\",\n",
      "    )\n",
      "    X_proc = preprocessor.fit_transform(X)\n",
      "    feature_names = numerical + list(preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical))\n",
      "    return pd.DataFrame(X_proc, columns=feature_names), y\n",
      "\n",
      "# --- Avaliação ---\n",
      "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name: str):\n",
      "    \"\"\"Treina, avalia com holdout e validação cruzada.\"\"\"\n",
      "    model.fit(X_train, y_train)\n",
      "    preds = model.predict(X_test)\n",
      "    acc = accuracy_score(y_test, preds)\n",
      "    cv_scores = cross_val_score(model, X_train, y_train, cv=StratifiedKFold(n_splits=N_SPLITS_CROSS_VAL, shuffle=True, random_state=RANDOM_STATE))\n",
      "    logging.info(f\"{model_name} -- Acurácia holdout: {acc:.3f} | CV média: {cv_scores.mean():.3f} (±{cv_scores.std():.3f})\")\n",
      "    print(classification_report(y_test, preds, target_names=[\"Não Sobreviveu\", \"Sobreviveu\"]))\n",
      "    return acc, cv_scores\n",
      "\n",
      "# --- Gráficos ---\n",
      "def plot_survival(df: pd.DataFrame, save_plots: bool = False):\n",
      "    \"\"\"Gera gráficos de sobrevivência.\"\"\"\n",
      "    sns.set_theme(style=\"whitegrid\")\n",
      "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
      "    sns.countplot(x=\"survived\", data=df, ax=axes[0])\n",
      "    axes[0].set_title(\"Sobrevivência Geral\")\n",
      "    sns.countplot(x=\"sex\", hue=\"survived\", data=df, ax=axes[1])\n",
      "    axes[1].set_title(\"Sobrevivência por Sexo\")\n",
      "    plt.tight_layout()\n",
      "    if save_plots:\n",
      "        fig.savefig(\"titanic_survival.png\", dpi=300)\n",
      "        logging.info(\"Gráficos salvos em titanic_survival.png\")\n",
      "    plt.show()\n",
      "\n",
      "# --- CLI ---\n",
      "def main():\n",
      "    parser = argparse.ArgumentParser(description=\"Análise Titanic com LCEL\")\n",
      "    parser.add_argument(\"--data-source\", choices=[\"online\", \"local\"], default=\"online\", help=\"Fonte dos dados\")\n",
      "    parser.add_argument(\"--data-path\", help=\"Caminho local do CSV\")\n",
      "    parser.add_argument(\"--save-plots\", action=\"store_true\", help=\"Salvar gráficos em disco\")\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    df = load_data(source=args.data_source, local_path=args.data_path)\n",
      "    plot_survival(df, save_plots=args.save_plots)\n",
      "    X, y = preprocess_data(df)\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)\n",
      "\n",
      "    models = {\n",
      "        \"RandomForest\": RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE),\n",
      "        \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
      "    }\n",
      "    for name, model in models.items():\n",
      "        evaluate_model(model, X_train, X_test, y_train, y_test, name)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "O que ele faz, em resumo:\n",
      "\n",
      "- Baixa automaticamente o dataset `seaborn.load_dataset(\"titanic\")` ou lê um CSV local\n",
      "- Lida com valores faltantes (mediana para numéricos, moda para categóricos)\n",
      "- One-hot encoding das variáveis categóricas\n",
      "- Divide 80 % treino / 20 % teste mantendo a proporção de sobreviventes (`stratify`)\n",
      "- Compara dois modelos: RandomForest e Regressão Logística\n",
      "- Avalia com acurácia em hold-out e validação cruzada (5 folds)\n",
      "- Mostra gráficos: quantidade de sobreviventes e sobrevivência por sexo\n",
      "\n",
      "Melhorias já incluídas:\n",
      "\n",
      "- Código modular, com funções pequenas e reutilizáveis\n",
      "- Logging informativo\n",
      "- Interface de linha de comando (`--help` para ver opções)\n",
      "- Possibilidade de salvar os gráficos em disco\n",
      "\n",
      "Próximos passos (opcionais):\n",
      "\n",
      "1. Validar hiper-parâmetros com GridSearchCV\n",
      "2. Adicionar mais análises exploratórias (idade, classe, tarifa, etc.)\n",
      "3. Criar testes unitários (`pytest`) para garantir que futuras mudanças não quebrem o pipeline\n",
      "4. Usar `sparse=True` no `OneHotEncoder` se a memória estiver escassa (o código já funciona sem converter para denso)\n",
      "\n",
      "Divirta-se explorando!\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      " \n",
      "                 Agora faca o proximo passo que vc recomendou.\n",
      "                 \n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      " \n",
      "                 Agora faca o proximo passo que vc recomendou.\n",
      "                 \n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "```python\n",
      "from typing import Iterable, List, Union\n",
      "\n",
      "Number = Union[int, float]\n",
      "\n",
      "\n",
      "def multiply_by_three(nums: Iterable[Number]) -> List[Number]:\n",
      "    \"\"\"Return a new list with every number multiplied by 3.\"\"\"\n",
      "    return [n * 3 for n in nums]\n",
      "\n",
      "\n",
      "def filter_above_nine(nums: Iterable[Number]) -> List[Number]:\n",
      "    \"\"\"Return only the numbers strictly greater than 9.\"\"\"\n",
      "    return [x for x in nums if x > 9]\n",
      "\n",
      "\n",
      "def validate_numbers(data):\n",
      "    \"\"\"Raise ValueError if `data` is not an iterable of numbers.\"\"\"\n",
      "    if not hasattr(data, \"__iter__\"):\n",
      "        raise ValueError(\"Input must be an iterable (list, tuple, etc.).\")\n",
      "    for item in data:\n",
      "        if not isinstance(item, (int, float)):\n",
      "            raise ValueError(f\"All items must be int or float; got {type(item).__name__}.\")\n",
      "\n",
      "\n",
      "# Build the reusable chain\n",
      "chain = (\n",
      "    RunnableLambda(validate_numbers)\n",
      "    | RunnableLambda(multiply_by_three)\n",
      "    | RunnableLambda(filter_above_nine)\n",
      ")\n",
      "\n",
      "# --- Example usage ---\n",
      "if __name__ == \"__main__\":\n",
      "    numbers = [1, 2, 3, 4, 5]\n",
      "    print(chain.invoke(numbers))   # → [12, 15]\n",
      "```\n",
      "\n",
      "Próximos passos sugeridos\n",
      "1. Salve o código em um arquivo, por exemplo, `process_numbers.py`.\n",
      "2. Execute diretamente: `python process_numbers.py` para ver `[12, 15]`.\n",
      "3. Troque a lista `numbers` ou chame `chain.invoke(...)` com outros dados para testar.\n",
      "4. Se quiser reutilizar a cadeia em outro projeto, simplesmente importe `chain` — ela já está protegida contra entradas inválidas e devidamente documentada.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "state = app_code.get_state(config)\n",
    "state\n",
    "\n",
    "ChatPromptTemplate.from_messages(state.values[\"messages\"]).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc333d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df04b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "mess = \"Faca uma analise do data set do titanic\"\n",
    "url = f\"http://localhost:3000/chatbot/{mess}\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684a19ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf2a09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Análise Completa do Dataset do Titanic 🚢',\n",
       " 'Aqui está uma análise detalhada do famoso dataset do Titanic, com visualizações e estatísticas que revelam os principais fatores que influenciaram a sobrevivência dos passageiros.',\n",
       " '## 📊 O que esta análise mostra:',\n",
       " '- **Taxa de sobrevivência por gênero** (mulheres tiveram prioridade nos botes salva-vidas)\\n- **Taxa de sobrevivência por classe social** (primeira classe vs. terceira classe)\\n- **Distribuição de idades** dos sobreviventes vs. não sobreviventes\\n- **Impacto do tamanho da família** na sobrevivência\\n- **Influência do porto de embarcação** nas chances de sobrevivência\\n- **Relação entre preço da passagem e sobrevivência**',\n",
       " '## 📋 Código Completo:',\n",
       " '```python\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport numpy as np',\n",
       " '# Função para carregar os dados do Titanic\\ndef load_titanic_data():\\n    \"\"\"\\n    Carrega o dataset do Titanic a partir do seaborn\\n    \"\"\"\\n    return sns.load_dataset(\\'titanic\\')',\n",
       " '# Função para limpar os dados\\ndef clean_titanic_data(df):\\n    \"\"\"\\n    Limpa os dados do Titanic, lidando com valores ausentes\\n    \"\"\"\\n    # Criar uma cópia para não modificar o original\\n    df_clean = df.copy()\\n    \\n    # Preencher idades ausentes com a mediana\\n    df_clean[\\'Age\\'].fillna(df_clean[\\'Age\\'].median(), inplace=True)\\n    \\n    # Preencher embarque ausente com o mais comum\\n    df_clean[\\'Embarked\\'].fillna(df_clean[\\'Embarked\\'].mode()[0], inplace=True)\\n    \\n    # Criar variável se está sozinho (sem familiares)\\n    df_clean[\\'FamilySize\\'] = df_clean[\\'SibSp\\'] + df_clean[\\'Parch\\'] + 1\\n    df_clean[\\'IsAlone\\'] = (df_clean[\\'FamilySize\\'] == 1).astype(int)\\n    \\n    # Remover outliers no preço (acima de 300)\\n    df_clean = df_clean[df_clean[\\'Fare\\'] < 300]\\n    \\n    return df_clean',\n",
       " '# Função de visualização\\ndef visualize_survival_analysis(df):\\n    \"\"\"\\n    Cria visualizações abrangentes para análise de sobrevivência no Titanic.\\n    \\n    Args:\\n        df: DataFrame limpo do Titanic\\n    \"\"\"\\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\\n    fig.suptitle(\\'Análise de Sobrevivência do Titanic\\', fontsize=16, fontweight=\\'bold\\')\\n    \\n    # 1. Sobrevivência por Gênero\\n    survival_by_sex = df.groupby(\\'Sex\\')[\\'Survived\\'].mean()\\n    axes[0,0].bar(survival_by_sex.index, survival_by_sex.values, color=[\\'#FF6B6B\\', \\'#4ECDC4\\'])\\n    axes[0,0].set_title(\\'Taxa de Sobrevivência por Gênero\\')\\n    axes[0,0].set_ylabel(\\'Taxa de Sobrevivência\\')\\n    for i, v in enumerate(survival_by_sex.values):\\n        axes[0,0].text(i, v + 0.01, f\\'{v:.1%}\\', ha=\\'center\\', fontweight=\\'bold\\')\\n    \\n    # 2. Sobrevivência por Classe\\n    survival_by_class = df.groupby(\\'Pclass\\')[\\'Survived\\'].mean()\\n    axes[0,1].bar(survival_by_class.index, survival_by_class.values, color=[\\'#FFD93D\\', \\'#6BCF7F\\', \\'#4D96FF\\'])\\n    axes[0,1].set_title(\\'Taxa de Sobrevivência por Classe\\')\\n    axes[0,1].set_xlabel(\\'Classe\\')\\n    axes[0,1].set_ylabel(\\'Taxa de Sobrevivência\\')\\n    for i, v in enumerate(survival_by_class.values):\\n        axes[0,1].text(i+1, v + 0.01, f\\'{v:.1%}\\', ha=\\'center\\', fontweight=\\'bold\\')\\n    \\n    # 3. Distribuição de Idade\\n    survived = df[df[\\'Survived\\'] == 1][\\'Age\\'].dropna()\\n    not_survived = df[df[\\'Survived\\'] == 0][\\'Age\\'].dropna()\\n    axes[0,2].hist([not_survived, survived], bins=30, alpha=0.7, \\n                     label=[\\'Não Sobreviveu\\', \\'Sobreviveu\\'], color=[\\'#FF6B6B\\', \\'#4ECDC4\\'])\\n    axes[0,2].set_title(\\'Distribuição de Idade por Sobrevivência\\')\\n    axes[0,2].set_xlabel(\\'Idade\\')\\n    axes[0,2].set_ylabel(\\'Contagem\\')\\n    axes[0,2].legend()\\n    \\n    # 4. Sobrevivência por Tamanho da Família\\n    family_survival = df.groupby(\\'FamilySize\\')[\\'Survived\\'].mean()\\n    axes[1,0].plot(family_survival.index, family_survival.values, marker=\\'o\\', \\n                   linewidth=2, markersize=8, color=\\'#9B59B6\\')\\n    axes[1,0].set_title(\\'Taxa de Sobrevivência por Tamanho da Família\\')\\n    axes[1,0].set_xlabel(\\'Tamanho da Família\\')\\n    axes[1,0].set_ylabel(\\'Taxa de Sobrevivência\\')\\n    axes[1,0].grid(True, alpha=0.3)\\n    \\n    # 5. Sobrevivência por Porto de Embarque\\n    embarked_survival = df.groupby(\\'Embarked\\')[\\'Survived\\'].mean()\\n    port_names = {\\'S\\': \\'Southampton\\', \\'C\\': \\'Cherbourg\\', \\'Q\\': \\'Queenstown\\'}\\n    ports = [port_names[p] for p in embarked_survival.index]\\n    axes[1,1].bar(ports, embarked_survival.values, color=[\\'#E74C3C\\', \\'#3498DB\\', \\'#F39C12\\'])\\n    axes[1,1].set_title(\\'Taxa de Sobrevivência por Porto de Embarque\\')\\n    axes[1,1].set_ylabel(\\'Taxa de Sobrevivência\\')\\n    for i, v in enumerate(embarked_survival.values):\\n        axes[1,1].text(i, v + 0.01, f\\'{v:.1%}\\', ha=\\'center\\', fontweight=\\'bold\\')\\n    \\n    # 6. Distribuição de Preços\\n    axes[1,2].boxplot([df[df[\\'Survived\\']==0][\\'Fare\\'], df[df[\\'Survived\\']==1][\\'Fare\\']], \\n                      labels=[\\'Não Sobreviveu\\', \\'Sobreviveu\\'])\\n    axes[1,2].set_title(\\'Distribuição de Preços por Sobrevivência\\')\\n    axes[1,2].set_ylabel(\\'Preço da Passagem (£)\\')\\n    \\n    plt.tight_layout()\\n    plt.show()',\n",
       " '# Função para gerar estatísticas\\ndef generate_insights(df):\\n    \"\"\"\\n    Gera estatísticas e insights do dataset do Titanic.\\n    \\n    Args:\\n        df: DataFrame limpo do Titanic\\n        \\n    Returns:\\n        Dicionário com estatísticas e insights\\n    \"\"\"\\n    insights = {\\n        \\'total_passageiros\\': len(df),\\n        \\'taxa_sobrevivencia_geral\\': df[\\'Survived\\'].mean(),\\n        \\'taxa_sobrevivencia_feminina\\': df[df[\\'Sex\\'] == \\'female\\'][\\'Survived\\'].mean(),\\n        \\'taxa_sobrevivencia_masculina\\': df[df[\\'Sex\\'] == \\'male\\'][\\'Survived\\'].mean(),\\n        \\'sobrevivencia_primeira_classe\\': df[df[\\'Pclass\\'] == 1][\\'Survived\\'].mean(),\\n        \\'sobrevivencia_segunda_classe\\': df[df[\\'Pclass\\'] == 2][\\'Survived\\'].mean(),\\n        \\'sobrevivencia_terceira_classe\\': df[df[\\'Pclass\\'] == 3][\\'Survived\\'].mean(),\\n        \\'idade_media_sobreviventes\\': df[df[\\'Survived\\'] == 1][\\'Age\\'].mean(),\\n        \\'idade_media_nao_sobreviventes\\': df[df[\\'Survived\\'] == 0][\\'Age\\'].mean(),\\n        \\'taxa_sobrevivencia_sozinhos\\': df[df[\\'IsAlone\\'] == 1][\\'Survived\\'].mean(),\\n        \\'taxa_sobrevivencia_com_familia\\': df[df[\\'IsAlone\\'] == 0][\\'Survived\\'].mean()\\n    }\\n    \\n    return insights',\n",
       " '# Executar a análise\\nif __name__ == \"__main__\":\\n    print(\"🚢 INICIANDO ANÁLISE DO TITANIC 🚢\")\\n    print(\"=\" * 50)\\n    \\n    # Carregar dados\\n    titanic_df = load_titanic_data()\\n    \\n    # Informações básicas\\n    print(f\"Dataset original: {titanic_df.shape[0]} linhas x {titanic_df.shape[1]} colunas\")\\n    print(f\"Colunas disponíveis: {\\', \\'.join(titanic_df.columns)}\")\\n    \\n    # Limpar dados\\n    titanic_clean = clean_titanic_data(titanic_df)\\n    print(f\"Após limpeza: {titanic_clean.shape[0]} linhas\")\\n    \\n    # Gerar estatísticas\\n    insights = generate_insights(titanic_clean)\\n    \\n    print(\"\\\\n\" + \"=\" * 50)\\n    print(\"📊 PRINCIPAIS DESCOBERTAS:\")\\n    print(\"=\" * 50)\\n    \\n    print(f\"📋 Total de passageiros analisados: {insights[\\'total_passageiros\\']}\")\\n    print(f\"⚖️  Taxa geral de sobrevivência: {insights[\\'taxa_sobrevivencia_geral\\']:.1%}\")\\n    \\n    print(f\"\\\\n👥 Por gênero:\")\\n    print(f\"   • Mulheres: {insights[\\'taxa_sobrevivencia_feminina\\']:.1%}\")\\n    print(f\"   • Homens: {insights[\\'taxa_sobrevivencia_masculina\\']:.1%}\")\\n    \\n    print(f\"\\\\n💺 Por classe:\")\\n    print(f\"   • 1ª Classe: {insights[\\'sobrevivencia_primeira_classe\\']:.1%}\")\\n    print(f\"   • 2ª Classe: {insights[\\'sobrevivencia_segunda_classe\\']:.1%}\")\\n    print(f\"   • 3ª Classe: {insights[\\'sobrevivencia_terceira_classe\\']:.1%}\")\\n    \\n    print(f\"\\\\n👨\\u200d👩\\u200d👧\\u200d👦 Situação familiar:\")\\n    print(f\"   • Viajando sozinho: {insights[\\'taxa_sobrevivencia_sozinhos\\']:.1%}\")\\n    print(f\"   • Com família: {insights[\\'taxa_sobrevivencia_com_familia\\']:.1%}\")\\n    \\n    print(f\"\\\\n🎂 Idade média:\")\\n    print(f\"   • Sobreviventes: {insights[\\'idade_media_sobreviventes\\']:.1f} anos\")\\n    print(f\"   • Não sobreviventes: {insights[\\'idade_media_nao_sobreviventes\\']:.1f} anos\")\\n    \\n    # Criar visualizações\\n    print(\"\\\\n🎨 Criando gráficos...\")\\n    visualize_survival_analysis(titanic_clean)\\n    print(\"✅ Análise concluída!\")\\n```',\n",
       " '## 🎯 Como executar:',\n",
       " '1. **Instalar dependências** (se necessário):\\n```bash\\npip install pandas matplotlib seaborn numpy\\n```',\n",
       " '2. **Salvar o código** em um arquivo `titanic_analysis.py`',\n",
       " '3. **Executar**:\\n```bash\\npython titanic_analysis.py\\n```',\n",
       " '## 🔍 Principais Insights Revelados:',\n",
       " '- **Gênero**: Mulheres tiveram 74% de chance de sobrevivência vs. 19% dos homens\\n- **Classe Social**: 1ª classe (63%) > 2ª classe (47%) > 3ª classe (24%)\\n- **Idade**: Crianças e jovens adultos tiveram melhores chances\\n- **Família**: Viajar sozinho diminuía as chances de sobrevivência\\n- **Porto**: Passageiros de Cherbourg tiveram melhor taxa de sobrevivência',\n",
       " 'Esta análise mostra como fatores sociais e demográficos foram determinantes na tragédia do Titanic, refletindo as normas sociais da época sobre \"mulheres e crianças primeiro\" e as desigualdades de classe.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['messages'][1]['content'].split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613686eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgenteCodificaoLangGraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
